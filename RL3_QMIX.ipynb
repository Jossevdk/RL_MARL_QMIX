{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jossevdk/RL_MARL_QMIX/blob/main/RL3_QMIX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTT57uLHmIVf"
      },
      "source": [
        "\n",
        "# **Disclaimer**\n",
        "This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intSkgKwmUZm"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "\n",
        "Welcome to the third assignment of the Reinforcement Learning course! In this assignment, you will implement and train a QMix algorithm for a multi-agent environment. The task takes place in a Pacman Capture the Flag game, where you control two blue Pacman agents working together to capture food and avoid red Pacman opponents.\n",
        "\n",
        "This assignment is divided into three sections:\n",
        "\n",
        "\n",
        "1.  Implementing QMix: You will construct the QMix architecture to combine individual agent Q-values into a global Q-value. Training the Agents: You will implement the training loop, including the QMix loss function. Getting it working here is key, don't do much too fast.\n",
        "2.   Try to improve your agents, with different training strategies. Don't experiment too much with new architectures, that will come later!\n",
        "3. \tExploring Beyond QMix: You will choose and implement another multi-agent RL algorithm and compare its performance with QMix.\n",
        "\n",
        "During this assignment, there will be four tournaments conducted in a round-robin format, where each team will compete against others:\n",
        "-\tIntermediate Tournaments (3): These serve as checkpoints to test your models and evaluate your training strategies.\n",
        "-\tFinal Tournament (1): This determines the ultimate winner! Who will forever be in the Hall of Fame?\n",
        "\n",
        "The first tournament will be on “bloxCapture.lay” and depending on the progress of the teams, the next tournaments will use more maps. For more information about the submission and deadlines check the assignment. The tournament results **will not impact your grade**.\n",
        "\n",
        "\n",
        "By the end of this assignment, you will gain practical experience in implementing and experimenting with multi-agent reinforcement learning algorithms!\n",
        "\n",
        "## Environment: [PacMan, a capture the flag variant ](https://ai.berkeley.edu/contest.html)\n",
        "\n",
        "The environment you'll be working in, PacMan Capture the Flag, is a multi-agent version of the classic PacMan game. It involves multiple agents that need to cooperate to achieve a shared objective. Each agent can only observe part of the environment(although this is close to the full environment) and needs to act based on its local observations, making this an ideal setup for exploring cooperative multi-agent reinforcement learning.\n",
        "The game logic comes from [here](https://ai.berkeley.edu/contest.html), the code is not to hard understand if you want to delve a little bit deeper. Additional to this base game you will also find some additional code so that the environment is more useable like other gym environemnts. This makes it easier to train your deep RL multi agent algorithms.\n",
        "You'll also see a display option in the code. You can visualize games quite easily, however for this we recommend, copying the notebook and running it locally.  \n",
        "\n",
        "**For more information on the environment check the additional file**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IBmjgcct1f9"
      },
      "source": [
        "\n",
        "# **Setup**\n",
        "\n",
        "Before we dive into coding, let’s make sure everything is set up correctly.\n",
        "\n",
        "1. Install Dependencies\n",
        "\n",
        "You’ll need to install the following libraries to run the notebook. Run the cell below to install them:\n",
        "\n",
        "\n",
        "*   PacMan Capture the Flag: a reinforcement learning environments.\n",
        "*   Packages you'll use throught the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoP_9-CHfydw",
        "outputId": "98573555-4fc3-4e4d-f9a4-bd9fb35de759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "Cloning into 'rl-lab-3-pacman'...\n",
            "remote: Enumerating objects: 801, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 801 (delta 42), reused 0 (delta 0), pack-reused 688 (from 1)\u001b[K\n",
            "Receiving objects: 100% (801/801), 19.26 MiB | 5.30 MiB/s, done.\n",
            "Resolving deltas: 100% (344/344), done.\n",
            "Note: switching to 'abdc4c05b877ebe71b48bf70d9c12c8ba80882f1'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git init\n",
        "!git clone https://student:fFYKVqdc976PXAuyhyz4@gitlab.ilabt.imec.be/emalomgr/rl-lab-3-pacman.git --branch student_version\n",
        "!mv ./rl-lab-3-pacman/* ./\n",
        "!rm -r ./rl-lab-3-pacman/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1-5e7HR3saX"
      },
      "source": [
        "2. Import Necessary Libraries\n",
        "\n",
        "Once the dependencies are installed, import the key libraries you’ll need throughout the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaNmbYmYrdGq",
        "outputId": "cf8d951e-2a2b-4483-cdeb-0d2c916085e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on the CPU\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from gymPacMan import gymPacMan_parallel_env\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "00mnpkAvlOSB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "96e3a491-5ec4-4e78-98b6-d4bab0b07e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded layout from file\n",
            "Loading Team: /content//agents/baselineTeam.py\n",
            "Arguments: {}\n",
            "Blue team starts\n",
            "Red team starts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseppe-wuyts3\u001b[0m (\u001b[33mseppe-wuyts3-universiteit-gent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241204_152922-__20241204_152922</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized/runs/__20241204_152922' target=\"_blank\">__20241204_152922</a></strong> to <a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized' target=\"_blank\">https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized/runs/__20241204_152922' target=\"_blank\">https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized/runs/__20241204_152922</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized/runs/__20241204_152922?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78c0c9560e20>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "# possible options: env = gymPacMan_parallel_env(layout_file = f'{file_path}/layouts/smallCapture.lay', display = False, length = 800, reward_forLegalAction= True, defenceReward = True, random_layout = True, enemieName = 'randomTeam', self_play = False)\n",
        "env = gymPacMan_parallel_env(layout_file='./layouts/tinyCapture.lay', display=False, length=300, enemieName='baselineTeam.py')\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# Start a new wandb run to track this script\n",
        "\n",
        "wandb.login(key = '', relogin = True)\n",
        "name_experiment =''\n",
        "wandb.init(project=\"\", id = f\"{name_experiment}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjgyVEbp8sow"
      },
      "source": [
        "# **Section 1: QMix Implementation**\n",
        "\n",
        "In this section, you will implement the QMix algorithm to control agents in the PacMan environment. QMix is a powerful algorithm in multi-agent reinforcement learning that allows for centralized training with decentralized execution. The key idea behind QMix is to learn a mixing network that combines individual agent Q-values into a global Q-value, which allows agents to make coordinated decisions while still acting independently during execution.\n",
        "\n",
        "## QMix Theory Overview\n",
        "\n",
        "QMix is a value-based multi-agent reinforcement learning algorithm designed for cooperative tasks. It addresses the challenge of decentralized control while maintaining a centralized training framework. The key idea is to learn individual Q-values for each agent and combine them into a global Q-value that represents the team’s joint policy.\n",
        "\n",
        "Core Concepts:\n",
        "1.\tIndividual Q-Values: Each agent has a separate Q-network that predicts the Q-values for its actions based on its local observations.\n",
        "2.\tGlobal Q-Value: A mixer network aggregates the individual Q-values into a global Q-value, ensuring that the global Q-value is monotonic with respect to individual Q-values. This monotonicity ensures that maximizing the global Q-value aligns with maximizing the individual Q-values.\n",
        "3.\tHypernetworks: QMix uses hypernetworks to generate the weights for the mixer network dynamically. These weights depend on the global state, allowing the mixer network to adapt its behavior based on the team’s overall situation.\n",
        "Step-by-Step Implementation\n",
        "\n",
        "You will be implementing QMix step by step, focusing on the following parts:\n",
        "\n",
        "1.\tImplement the individual agent Q-networks.\n",
        "2.\tBuild the mixing network to combine individual Q-values.\n",
        "3.\tSet up the loss function and training loop.\n",
        "4.\tTrain the agents in the PacMan environment.\n",
        "\n",
        "##Let’s begin!\n",
        "\n",
        "## 1.1   Agent Q-Network Implementation\n",
        "\n",
        "Each agent in the QMix algorithm has its own Q-network that estimates the action-value function based on the agent’s local observation. The Q-network takes the agent’s observation as input and outputs the Q-values for each possible action.\n",
        "\n",
        "**Task:** Implement IQL.\n",
        "\n",
        "**Keep it simple**\n",
        "\n",
        "Don't use too advanced architecures from the start and train against random agents on a small map. Start on \"tinyCapture.lay\" and move on to \"smallCapture.lay\" when you see your algorithm works This way you can see quicker whether your algorithm is working.\n",
        "\n",
        "**Check if the agents learn without QMix**\n",
        "\n",
        "Do a run with IQL without the mixing network and another run with QMix and reflect on the improvements in the performance of QMix.\n",
        "\n",
        "**Evaluate during training**\n",
        "\n",
        "Try to evaluate your agents while you train, so you can see faster if they learn or not. A good tool for this is WANDB, where you can easily compare different stragegies during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnE1LUgR0hzY"
      },
      "outputs": [],
      "source": [
        "class AgentQNetwork(nn.Module):\n",
        "    def __init__(self,obs_shape, action_dim,hidden_dim=64):\n",
        "        super(AgentQNetwork, self).__init__()\n",
        "        self.c1 = nn.Conv2d(obs_shape[0], hidden_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.c2 = nn.Conv2d(hidden_dim, 2*hidden_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = torch.relu(self.c1(obs))\n",
        "        x = torch.relu(self.c2(x))\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        q_values = self.fc2(x)\n",
        "        return q_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dYpvxTc8SUh"
      },
      "source": [
        "## test for IQL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPnO0KhC-d1w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkX8usZy--AA"
      },
      "outputs": [],
      "source": [
        "AGENTS_NUM = [1, 3]\n",
        "##param\n",
        "LEARNING_RATE = 1e-3\n",
        "GAMMA = 0.99\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.1\n",
        "EPSILON_DECAY = 5000\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 10000\n",
        "NUM_EPISODES = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpA1h5JC_IJQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H9qtVQS81ds"
      },
      "source": [
        "\t•\tobs_dim: The dimension of the agent’s local observation.\n",
        "\t•\taction_dim: The number of possible actions the agent can take.\n",
        "\n",
        "## 1.2 Mixing Network\n",
        "\n",
        "The mixing network is responsible for combining the individual Q-values from each agent into a global Q-value. The mixing network ensures that the global Q-value is a monotonic function of each agent’s Q-value, which allows the system to maintain decentralized decision-making at runtime.\n",
        "\n",
        "**Task:** Implement the mixing network.\n",
        "\n",
        "The mixing network will take the Q-values of all agents as input and output a single global Q-value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aIiC5gD8yZT"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class SimpleQMixer(nn.Module):\n",
        "    def __init__(self, n_agents, state_shape):\n",
        "        super(SimpleQMixer, self).__init__()\n",
        "\n",
        "        self.n_agents = n_agents\n",
        "        self.state_shape = state_shape\n",
        "        print(state_shape)\n",
        "        # Much simpler state processing\n",
        "        self.state_conv = nn.Sequential(\n",
        "            nn.Conv3d(state_shape[0], 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * state_shape[1] * state_shape[2] * state_shape[3],  n_agents)  # Output: (batch_size, 32 * n_agents)\n",
        "        )\n",
        "        # Initialize weights positively\n",
        "        for m in self.state_conv:\n",
        "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
        "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.1)\n",
        "        # Single layer mixing network\n",
        "        self.mixing_layer = nn.Linear(n_agents , 1, bias=False)\n",
        "\n",
        "        # Initialize close to equal weights\n",
        "        nn.init.uniform_(self.mixing_layer.weight, 0.9, 1.1)\n",
        "\n",
        "    def forward(self, agent_qs, states):\n",
        "\n",
        "        bs = agent_qs.size(0)\n",
        "        print(bs)\n",
        "        weights = F.softplus(self.state_conv(states))\n",
        "\n",
        "\n",
        "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
        "\n",
        "        weighted_qs = agent_qs * weights.unsqueeze(1)\n",
        "\n",
        "        q_tot = self.mixing_layer(weighted_qs.squeeze(1))\n",
        "        print(q_tot.view(bs, -1, 1).shape)\n",
        "        return q_tot.view(bs, -1, 1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionMixer(nn.Module):\n",
        "    def __init__(self, n_agents, state_shape=(2, 8, 10, 20), embed_dim=64, n_heads=4):\n",
        "        super(AttentionMixer, self).__init__()\n",
        "\n",
        "        self.n_agents = n_agents  # Aantal agents\n",
        "        self.state_shape = state_shape[1:]  # Verwijder de agent-dimensie\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Shared Conv2D encoder for each agent\n",
        "        self.shared_encoder = nn.Sequential(\n",
        "            nn.Conv2d(self.state_shape[0], 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Dynamically compute flattened size for the linear layer\n",
        "        dummy_input = torch.zeros(1, *self.state_shape)  # Dummy input voor een enkele agent\n",
        "        with torch.no_grad():\n",
        "            conv_out_size = self.shared_encoder(dummy_input).size(1)\n",
        "\n",
        "        # Fully connected layer to project to embed_dim\n",
        "        self.fc = nn.Linear(conv_out_size, embed_dim)\n",
        "\n",
        "        # Multi-head attention for inter-agent interaction\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads, batch_first=True)\n",
        "\n",
        "        # Hypernetwork for mixing weights and biases\n",
        "        self.hyper_w = nn.Sequential(\n",
        "            nn.Linear(embed_dim * n_agents, embed_dim * embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * embed_dim, n_agents * embed_dim)\n",
        "        )\n",
        "        self.hyper_b = nn.Sequential(\n",
        "            nn.Linear(embed_dim * n_agents, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Final layer to compute Q_tot\n",
        "        self.final_layer = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, agent_qs, states):\n",
        "        \"\"\"\n",
        "        :param agent_qs: Tensor of shape [batch_size, n_agents]\n",
        "        :param states: Tensor of shape [batch_size, n_agents, 8, 10, 20]\n",
        "        :return: Q_tot: Total Q-value for the batch\n",
        "        \"\"\"\n",
        "\n",
        "        #TODO: SOFTPLUS\n",
        "\n",
        "        batch_size = states.size(0)\n",
        "\n",
        "        # Process each agent's state individually using the shared encoder\n",
        "        agent_features = []\n",
        "        for i in range(self.n_agents):\n",
        "            agent_state = states[:, i, :, :, :]  # Shape: [batch_size, 8, 10, 20]\n",
        "            encoded_state = self.shared_encoder(agent_state)  # Encode individual agent\n",
        "            projected_state = self.fc(encoded_state)  # Project to embed_dim\n",
        "            agent_features.append(projected_state)\n",
        "\n",
        "        # Stack agent features: Shape [batch_size, n_agents, embed_dim]\n",
        "        agent_features = torch.stack(agent_features, dim=1)\n",
        "\n",
        "        # Apply attention to model inter-agent interaction\n",
        "        attention_out, _ = self.attention(agent_features, agent_features, agent_features)\n",
        "\n",
        "        # Flatten attention output for hypernetwork\n",
        "        attention_out_flat = attention_out.reshape(batch_size, -1)  # Shape: [batch_size, n_agents * embed_dim]\n",
        "\n",
        "        # Compute hypernetwork weights and biases\n",
        "        hyper_w = self.hyper_w(attention_out_flat).view(batch_size, self.n_agents, self.embed_dim)\n",
        "        hyper_b = self.hyper_b(attention_out_flat).view(batch_size, self.embed_dim)\n",
        "\n",
        "        # Combine agent Q-values with weights and biases\n",
        "        agent_qs = agent_qs.view(batch_size, self.n_agents, 1)  # Shape: [batch_size, n_agents, 1]\n",
        "        weighted_qs = torch.bmm(agent_qs.transpose(1, 2), hyper_w).squeeze(1) + hyper_b\n",
        "\n",
        "        # Final projection to Q_tot\n",
        "        q_tot = self.final_layer(weighted_qs).unsqueeze(1)\n",
        "        return q_tot\n"
      ],
      "metadata": {
        "id": "8E-mSFq9gIq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OUjn7xq88V6"
      },
      "source": [
        "\t•\tstate_dim: The dimension of the global state (available during centralized training).\n",
        "\t•\tn_agents: The number of agents, which determines the number of Q-values being mixed.\n",
        "\t•\tWeights and biases: The weights and biases of the mixing network depend on the global state, ensuring that different states lead to different weightings of agent Q-values.\n",
        "\n",
        "## 1.3 Loss Function and Training Loop\n",
        "\n",
        "The agents need to learn their Q-values by minimizing the Temporal Difference (TD) error. The loss is computed as the difference between the predicted Q-value (from the agent’s Q-network) and the target Q-value (computed using the Bellman equation).\n",
        "\n",
        "**Task:** Set up the loss function and implement the training loop for QMix.\n",
        "\n",
        "**Hint:** Use the huber loss for more stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2c2BYMg84p3"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(agent_q_networks, mixing_network, target_q_networks, batch, weights = None, gamma=0.99, lambda_=0.1):\n",
        "    states, actions, rewards, next_states, dones = batch\n",
        "\n",
        "    # Convert to tensors and move to device\n",
        "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "\n",
        "     # Get current Q-values for each agent\n",
        "    agent_qs = []\n",
        "    for i, agent_q_network in enumerate(agent_q_networks):\n",
        "        agent_q = agent_q_network(states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
        "        agent_q = agent_q.gather(dim=1, index=actions[:,i].unsqueeze(1))  # Select action Q-values\n",
        "        agent_qs.append(agent_q)\n",
        "    agent_qs = torch.cat(agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
        "    # Get target Q-values for each agent using Double DQN\n",
        "    with torch.no_grad():\n",
        "        target_agent_qs = []\n",
        "        for i, (agent_q_network, target_q_network) in enumerate(\n",
        "            zip(agent_q_networks, target_q_networks)\n",
        "        ):\n",
        "            # Double DQN: Get greedy actions from the current network\n",
        "            next_q_values = agent_q_network(next_states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
        "            greedy_actions = torch.argmax(next_q_values, dim=1, keepdim=True)\n",
        "            # Get target Q-values using the target network\n",
        "            target_q_values = target_q_network(next_states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
        "            target_q = target_q_values.gather(1, greedy_actions)\n",
        "            done_mask = dones[:, 0, 0].unsqueeze(1)\n",
        "            filtered_target_q = target_q * (1 - done_mask)\n",
        "            target_agent_qs.append(filtered_target_q)\n",
        "        target_agent_qs = torch.cat(target_agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
        "\n",
        "        # Compute the total target Q-value using the mixing network\n",
        "        target_q_tot = mixing_network(target_agent_qs, next_states).squeeze(1)   # Shape: (batch_size, 1)\n",
        "    target_q_tot = rewards.sum(dim=2) + gamma * target_q_tot\n",
        "\n",
        "\n",
        "    # Compute the global Q-value using the mixing network\n",
        "    q_tot = mixing_network(agent_qs, states).squeeze(1)  # Shape: (batch_size, 1)\n",
        "\n",
        "    # Compute Huber loss\n",
        "    td_loss = F.huber_loss(target_q_tot, q_tot,  reduction=\"none\")\n",
        "\n",
        "    # Apply weights if prioritized experience replay is used\n",
        "    if weights is not None:\n",
        "        td_loss *= weights\n",
        "\n",
        "    # Return the mean loss\n",
        "    total_loss = td_loss.mean()\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qb_cFvZ-Bz8"
      },
      "source": [
        "## 1.4 Training the QMix Algorithm\n",
        "\n",
        "Now that you have defined the agent Q-networks, the mixing network, and the loss function, it’s time to train the agents in the gym environment.\n",
        "\n",
        "**Task:** Implement the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTtfvR9w-BJQ"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size=10000):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        # Restructure the batch into separate arrays for states, actions, rewards, next_states, and dones\n",
        "        states = np.array([exp[0].cpu().numpy() for exp in experiences], dtype=np.float32)\n",
        "        actions = np.array([exp[1] for exp in experiences], dtype=np.int64)\n",
        "        rewards = np.array([exp[2] for exp in experiences])\n",
        "        next_states = np.array([exp[3].cpu().numpy() for exp in experiences])\n",
        "        dones = np.array([exp[4] for exp in experiences])\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "def epsilon_greedy_action(agent_q_network, state, epsilon, legal_actions):\n",
        "    if random.random() < epsilon:\n",
        "        # Explore: take a random action\n",
        "        action = random.choice(legal_actions)\n",
        "    else:\n",
        "        state = torch.unsqueeze(state.clone().detach(), 0).to(device)\n",
        "        q_values = agent_q_network(state).cpu().detach().numpy()\n",
        "        action = np.random.choice(np.flatnonzero(q_values == q_values.max()))\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def update_target_network(agent_q_networks, target_q_networks):\n",
        "    for target, source in zip(target_q_networks, agent_q_networks):\n",
        "        target.load_state_dict(source.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "\n",
        "class MultiAgentPrioritizedReplayBuffer:\n",
        "    def __init__(self, buffer_size=10000, alpha=0.6, num_agents=2):\n",
        "        \"\"\"\n",
        "        Multi-agent Prioritized Replay Buffer\n",
        "        :param buffer_size: Maximum aantal ervaringen in de buffer\n",
        "        :param alpha: Prioriteitsparameter (0 = geen prioriteit, 1 = volledig prioriteit)\n",
        "        :param num_agents: Aantal agents in de omgeving\n",
        "        \"\"\"\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.alpha = alpha\n",
        "        self.num_agents = num_agents\n",
        "\n",
        "    def add(self, experience):\n",
        "        \"\"\"\n",
        "        Voeg een ervaring toe aan de buffer.\n",
        "        :param experience: Tuple met ervaringen van alle agents\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4, q_networks=None, target_networks=None, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Sample een batch met prioriteitsgebaseerd samplen.\n",
        "        Bereken TD-errors dynamisch voor de gehele buffer.\n",
        "        :param batch_size: Aantal ervaringen in de batch\n",
        "        :param beta: Gewicht voor importance-sampling correctie\n",
        "        :param q_networks: Lijst van Q-netwerken (één per agent)\n",
        "        :param target_networks: Lijst van target Q-netwerken (één per agent)\n",
        "        :param gamma: Kortingfactor\n",
        "        :return: Tuple met gesamplede gegevens en indices\n",
        "        \"\"\"\n",
        "        # Extracteer de data uit de buffer\n",
        "        states, actions, rewards, next_states, terminations = zip(*self.buffer)\n",
        "\n",
        "        # Bereken TD-errors voor alle ervaringen in de buffer\n",
        "        td_errors = self.compute_td_errors(\n",
        "            np.array(states), np.array(actions), np.array(rewards),\n",
        "            np.array(next_states), np.array(terminations),\n",
        "            q_networks, target_networks, gamma\n",
        "        )\n",
        "\n",
        "        # Bereken prioriteiten\n",
        "        priorities = np.mean(np.abs(td_errors), axis=0) + 1e-6  # Gemiddelde prioriteit per ervaring\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        # Sample indices op basis van prioriteiten\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "        experiences = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        # Bereken importance-sampling gewichten\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** -beta\n",
        "        weights /= weights.max()  # Normaliseer\n",
        "\n",
        "        # Herstructureren van de batch\n",
        "        states_batch, actions_batch, rewards_batch, next_states_batch, terminations_batch = zip(*experiences)\n",
        "        states_batch = np.array(states_batch, dtype=np.float32)\n",
        "        actions_batch = np.array(actions_batch, dtype=np.int64)\n",
        "        rewards_batch = np.array(rewards_batch, dtype=np.float32)\n",
        "        next_states_batch = np.array(next_states_batch, dtype=np.float32)\n",
        "        terminations_batch = np.array(terminations_batch, dtype=np.float32)\n",
        "\n",
        "        return states_batch, actions_batch, rewards_batch, next_states_batch, terminations_batch, weights, indices\n",
        "\n",
        "    def compute_td_errors(self, states, actions, rewards, next_states, terminations, q_networks, target_networks, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Bereken TD-errors voor meerdere agents.\n",
        "        :param states: Array van toestanden [batch_size, num_agents, state_shape]\n",
        "        :param actions: Array van acties [batch_size, num_agents]\n",
        "        :param rewards: Array van beloningen [batch_size, num_agents]\n",
        "        :param next_states: Array van volgende toestanden [batch_size, num_agents, state_shape]\n",
        "        :param terminations: Array van terminatie-vlaggen [batch_size, num_agents]\n",
        "        :param q_networks: Lijst van Q-netwerken (één per agent)\n",
        "        :param target_networks: Lijst van target Q-netwerken (één per agent)\n",
        "        :param gamma: Kortingfactor\n",
        "        :return: Array van TD-errors [num_agents, batch_size]\n",
        "        \"\"\"\n",
        "        td_errors = []\n",
        "        for agent_idx in range(len(q_networks)):\n",
        "            # Extract data voor deze agent\n",
        "            state = torch.tensor(states[agent_idx, :,:,:], dtype=torch.float32)\n",
        "            action = torch.tensor(actions[agent_idx], dtype=torch.int64)\n",
        "            reward = torch.tensor(rewards[agent_idx], dtype=torch.float32)\n",
        "            next_state = torch.tensor(next_states[agent_idx, :,:,:], dtype=torch.float32)\n",
        "            termination = torch.tensor(terminations[agent_idx], dtype=torch.float32)\n",
        "\n",
        "            # Bereken Q(s, a) van het hoofdnetwerk\n",
        "            q_values = q_networks[agent_idx](state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Bereken target Q(s', a') met het targetnetwerk\n",
        "            next_q_values = target_networks[agent_idx](next_state).max(1)[0]\n",
        "            targets = reward + gamma * next_q_values * (1 - termination)\n",
        "\n",
        "            # TD-error: δ = Q(s, a) - target\n",
        "            td_error = q_values - targets.detach()\n",
        "            td_errors.append(td_error.cpu().numpy())\n",
        "\n",
        "        return np.array(td_errors)\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"\n",
        "        Retourneer de huidige grootte van de buffer.\n",
        "        \"\"\"\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "xDxjxi5-E-Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xN1Mc_I0Ok4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_qmix(env, agent_q_networks, mixing_network, target_q_networks, replay_buffer, n_episodes=10000,\n",
        "               batch_size=32, gamma=0.9):\n",
        "    optimizer = optim.Adam([param for net in agent_q_networks for param in net.parameters()], lr=0.001)\n",
        "    mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=0.0001)\n",
        "\n",
        "    epsilon = .75 # Initial exploration probability\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_decay = 0.99\n",
        "    target_update_frequency = 20\n",
        "    steps_counter = 0\n",
        "    legal_actions = [0, 1, 2, 3, 4]\n",
        "    agent_indexes = [1, 3]\n",
        "\n",
        "    episode_rewards = []\n",
        "    win_rates = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        done = {agent_id: False for agent_id in agent_indexes}\n",
        "        env.reset()\n",
        "        blue_player1_reward = 0\n",
        "        blue_player2_reward = 0\n",
        "        score = 0\n",
        "        while not all(done.values()):\n",
        "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
        "            states = []\n",
        "            for i, agent_index in enumerate(agent_indexes):\n",
        "                obs_agent = env.get_Observation(agent_index)\n",
        "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
        "                states.append(state)\n",
        "                action = epsilon_greedy_action(agent_q_networks[i], state, epsilon, legal_actions)\n",
        "                actions[agent_index] = action\n",
        "\n",
        "\n",
        "\n",
        "            next_states, rewards, terminations, info = env.step(actions)\n",
        "            score -= info[\"score_change\"]\n",
        "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
        "            blue_player1_reward += rewards[1]\n",
        "            blue_player2_reward += rewards[3]\n",
        "\n",
        "            next_states_converted = []\n",
        "            rewards_converted = []\n",
        "            terminations_converted = []\n",
        "            actions_converted = []\n",
        "\n",
        "            for index in agent_indexes:\n",
        "                next_states_converted.append(list(next_states.values())[index])\n",
        "                rewards_converted.append(rewards[index])\n",
        "                terminations_converted.append(terminations[index])\n",
        "                actions_converted.append(actions[index])\n",
        "\n",
        "            next_states_converted = torch.stack(next_states_converted)\n",
        "            states_converted = torch.stack(states)\n",
        "            rewards_converted = [rewards_converted]\n",
        "            terminations_converted = [terminations_converted]\n",
        "            replay_buffer.add(\n",
        "                (states_converted, actions_converted, rewards_converted, next_states_converted, terminations_converted))\n",
        "\n",
        "            if replay_buffer.size() >= batch_size:\n",
        "                batch = replay_buffer.sample(batch_size)\n",
        "                loss= compute_td_loss(agent_q_networks, mixing_network, target_q_networks, batch,\n",
        "                                               gamma=gamma)\n",
        "                wandb.log({\"loss\": loss})\n",
        "\n",
        "\n",
        "                # Zero gradients for all optimizers\n",
        "                optimizer.zero_grad()\n",
        "                mixing_optimizer.zero_grad()\n",
        "\n",
        "                # Backpropagate once for all losses\n",
        "                loss.backward()\n",
        "\n",
        "                # Update weights\n",
        "                optimizer.step()\n",
        "                mixing_optimizer.step()\n",
        "\n",
        "        steps_counter += 1\n",
        "        wandb.log({\"epsilon\": epsilon})\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        wandb.log({\"blue_player1_reward\": blue_player1_reward})\n",
        "        wandb.log({\"blue_player2_reward\": blue_player2_reward})\n",
        "        wandb.log({\"episode\": episode})\n",
        "        wandb.log({'score': score})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if (episode + 1) % target_update_frequency == 0:\n",
        "            update_target_network(agent_q_networks, target_q_networks)\n",
        "\n",
        "        if (episode + 1) % target_update_frequency == 0:\n",
        "            torch.save(agent_q_networks[0].state_dict(),f'agent1_{episode+1}.pth')\n",
        "            torch.save(agent_q_networks[1].state_dict(),f'agent3_{episode+1}.pth')\n",
        "\n",
        "        # Log rewards and win rates\n",
        "        episode_rewards.append(blue_player1_reward+blue_player2_reward)\n",
        "        print(blue_player1_reward)\n",
        "        print(blue_player2_reward)\n",
        "        print(f\"Episode reward: {blue_player1_reward+blue_player2_reward}\")\n",
        "\n",
        "        if np.sum(env.game.state.getBlueFood().data) == 0:\n",
        "            win_rates.append(1)\n",
        "        else:\n",
        "            win_rates.append(0)\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            avg_win_rate = np.mean(win_rates[-10:])\n",
        "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}, Win Rate: {avg_win_rate:.2f}\")\n",
        "\n",
        "    # Plot rewards and win rates\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(episode_rewards, label=\"Average Reward\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(\"Average Reward per Episode\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    smoothed_win_rates = np.convolve(win_rates, np.ones(10)/10, mode='valid') if len(win_rates) > 10 else win_rates\n",
        "    plt.plot(smoothed_win_rates, label=\"Win Rate (Smoothed)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Win Rate\")\n",
        "    plt.title(\"Win Rate Over Episodes\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrI2Ff3uoJ6J",
        "outputId": "4f8ac6a3-c3e9-4393-9bc7-7fa2531f7f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded layout from file\n",
            "Loading Team: /content//agents/randomTeam.py\n",
            "Arguments: {}\n",
            "Blue team starts\n",
            "[4, 8, 11, 14]\n"
          ]
        }
      ],
      "source": [
        "# Get number of agents and their observation/action space dimensions\n",
        "env = gymPacMan_parallel_env(display=False, layout_file=\"layouts/smallCapture.lay\")\n",
        "n_agents = int(len(env.agents) / 2)\n",
        "action_dim_individual_agent = 5  # North, South, East, West, Stop\n",
        "\n",
        "obs_individual_agent = env.get_Observation(0)\n",
        "obs_shape = obs_individual_agent.shape\n",
        "\n",
        "agent_q_networks = [AgentQNetwork(obs_shape=obs_shape, action_dim=action_dim_individual_agent).to(device) for _ in\n",
        "                    range(n_agents)]\n",
        "target_q_networks = [AgentQNetwork(obs_shape=obs_shape, action_dim=action_dim_individual_agent).to(device) for _ in\n",
        "                     range(n_agents)]\n",
        "\n",
        "# Initialize the mixing network\n",
        "state_shape = (2, 8, env.layout.height, env.layout.width)  # Veronderstelling: de toestand is een concatenatie van agent-observaties\n",
        "global_state_shape = [len(env.agents)] + list(obs_shape)\n",
        "print(global_state_shape)\n",
        "mixing_network = AttentionMixer(n_agents=n_agents, state_shape=state_shape).to(device)\n",
        "\n",
        "# Initialize target Q-networks with the same weights as the main Q-networks\n",
        "update_target_network(agent_q_networks, target_q_networks)\n",
        "\n",
        "# Initialize the replay buffer\n",
        "replay_buffer = ReplayBuffer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK95aBtt0T6U",
        "outputId": "8b427826-7e4a-4ade-8939-f82ee387a963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red team starts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ba10884623e0>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
            "<ipython-input-11-ba10884623e0>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.989999999999981\n",
            "4.989999999999981\n",
            "Episode reward: 9.979999999999961\n",
            "Red team starts\n",
            "3.5699999999999887\n",
            "3.5699999999999887\n",
            "Episode reward: 7.1399999999999775\n",
            "Blue team starts\n",
            "4.549999999999983\n",
            "4.549999999999983\n",
            "Episode reward: 9.099999999999966\n",
            "Blue team starts\n",
            "3.2399999999999913\n",
            "3.2399999999999913\n",
            "Episode reward: 6.479999999999983\n",
            "Red team starts\n",
            "7.82999999999998\n",
            "7.82999999999998\n",
            "Episode reward: 15.65999999999996\n",
            "Blue team starts\n",
            "3.089999999999989\n",
            "3.089999999999989\n",
            "Episode reward: 6.179999999999978\n",
            "Blue team starts\n",
            "3.7999999999999927\n",
            "3.7999999999999927\n",
            "Episode reward: 7.599999999999985\n",
            "Blue team starts\n",
            "2.909999999999992\n",
            "2.909999999999992\n",
            "Episode reward: 5.819999999999984\n",
            "Blue team starts\n",
            "3.0099999999999913\n",
            "3.0099999999999913\n",
            "Episode reward: 6.019999999999983\n",
            "Blue team starts\n",
            "5.09999999999999\n",
            "5.09999999999999\n",
            "Episode reward: 10.19999999999998\n",
            "Episode 10/500, Average Reward: 8.42, Win Rate: 0.00\n",
            "Red team starts\n",
            "4.349999999999988\n",
            "4.349999999999988\n",
            "Episode reward: 8.699999999999976\n",
            "Blue team starts\n",
            "2.7199999999999904\n",
            "2.7199999999999904\n",
            "Episode reward: 5.439999999999981\n",
            "Red team starts\n",
            "2.9199999999999924\n",
            "2.9199999999999924\n",
            "Episode reward: 5.839999999999985\n",
            "Blue team starts\n",
            "3.6099999999999928\n",
            "3.6099999999999928\n",
            "Episode reward: 7.2199999999999855\n",
            "Blue team starts\n",
            "2.6099999999999963\n",
            "2.6099999999999963\n",
            "Episode reward: 5.219999999999993\n",
            "Blue team starts\n",
            "2.8299999999999934\n",
            "2.8299999999999934\n",
            "Episode reward: 5.659999999999987\n",
            "Red team starts\n",
            "2.559999999999994\n",
            "2.559999999999994\n",
            "Episode reward: 5.119999999999988\n",
            "Red team starts\n",
            "2.609999999999994\n",
            "2.609999999999994\n",
            "Episode reward: 5.219999999999988\n",
            "Blue team starts\n",
            "2.349999999999997\n",
            "2.349999999999997\n",
            "Episode reward: 4.699999999999994\n",
            "Red team starts\n",
            "2.4299999999999953\n",
            "2.4299999999999953\n",
            "Episode reward: 4.8599999999999905\n",
            "Episode 20/500, Average Reward: 5.80, Win Rate: 0.00\n",
            "Blue team starts\n",
            "4.709999999999981\n",
            "4.709999999999981\n",
            "Episode reward: 9.419999999999963\n",
            "Blue team starts\n",
            "4.659999999999974\n",
            "4.659999999999974\n",
            "Episode reward: 9.319999999999949\n",
            "Blue team starts\n",
            "2.5099999999999922\n",
            "2.5099999999999922\n",
            "Episode reward: 5.0199999999999845\n",
            "Blue team starts\n",
            "2.559999999999996\n",
            "2.559999999999996\n",
            "Episode reward: 5.119999999999992\n",
            "Red team starts\n",
            "2.4899999999999944\n",
            "2.4899999999999944\n",
            "Episode reward: 4.979999999999989\n",
            "Red team starts\n",
            "3.0599999999999925\n",
            "3.0599999999999925\n",
            "Episode reward: 6.119999999999985\n",
            "Blue team starts\n",
            "4.039999999999986\n",
            "4.039999999999986\n",
            "Episode reward: 8.079999999999972\n",
            "Blue team starts\n",
            "2.6899999999999946\n",
            "2.6899999999999946\n",
            "Episode reward: 5.379999999999989\n",
            "Blue team starts\n",
            "4.189999999999989\n",
            "4.189999999999989\n",
            "Episode reward: 8.379999999999978\n",
            "Red team starts\n",
            "2.2499999999999964\n",
            "2.2499999999999964\n",
            "Episode reward: 4.499999999999993\n",
            "Episode 30/500, Average Reward: 6.63, Win Rate: 0.10\n",
            "Blue team starts\n",
            "2.1400000000000006\n",
            "2.1400000000000006\n",
            "Episode reward: 4.280000000000001\n",
            "Blue team starts\n",
            "3.1499999999999893\n",
            "3.1499999999999893\n",
            "Episode reward: 6.2999999999999785\n",
            "Red team starts\n",
            "3.3599999999999883\n",
            "3.3599999999999883\n",
            "Episode reward: 6.719999999999977\n",
            "Blue team starts\n",
            "3.8099999999999854\n",
            "3.8099999999999854\n",
            "Episode reward: 7.619999999999971\n",
            "Red team starts\n",
            "13.939999999999984\n",
            "13.939999999999984\n",
            "Episode reward: 27.879999999999967\n",
            "Blue team starts\n",
            "2.7999999999999927\n",
            "2.7999999999999927\n",
            "Episode reward: 5.599999999999985\n",
            "Red team starts\n",
            "2.269999999999998\n",
            "2.269999999999998\n",
            "Episode reward: 4.539999999999996\n",
            "Blue team starts\n",
            "2.149999999999999\n",
            "2.149999999999999\n",
            "Episode reward: 4.299999999999998\n",
            "Red team starts\n",
            "2.679999999999995\n",
            "2.679999999999995\n",
            "Episode reward: 5.35999999999999\n",
            "Red team starts\n",
            "3.0999999999999885\n",
            "3.0999999999999885\n",
            "Episode reward: 6.199999999999977\n",
            "Episode 40/500, Average Reward: 7.88, Win Rate: 0.00\n",
            "Blue team starts\n",
            "2.8699999999999917\n",
            "2.8699999999999917\n",
            "Episode reward: 5.739999999999983\n",
            "Red team starts\n",
            "2.709999999999994\n",
            "2.709999999999994\n",
            "Episode reward: 5.419999999999988\n",
            "Blue team starts\n",
            "3.00999999999999\n",
            "3.00999999999999\n",
            "Episode reward: 6.01999999999998\n",
            "Red team starts\n",
            "7.04999999999999\n",
            "7.04999999999999\n",
            "Episode reward: 14.09999999999998\n",
            "Red team starts\n",
            "1.5300000000000011\n",
            "1.5300000000000011\n",
            "Episode reward: 3.0600000000000023\n",
            "Blue team starts\n",
            "1.6100000000000012\n",
            "1.6100000000000012\n",
            "Episode reward: 3.2200000000000024\n",
            "Blue team starts\n",
            "1.9000000000000015\n",
            "1.9000000000000015\n",
            "Episode reward: 3.800000000000003\n",
            "Blue team starts\n",
            "1.6500000000000012\n",
            "1.6500000000000012\n",
            "Episode reward: 3.3000000000000025\n",
            "Red team starts\n",
            "1.8100000000000014\n",
            "1.8100000000000014\n",
            "Episode reward: 3.6200000000000028\n",
            "Blue team starts\n",
            "9.209999999999996\n",
            "9.209999999999996\n",
            "Episode reward: 18.41999999999999\n",
            "Episode 50/500, Average Reward: 6.67, Win Rate: 0.10\n",
            "Red team starts\n",
            "1.8800000000000014\n",
            "1.8800000000000014\n",
            "Episode reward: 3.760000000000003\n",
            "Red team starts\n",
            "3.6299999999999852\n",
            "3.6299999999999852\n",
            "Episode reward: 7.2599999999999705\n",
            "Red team starts\n",
            "5.529999999999978\n",
            "5.529999999999978\n",
            "Episode reward: 11.059999999999956\n",
            "Red team starts\n",
            "1.8800000000000014\n",
            "1.8800000000000014\n",
            "Episode reward: 3.760000000000003\n",
            "Blue team starts\n",
            "1.300000000000001\n",
            "1.300000000000001\n",
            "Episode reward: 2.600000000000002\n",
            "Red team starts\n",
            "1.7900000000000014\n",
            "1.7900000000000014\n",
            "Episode reward: 3.5800000000000027\n",
            "Blue team starts\n",
            "5.729999999999984\n",
            "5.729999999999984\n",
            "Episode reward: 11.459999999999969\n",
            "Blue team starts\n",
            "3.479999999999984\n",
            "3.479999999999984\n",
            "Episode reward: 6.959999999999968\n",
            "Blue team starts\n",
            "1.8200000000000014\n",
            "1.8200000000000014\n",
            "Episode reward: 3.640000000000003\n",
            "Red team starts\n",
            "1.6900000000000013\n",
            "1.6900000000000013\n",
            "Episode reward: 3.3800000000000026\n",
            "Episode 60/500, Average Reward: 5.75, Win Rate: 0.00\n",
            "Blue team starts\n",
            "1.6800000000000013\n",
            "1.6800000000000013\n",
            "Episode reward: 3.3600000000000025\n",
            "Blue team starts\n",
            "1.9800000000000015\n",
            "1.9800000000000015\n",
            "Episode reward: 3.960000000000003\n",
            "Red team starts\n",
            "2.06\n",
            "2.06\n",
            "Episode reward: 4.12\n",
            "Blue team starts\n",
            "1.8700000000000014\n",
            "1.8700000000000014\n",
            "Episode reward: 3.740000000000003\n",
            "Blue team starts\n",
            "3.089999999999996\n",
            "3.089999999999996\n",
            "Episode reward: 6.179999999999992\n",
            "Red team starts\n",
            "1.7800000000000011\n",
            "1.7800000000000011\n",
            "Episode reward: 3.5600000000000023\n",
            "Red team starts\n",
            "3.179999999999985\n",
            "3.179999999999985\n",
            "Episode reward: 6.35999999999997\n",
            "Red team starts\n",
            "2.090000000000001\n",
            "2.090000000000001\n",
            "Episode reward: 4.180000000000002\n",
            "Blue team starts\n",
            "2.010000000000001\n",
            "2.010000000000001\n",
            "Episode reward: 4.020000000000002\n",
            "Red team starts\n",
            "4.119999999999987\n",
            "4.119999999999987\n",
            "Episode reward: 8.239999999999974\n",
            "Episode 70/500, Average Reward: 4.77, Win Rate: 0.00\n",
            "Blue team starts\n",
            "1.5100000000000011\n",
            "1.5100000000000011\n",
            "Episode reward: 3.0200000000000022\n",
            "Blue team starts\n",
            "1.1800000000000008\n",
            "1.1800000000000008\n",
            "Episode reward: 2.3600000000000017\n",
            "Blue team starts\n",
            "1.470000000000001\n",
            "1.470000000000001\n",
            "Episode reward: 2.940000000000002\n",
            "Red team starts\n",
            "1.2000000000000008\n",
            "1.2000000000000008\n",
            "Episode reward: 2.4000000000000017\n",
            "Blue team starts\n",
            "1.1500000000000008\n",
            "1.1500000000000008\n",
            "Episode reward: 2.3000000000000016\n",
            "Blue team starts\n",
            "2.6899999999999893\n",
            "2.6899999999999893\n",
            "Episode reward: 5.379999999999979\n",
            "Blue team starts\n",
            "1.6200000000000012\n",
            "1.6200000000000012\n",
            "Episode reward: 3.2400000000000024\n",
            "Red team starts\n",
            "1.270000000000001\n",
            "1.270000000000001\n",
            "Episode reward: 2.540000000000002\n",
            "Blue team starts\n",
            "1.8600000000000014\n",
            "1.8600000000000014\n",
            "Episode reward: 3.720000000000003\n",
            "Red team starts\n",
            "1.5100000000000011\n",
            "1.5100000000000011\n",
            "Episode reward: 3.0200000000000022\n",
            "Episode 80/500, Average Reward: 3.09, Win Rate: 0.00\n",
            "Red team starts\n",
            "2.589999999999998\n",
            "2.589999999999998\n",
            "Episode reward: 5.179999999999996\n",
            "Blue team starts\n",
            "1.2200000000000009\n",
            "1.2200000000000009\n",
            "Episode reward: 2.4400000000000017\n",
            "Red team starts\n",
            "6.059999999999976\n",
            "6.059999999999976\n",
            "Episode reward: 12.119999999999951\n",
            "Red team starts\n",
            "9.429999999999977\n",
            "9.429999999999977\n",
            "Episode reward: 18.859999999999953\n",
            "Red team starts\n",
            "4.239999999999986\n",
            "4.239999999999986\n",
            "Episode reward: 8.479999999999972\n",
            "Red team starts\n",
            "1.0700000000000007\n",
            "1.0700000000000007\n",
            "Episode reward: 2.1400000000000015\n",
            "Red team starts\n",
            "9.949999999999989\n",
            "9.949999999999989\n",
            "Episode reward: 19.899999999999977\n",
            "Red team starts\n",
            "2.5999999999999908\n",
            "2.5999999999999908\n",
            "Episode reward: 5.1999999999999815\n",
            "Red team starts\n",
            "1.360000000000001\n",
            "1.360000000000001\n",
            "Episode reward: 2.720000000000002\n",
            "Blue team starts\n",
            "1.0700000000000007\n",
            "1.0700000000000007\n",
            "Episode reward: 2.1400000000000015\n",
            "Episode 90/500, Average Reward: 7.92, Win Rate: 0.00\n",
            "Blue team starts\n",
            "0.9600000000000006\n",
            "0.9600000000000006\n",
            "Episode reward: 1.9200000000000013\n",
            "Blue team starts\n",
            "2.139999999999999\n",
            "2.139999999999999\n",
            "Episode reward: 4.279999999999998\n",
            "Blue team starts\n",
            "2.6899999999999915\n",
            "2.6899999999999915\n",
            "Episode reward: 5.379999999999983\n",
            "Blue team starts\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train QMix in the environment\n",
        "train_qmix(env, agent_q_networks, mixing_network, target_q_networks, replay_buffer, n_episodes=500, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "068q7vBcoJ6K"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Function to run the environment with the current policy\n",
        "\n",
        "def run_environment(env, agent_q_networks, n_episodes=1):\n",
        "    epsilon = 0.0 # No exploration during testing\n",
        "    for episode in range(1):\n",
        "        time.sleep(1)\n",
        "        observations, info = env.reset()\n",
        "        state = torch.stack([torch.tensor(obs, dtype=torch.float32).to(device) for obs in observations.values()])\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        print(f\"\\n\\n Episode {episode + 1}/{n_episodes} \\n\\n\")\n",
        "        print(\"Initial state shape:\", [s.shape for s in state])\n",
        "        print(\"Initial state:\", state.shape, \"\\n\")\n",
        "\n",
        "        while not done:\n",
        "            actions = {}\n",
        "\n",
        "            for agent_idx, agent_q_network in enumerate(agent_q_networks):\n",
        "                legal_actions = info['legal_actions'][env.agents[AGENTS_NUM[agent_idx]]]\n",
        "                state_tensor = state[AGENTS_NUM[agent_idx]].unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "                # Debugging the state tensor for each agent\n",
        "                actions[env.agents[AGENTS_NUM[agent_idx]]] = epsilon_greedy_action(agent_q_network, state_tensor, epsilon, legal_actions)\n",
        "\n",
        "\n",
        "\n",
        "            next_observations, rewards, terminations, info = env.step(actions)\n",
        "            next_state = torch.stack([torch.tensor(obs, dtype=torch.float32).to(device) for obs in next_observations.values()])\n",
        "\n",
        "            print(\"Actions taken:\", actions)\n",
        "            print(\"Next state shape:\", [s.shape for s in next_state])\n",
        "            print(\"Rewards:\", rewards)\n",
        "            print(\"Terminations:\", terminations)\n",
        "\n",
        "            state = next_state\n",
        "            done = all(terminations.values())\n",
        "            episode_reward += sum(rewards.values())\n",
        "\n",
        "        print(f\"Episode reward: {episode_reward}\")\n",
        "\n",
        "# Run the environment with the current policy\n",
        "env = gymPacMan_parallel_env(display=True, layout_file=\"layouts/tinyCapture.lay\")\n",
        "print(env.layout.height)\n",
        "print(env.layout.width)\n",
        "run_environment(env, agent_q_networks, n_episodes=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxm-zQIIoJ6K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3qJ2lOZ-Jk6"
      },
      "source": [
        "## 1.5 Reflection Questions\n",
        "\n",
        "Evaluate your results on \"smallCapture.lay\" against random agents and answer the following questions:\n",
        "\n",
        "*  How do your QMix agents improve over time during the training?\n",
        "*  How does the performance of QMix compare to the performance of IQL?\n",
        "*  Do you observe different roles for the agents within a team?\n",
        "*  What other reflection questions can you think of yourself?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "You’ve now implemented the QMix algorithm for the PacMan capture the flag environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7hGqk6LKeEg"
      },
      "source": [
        "# **Section 2: Improving the training strategy**\n",
        "\n",
        "In this section, you will design and implement an improved training strategy for the blue Pacman agents on the map \"bloxCapture.lay\". Don't try to change the architecture too much, to look at the effect of the training strategy. The objective is to train the agents effectively while considering the challenges of multi-agent environments, such as who you play against, exploration, credit assignment, and balancing exploitation with learning stability. You don't have to do everything. Choose some objective you want to study, do some experiments, and interpret your results. You have to do the first topic and at least one more, but feel free to choose how many you do.\n",
        "\n",
        "## 2.1 Understanding Training Challenges\n",
        "\n",
        "Before jumping into implementation, reflect on the following challenges in training multi-agent systems:\n",
        "1. Which enemies?:\n",
        "* When do you play against which enemies? So that it does not become too easy or is not too hard from the start.\n",
        "* How well does self-play perform?\n",
        "2.\tExploration vs. Exploitation:\n",
        "* How will you encourage the agents to explore diverse strategies without overcommitting to suboptimal behaviors early on?\n",
        "* Will you use an exploration strategy like epsilon-greedy, where agents take random actions with some probability, or another method such as Boltzmann exploration?\n",
        "* Should the exploration strategy be adaptive, changing over time as the agents learn?\n",
        "4.\tStability of Learning:\n",
        "* What techniques can you use to avoid instabilities like divergent Q-values or poor convergence?\n",
        "5.\tCoordination Between Agents:\n",
        "* How do you ensure the agents learn coordinated policies rather than acting independently?\n",
        "6.\tReward Shaping and Credit Assignment:\n",
        "* How can you ensure that each agent is rewarded fairly for their contributions to the team’s success?\n",
        "* Should you use a dense reward (frequent small rewards) or a sparse reward (large rewards at key events)?\n",
        "7.\tReplay Buffer Usage\n",
        "* How can you ensure the replay buffer stores diverse experiences, and how frequently will you sample from it? And which experiences do you sample more than others?\n",
        "\n",
        "## 2.2 Reflection Questions\n",
        "\n",
        "*  For each training challenge, what have you tried, and why?  \n",
        "*   Did your agents improve, and how do you know?\n",
        "*   What trends do you observe in the rewards over episodes, how does the score and loss evolve?\n",
        "* If the agents are not learning effectively, what modifications would you make to your strategy?\n",
        "* What other reflection questions can you think of yourself?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGslIWIeMWSS"
      },
      "source": [
        "# **Section 3: Experimenting with Your Own Algorithm**\n",
        "\n",
        "In this section, you are encouraged to explore beyond the QMix algorithm. The goal is for you to select a different multi-agent reinforcement learning algorithm or propose your own modifications to QMix. You will implement and test this algorithm on the Pacman Capture the Flag environment on the map \"bloxCapture.lay\". Finally, also try train your algorithm on random maps.\n",
        "\n",
        "This section is open-ended, allowing you to experiment and think critically about the challenges and opportunities in multi-agent learning.\n",
        "## 3.1 Suggested Directions\n",
        "\n",
        "Here are some ideas to get you started. You may choose one of these or propose a completely new direction:\n",
        "1.\tPolicy Gradient Approaches\n",
        "\n",
        "* Implement a multi-agent Proximal Policy Optimization (PPO) or Actor-Critic algorithm.\n",
        "* How do policy gradient methods handle coordination between agents compared to value-based methods like QMix?\n",
        "2.\tCounterfactual Multi-Agent Policy Gradients (COMA)\n",
        "* Explore COMA, which uses counterfactual baselines to address the credit assignment problem.\n",
        "* How does COMA adjust the contribution of each agent to the team’s reward?\n",
        "3.\tModifications to QMix\n",
        "* Experiment with a different mixer architecture. For example, add attention mechanisms or use more complex neural networks for mixing Q-values.\n",
        "* Modify the reward function to encourage specific behaviors like offense or defense in the game.\n",
        "4.\tHybrid Approaches\n",
        "* Combine value-based and policy-gradient methods, such as integrating PPO with QMix.\n",
        "5. Have an idea for something else, go right ahead!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sajuh8nfMkTW"
      },
      "source": [
        "## 3.2 Reflection Questions\n",
        "\n",
        "After implementing your chosen algorithm, reflect on the following:\n",
        "\n",
        "1. Design choices\n",
        "* Which things did you implement, and why?\n",
        "* Explain your training strategies.\n",
        "\n",
        "2.\tPerformance\n",
        "* How does your algorithm perform compared to standard QMix?\n",
        "\n",
        "3.\tStrengths and Weaknesses\n",
        "* What are the strengths of your chosen approach in the multi-agent Pacman environment?\n",
        "* What are the weaknesses or challenges you encountered?\n",
        "\n",
        "4.\tCoordination\n",
        "* Did your algorithm encourage better coordination between agents? Why or why not?\n",
        "\n",
        "5.\tGeneralization\n",
        "* How well do you think your algorithm generalizes to random maps? Did you have to change something to make this work?\n",
        "\n",
        "What other reflection questions can you think of yourself?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkRulQ_cMn-g"
      },
      "outputs": [],
      "source": [
        "## mogelijke upgrade buffer\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, buffer_size=10000, alpha=0.6):\n",
        "        \"\"\"\n",
        "        Prioritized Replay Buffer\n",
        "        :param buffer_size: Maximum number of experiences in the buffer\n",
        "        :param alpha: Degree of prioritization (0 = no prioritization, 1 = full prioritization)\n",
        "        \"\"\"\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.priorities = deque(maxlen=buffer_size)  # Stores priorities of experiences\n",
        "        self.alpha = alpha  # Controls the level of prioritization\n",
        "\n",
        "    def add(self, experience, priority=1.0):\n",
        "        \"\"\"\n",
        "        Add an experience to the buffer with an initial priority.\n",
        "        :param experience: Tuple (state, action, reward, next_state, done)\n",
        "        :param priority: Initial priority of the experience\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "        self.priorities.append(priority)\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        \"\"\"\n",
        "        Sample a batch of experiences, using prioritized sampling.\n",
        "        :param batch_size: Number of experiences to sample\n",
        "        :param beta: Degree of importance-sampling correction (0 = no correction, 1 = full correction)\n",
        "        :return: Tuple (states, actions, rewards, next_states, dones, weights, indices)\n",
        "        \"\"\"\n",
        "        # Compute sampling probabilities\n",
        "        priorities = np.array(self.priorities, dtype=np.float32)\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        # Sample indices based on priorities\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "        experiences = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        # Importance-sampling weights\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** -beta\n",
        "        weights /= weights.max()  # Normalize weights to [0, 1]\n",
        "\n",
        "        # Restructure the batch\n",
        "        states = np.array([exp[0].cpu().numpy() for exp in experiences], dtype=np.float32)\n",
        "        actions = np.array([exp[1] for exp in experiences], dtype=np.int64)\n",
        "        rewards = np.array([exp[2] for exp in experiences])\n",
        "        next_states = np.array([exp[3].cpu().numpy() for exp in experiences])\n",
        "        dones = np.array([exp[4] for exp in experiences])\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, weights, indices\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        \"\"\"\n",
        "        Update the priorities of sampled experiences.\n",
        "        :param indices: Indices of the sampled experiences\n",
        "        :param priorities: New priorities for these experiences\n",
        "        \"\"\"\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "5o7eoBGNa9rO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}