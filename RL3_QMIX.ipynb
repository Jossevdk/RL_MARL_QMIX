{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jossevdk/RL_MARL_QMIX/blob/main/RL3_QMIX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTT57uLHmIVf"
   },
   "source": [
    "\n",
    "# **Disclaimer**\n",
    "This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intSkgKwmUZm"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "\n",
    "Welcome to the third assignment of the Reinforcement Learning course! In this assignment, you will implement and train a QMix algorithm for a multi-agent environment. The task takes place in a Pacman Capture the Flag game, where you control two blue Pacman agents working together to capture food and avoid red Pacman opponents.\n",
    "\n",
    "This assignment is divided into three sections:\n",
    "\n",
    "\n",
    "1.  Implementing QMix: You will construct the QMix architecture to combine individual agent Q-values into a global Q-value. Training the Agents: You will implement the training loop, including the QMix loss function. Getting it working here is key, don't do much too fast.\n",
    "2.   Try to improve your agents, with different training strategies. Don't experiment too much with new architectures, that will come later!\n",
    "3. \tExploring Beyond QMix: You will choose and implement another multi-agent RL algorithm and compare its performance with QMix.\n",
    "\n",
    "During this assignment, there will be four tournaments conducted in a round-robin format, where each team will compete against others:\n",
    "-\tIntermediate Tournaments (3): These serve as checkpoints to test your models and evaluate your training strategies.\n",
    "-\tFinal Tournament (1): This determines the ultimate winner! Who will forever be in the Hall of Fame?\n",
    "\n",
    "The first tournament will be on “bloxCapture.lay” and depending on the progress of the teams, the next tournaments will use more maps. For more information about the submission and deadlines check the assignment. The tournament results **will not impact your grade**.\n",
    "\n",
    "\n",
    "By the end of this assignment, you will gain practical experience in implementing and experimenting with multi-agent reinforcement learning algorithms!\n",
    "\n",
    "## Environment: [PacMan, a capture the flag variant ](https://ai.berkeley.edu/contest.html)\n",
    "\n",
    "The environment you'll be working in, PacMan Capture the Flag, is a multi-agent version of the classic PacMan game. It involves multiple agents that need to cooperate to achieve a shared objective. Each agent can only observe part of the environment(although this is close to the full environment) and needs to act based on its local observations, making this an ideal setup for exploring cooperative multi-agent reinforcement learning.\n",
    "The game logic comes from [here](https://ai.berkeley.edu/contest.html), the code is not to hard understand if you want to delve a little bit deeper. Additional to this base game you will also find some additional code so that the environment is more useable like other gym environemnts. This makes it easier to train your deep RL multi agent algorithms.\n",
    "You'll also see a display option in the code. You can visualize games quite easily, however for this we recommend, copying the notebook and running it locally.  \n",
    "\n",
    "**For more information on the environment check the additional file**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IBmjgcct1f9"
   },
   "source": [
    "\n",
    "# **Setup**\n",
    "\n",
    "Before we dive into coding, let’s make sure everything is set up correctly.\n",
    "\n",
    "1. Install Dependencies\n",
    "\n",
    "You’ll need to install the following libraries to run the notebook. Run the cell below to install them:\n",
    "\n",
    "\n",
    "*   PacMan Capture the Flag: a reinforcement learning environments.\n",
    "*   Packages you'll use throught the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoP_9-CHfydw",
    "outputId": "98573555-4fc3-4e4d-f9a4-bd9fb35de759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in C:/Users/josse/OneDrive/Documenten/creditcontract/RL/RL_MARL_QMIX/.git/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'rl-lab-3-pacman' already exists and is not an empty directory.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!git init\n",
    "!git clone https://student:fFYKVqdc976PXAuyhyz4@gitlab.ilabt.imec.be/emalomgr/rl-lab-3-pacman.git --branch student_version\n",
    "!mv ./rl-lab-3-pacman/* ./\n",
    "!rm -r ./rl-lab-3-pacman/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1-5e7HR3saX"
   },
   "source": [
    "2. Import Necessary Libraries\n",
    "\n",
    "Once the dependencies are installed, import the key libraries you’ll need throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaNmbYmYrdGq",
    "outputId": "cf8d951e-2a2b-4483-cdeb-0d2c916085e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from gymPacMan import gymPacMan_parallel_env\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "collapsed": true,
    "id": "00mnpkAvlOSB",
    "outputId": "96e3a491-5ec4-4e78-98b6-d4bab0b07e28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Blue team starts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# possible options: env = gymPacMan_parallel_env(layout_file = f'{file_path}/layouts/smallCapture.lay', display = False, length = 800, reward_forLegalAction= True, defenceReward = True, random_layout = True, enemieName = 'randomTeam', self_play = False)\n",
    "env = gymPacMan_parallel_env(layout_file='./layouts/tinyCapture.lay', display=False, length=300, enemieName='randomTeam.py')\n",
    "obs, _ = env.reset()\n",
    "\n",
    "\n",
    "# Start a new wandb run to track this script\n",
    "\n",
    "wandb.login(key = '', relogin = True)\n",
    "name_experiment =''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 20, 20])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "torch.Size([8, 10, 11])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 11, 14])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 7, 20])\n",
      "0\n",
      "Loaded random layout\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 20, 20])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "torch.Size([8, 9, 10])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "torch.Size([8, 11, 17])\n"
     ]
    }
   ],
   "source": [
    "layout = ['bloxCapture.lay','defence.lay','smallCapture.lay','tinyCapture.lay','verdedeging2.lay','win.lay','win2.lay']\n",
    "for lay in layout:\n",
    "    env = gymPacMan_parallel_env(layout_file=f'./layouts/{lay}', display=False, length=300, enemieName='randomTeam.py')\n",
    "    print(env.get_Observation(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjgyVEbp8sow"
   },
   "source": [
    "# **Section 1: QMix Implementation**\n",
    "\n",
    "In this section, you will implement the QMix algorithm to control agents in the PacMan environment. QMix is a powerful algorithm in multi-agent reinforcement learning that allows for centralized training with decentralized execution. The key idea behind QMix is to learn a mixing network that combines individual agent Q-values into a global Q-value, which allows agents to make coordinated decisions while still acting independently during execution.\n",
    "\n",
    "## QMix Theory Overview\n",
    "\n",
    "QMix is a value-based multi-agent reinforcement learning algorithm designed for cooperative tasks. It addresses the challenge of decentralized control while maintaining a centralized training framework. The key idea is to learn individual Q-values for each agent and combine them into a global Q-value that represents the team’s joint policy.\n",
    "\n",
    "Core Concepts:\n",
    "1.\tIndividual Q-Values: Each agent has a separate Q-network that predicts the Q-values for its actions based on its local observations.\n",
    "2.\tGlobal Q-Value: A mixer network aggregates the individual Q-values into a global Q-value, ensuring that the global Q-value is monotonic with respect to individual Q-values. This monotonicity ensures that maximizing the global Q-value aligns with maximizing the individual Q-values.\n",
    "3.\tHypernetworks: QMix uses hypernetworks to generate the weights for the mixer network dynamically. These weights depend on the global state, allowing the mixer network to adapt its behavior based on the team’s overall situation.\n",
    "Step-by-Step Implementation\n",
    "\n",
    "You will be implementing QMix step by step, focusing on the following parts:\n",
    "\n",
    "1.\tImplement the individual agent Q-networks.\n",
    "2.\tBuild the mixing network to combine individual Q-values.\n",
    "3.\tSet up the loss function and training loop.\n",
    "4.\tTrain the agents in the PacMan environment.\n",
    "\n",
    "##Let’s begin!\n",
    "\n",
    "## 1.1   Agent Q-Network Implementation\n",
    "\n",
    "Each agent in the QMix algorithm has its own Q-network that estimates the action-value function based on the agent’s local observation. The Q-network takes the agent’s observation as input and outputs the Q-values for each possible action.\n",
    "\n",
    "**Task:** Implement IQL.\n",
    "\n",
    "**Keep it simple**\n",
    "\n",
    "Don't use too advanced architecures from the start and train against random agents on a small map. Start on \"tinyCapture.lay\" and move on to \"smallCapture.lay\" when you see your algorithm works This way you can see quicker whether your algorithm is working.\n",
    "\n",
    "**Check if the agents learn without QMix**\n",
    "\n",
    "Do a run with IQL without the mixing network and another run with QMix and reflect on the improvements in the performance of QMix.\n",
    "\n",
    "**Evaluate during training**\n",
    "\n",
    "Try to evaluate your agents while you train, so you can see faster if they learn or not. A good tool for this is WANDB, where you can easily compare different stragegies during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "QnE1LUgR0hzY"
   },
   "outputs": [],
   "source": [
    "class AgentQNetwork(nn.Module):\n",
    "    def __init__(self,obs_shape, action_dim,hidden_dim=64):\n",
    "        super(AgentQNetwork, self).__init__()\n",
    "        self.c1 = nn.Conv2d(obs_shape[0], 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.c2 = nn.Conv2d(32, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.c3 = nn.Conv2d(hidden_dim, 2*hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.c4 = nn.Conv2d(2*hidden_dim, 4*hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.c1(obs))\n",
    "        x = torch.relu(self.c2(x))\n",
    "        x = torch.relu(self.c3(x))\n",
    "        x = torch.relu(self.c4(x))\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AgentQNetwork2(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, hidden_dim=64):\n",
    "        super(AgentQNetwork2, self).__init__()\n",
    "        \n",
    "        self.c1 = nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=1, padding='same')\n",
    "        self.c2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n",
    "        self.c3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64,32)\n",
    "        self.fc2 = nn.Linear(32, action_dim)\n",
    "        \n",
    "        self.activation = nn.SiLU()  # Use SiLU as the activation function\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.activation(self.bn1(self.c1(obs)))\n",
    "        x = self.activation(self.bn2(self.c2(x)))\n",
    "        x = self.activation(self.bn3(self.c3(x)))\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTransformerQNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, d_model=32, n_heads=2, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten(2)  # (batch, channels, H*W)\n",
    "        self.token_proj = nn.Linear(32, d_model)  # Project to transformer tokens\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, n_heads),\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, action_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.cnn(obs)\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = self.flatten(x).permute(0, 2, 1)  # Tokens: (batch, H*W, channels)\n",
    "        x = self.token_proj(x)  # Project tokens\n",
    "        x = self.transformer(x)  # Apply transformer\n",
    "        x = x.mean(dim=1)  # Pool tokens\n",
    "        q_values = self.fc(x)\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dYpvxTc8SUh"
   },
   "source": [
    "## test for IQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H9qtVQS81ds"
   },
   "source": [
    "\t•\tobs_dim: The dimension of the agent’s local observation.\n",
    "\t•\taction_dim: The number of possible actions the agent can take.\n",
    "\n",
    "## 1.2 Mixing Network\n",
    "\n",
    "The mixing network is responsible for combining the individual Q-values from each agent into a global Q-value. The mixing network ensures that the global Q-value is a monotonic function of each agent’s Q-value, which allows the system to maintain decentralized decision-making at runtime.\n",
    "\n",
    "**Task:** Implement the mixing network.\n",
    "\n",
    "The mixing network will take the Q-values of all agents as input and output a single global Q-value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "1aIiC5gD8yZT"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class SimpleQMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape):\n",
    "        super(SimpleQMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.state_shape = state_shape\n",
    "        print(state_shape)\n",
    "        # Much simpler state processing\n",
    "        self.state_conv = nn.Sequential(\n",
    "            nn.Conv3d(state_shape[0], 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * state_shape[1] * state_shape[2] * state_shape[3],  n_agents)  # Output: (batch_size, 32 * n_agents)\n",
    "        )\n",
    "        # Initialize weights positively\n",
    "        for m in self.state_conv:\n",
    "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "        # Single layer mixing network\n",
    "        self.mixing_layer = nn.Linear(n_agents , 1, bias=False)\n",
    "\n",
    "        # Initialize close to equal weights\n",
    "        nn.init.uniform_(self.mixing_layer.weight, 0.9, 1.1)\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "\n",
    "        bs = agent_qs.size(0)\n",
    "        weights = F.softplus(self.state_conv(states))\n",
    "\n",
    "\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "\n",
    "        weighted_qs = agent_qs * weights.unsqueeze(1)\n",
    "\n",
    "        q_tot = self.mixing_layer(weighted_qs.squeeze(1))\n",
    "        return q_tot.view(bs, -1, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "8E-mSFq9gIq-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape=(2, 8, 10, 20), embed_dim=32, n_heads=4):\n",
    "        super(AttentionMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.state_shape = state_shape[1:]  # Remove agent dimension\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Shared Conv2D encoder for each agent\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Conv2d(self.state_shape[0], 16, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to project to embed_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, embed_dim),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "        for m in self.shared_encoder:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "        for m in self.fc:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        # Multi-head attention for inter-agent interaction\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads, batch_first=True)\n",
    "\n",
    "        # Hypernetwork for mixing weights and biases\n",
    "        self.hyper_w = nn.Sequential(\n",
    "            nn.Linear(embed_dim * n_agents, embed_dim * embed_dim),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Linear(embed_dim * embed_dim, n_agents * embed_dim)\n",
    "        )\n",
    "        self.hyper_b = nn.Sequential(\n",
    "            nn.Linear(embed_dim * n_agents, embed_dim),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Final layer to compute Q_tot\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "        for m in self.hyper_w:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        for m in self.hyper_b:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        for m in self.final_layer:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "        batch_size = states.size(0)\n",
    "\n",
    "        # Process each agent's state individually using the shared encoder\n",
    "        agent_features = []\n",
    "        for i in range(self.n_agents):\n",
    "            agent_state = states[:, i, :, :, :]  # Shape: [batch_size, 8, 10, 20]\n",
    "            encoded_state = self.shared_encoder(agent_state)  # Encode individual agent\n",
    "            encoded_state = encoded_state.view(batch_size, -1)  # Flatten to [batch_size, 128]\n",
    "            projected_state = self.fc(encoded_state)  # Project to embed_dim\n",
    "            agent_features.append(projected_state)\n",
    "\n",
    "        # Stack agent features: Shape [batch_size, n_agents, embed_dim]\n",
    "        agent_features = torch.stack(agent_features, dim=1)\n",
    "        # Apply attention to model inter-agent interaction\n",
    "        attention_out, _ = self.attention(agent_features, agent_features, agent_features)\n",
    "\n",
    "        # Flatten attention output for hypernetwork\n",
    "        attention_out_flat = attention_out.reshape(batch_size, -1)  # Shape: [batch_size, n_agents * embed_dim]\n",
    "\n",
    "        # Compute hypernetwork weights and biases\n",
    "        hyper_w = F.softplus(self.hyper_w(attention_out_flat).view(batch_size, self.n_agents, self.embed_dim))\n",
    "        hyper_b = self.hyper_b(attention_out_flat).view(batch_size, self.embed_dim)\n",
    "\n",
    "        # Combine agent Q-values with weights and biases\n",
    "        agent_qs = agent_qs.view(batch_size, self.n_agents, 1)  # Shape: [batch_size, n_agents, 1]\n",
    "        weighted_qs = torch.bmm(agent_qs.transpose(1, 2), hyper_w).squeeze(1) + hyper_b\n",
    "\n",
    "        # Final projection to Q_tot\n",
    "        q_tot = self.final_layer(weighted_qs).unsqueeze(1)\n",
    "        return q_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OUjn7xq88V6"
   },
   "source": [
    "\t•\tstate_dim: The dimension of the global state (available during centralized training).\n",
    "\t•\tn_agents: The number of agents, which determines the number of Q-values being mixed.\n",
    "\t•\tWeights and biases: The weights and biases of the mixing network depend on the global state, ensuring that different states lead to different weightings of agent Q-values.\n",
    "\n",
    "## 1.3 Loss Function and Training Loop\n",
    "\n",
    "The agents need to learn their Q-values by minimizing the Temporal Difference (TD) error. The loss is computed as the difference between the predicted Q-value (from the agent’s Q-network) and the target Q-value (computed using the Bellman equation).\n",
    "\n",
    "**Task:** Set up the loss function and implement the training loop for QMix.\n",
    "\n",
    "**Hint:** Use the huber loss for more stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2c2BYMg84p3"
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(agent_q_networks, mixing_network, target_q_networks, batch, weights = None, gamma=0.99, lambda_=0.1):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "     # Get current Q-values for each agent\n",
    "    agent_qs = []\n",
    "    for i, agent_q_network in enumerate(agent_q_networks):\n",
    "        agent_q = agent_q_network(states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "        agent_q = agent_q.gather(dim=1, index=actions[:,i].unsqueeze(1))  # Select action Q-values\n",
    "        agent_qs.append(agent_q)\n",
    "    agent_qs = torch.cat(agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
    "    # Get target Q-values for each agent using Double DQN\n",
    "    with torch.no_grad():\n",
    "        target_agent_qs = []\n",
    "        for i, (agent_q_network, target_q_network) in enumerate(\n",
    "            zip(agent_q_networks, target_q_networks)\n",
    "        ):\n",
    "            # Double DQN: Get greedy actions from the current network\n",
    "            next_q_values = agent_q_network(next_states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "            greedy_actions = torch.argmax(next_q_values, dim=1, keepdim=True)\n",
    "            # Get target Q-values using the target network\n",
    "            target_q_values = target_q_network(next_states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "            target_q = target_q_values.gather(1, greedy_actions)\n",
    "            done_mask = dones[:, 0, 0].unsqueeze(1)\n",
    "            filtered_target_q = target_q * (1 - done_mask)\n",
    "            target_agent_qs.append(filtered_target_q)\n",
    "        target_agent_qs = torch.cat(target_agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
    "\n",
    "        # Compute the total target Q-value using the mixing network\n",
    "        target_q_tot = mixing_network(target_agent_qs, next_states).squeeze(1)   # Shape: (batch_size, 1)\n",
    "    target_q_tot = rewards.sum(dim=2) + gamma * target_q_tot\n",
    "\n",
    "    \n",
    "    # Compute the global Q-value using the mixing network\n",
    "    q_tot = mixing_network(agent_qs, states).squeeze(1)  # Shape: (batch_size, 1)\n",
    "    # Compute Huber loss\n",
    "    td_loss = F.huber_loss(target_q_tot, q_tot,  reduction=\"none\")\n",
    "\n",
    "    # Apply weights if prioritized experience replay is used\n",
    "    if weights is not None:\n",
    "        td_loss *= weights\n",
    "\n",
    "    # Return the mean loss\n",
    "    total_loss = td_loss.mean()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qb_cFvZ-Bz8"
   },
   "source": [
    "## 1.4 Training the QMix Algorithm\n",
    "\n",
    "Now that you have defined the agent Q-networks, the mixing network, and the loss function, it’s time to train the agents in the gym environment.\n",
    "\n",
    "**Task:** Implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "OTtfvR9w-BJQ"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.state_shape = (2,8, 20, 20)\n",
    "\n",
    "    def add(self, experience):\n",
    "        # Zorg dat de states en next_states naar de CPU worden gebracht en als numpy worden opgeslagen\n",
    "        padded_state = self._pad_to_shape(experience[0].cpu().numpy(), self.state_shape)\n",
    "        padded_next_state = self._pad_to_shape(experience[3].cpu().numpy(), self.state_shape)\n",
    "\n",
    "        # Maak een nieuwe ervaring met de gepadde states\n",
    "        padded_experience = (\n",
    "            padded_state,\n",
    "            experience[1],  # Actie blijft ongewijzigd\n",
    "            experience[2],  # Reward blijft ongewijzigd\n",
    "            padded_next_state,\n",
    "            experience[4],  # Done blijft ongewijzigd\n",
    "        )\n",
    "        self.buffer.append(padded_experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Restructure the batch into separate arrays\n",
    "        states = np.array([exp[0] for exp in experiences], dtype=np.float32)\n",
    "        actions = np.array([exp[1] for exp in experiences], dtype=np.int64)\n",
    "        rewards = np.array([exp[2] for exp in experiences], dtype=np.float32)\n",
    "        next_states = np.array([exp[3] for exp in experiences], dtype=np.float32)\n",
    "        dones = np.array([exp[4] for exp in experiences], dtype=np.float32)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def _pad_to_shape(self, state, target_shape):\n",
    "        # Zorg ervoor dat de state dezelfde vorm heeft als target_shape\n",
    "        pad_width = [(0, max(0, target_dim - current_dim)) \n",
    "                     for current_dim, target_dim in zip(state.shape, target_shape)]\n",
    "        return np.pad(state, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "\n",
    "def epsilon_greedy_action(agent_q_network, state, epsilon, legal_actions):\n",
    "    if random.random() < epsilon:\n",
    "        # Explore: take a random action\n",
    "        action = random.choice(legal_actions)\n",
    "    else:\n",
    "        state = torch.unsqueeze(state.clone().detach(), 0).to(device)\n",
    "        q_values = agent_q_network(state).cpu().detach().numpy()\n",
    "        action = np.random.choice(np.flatnonzero(q_values == q_values.max()))\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_target_network(agent_q_networks, target_q_networks):\n",
    "    for target, source in zip(target_q_networks, agent_q_networks):\n",
    "        target.load_state_dict(source.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "xDxjxi5-E-Kd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "class MultiAgentPrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000, alpha=0.6, num_agents=2):\n",
    "        \"\"\"\n",
    "        Multi-agent Prioritized Replay Buffer\n",
    "        :param buffer_size: Maximum aantal ervaringen in de buffer\n",
    "        :param alpha: Prioriteitsparameter (0 = geen prioriteit, 1 = volledig prioriteit)\n",
    "        :param num_agents: Aantal agents in de omgeving\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.alpha = alpha\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"\n",
    "        Voeg een ervaring toe aan de buffer.\n",
    "        :param experience: Tuple met ervaringen van alle agents\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4, q_networks=None, target_networks=None, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Sample een batch met prioriteitsgebaseerd samplen.\n",
    "        Bereken TD-errors dynamisch voor de gehele buffer.\n",
    "        :param batch_size: Aantal ervaringen in de batch\n",
    "        :param beta: Gewicht voor importance-sampling correctie\n",
    "        :param q_networks: Lijst van Q-netwerken (één per agent)\n",
    "        :param target_networks: Lijst van target Q-netwerken (één per agent)\n",
    "        :param gamma: Kortingfactor\n",
    "        :return: Tuple met gesamplede gegevens en indices\n",
    "        \"\"\"\n",
    "        # Extracteer de data uit de buffer\n",
    "        states, actions, rewards, next_states, terminations = zip(*self.buffer)\n",
    "\n",
    "        # Bereken TD-errors voor alle ervaringen in de buffer\n",
    "        td_errors = self.compute_td_errors(\n",
    "            np.array(states), np.array(actions), np.array(rewards),\n",
    "            np.array(next_states), np.array(terminations),\n",
    "            q_networks, target_networks, gamma\n",
    "        )\n",
    "\n",
    "        # Bereken prioriteiten\n",
    "        priorities = np.mean(np.abs(td_errors), axis=0) + 1e-6  # Gemiddelde prioriteit per ervaring\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        # Sample indices op basis van prioriteiten\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # Bereken importance-sampling gewichten\n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** -beta\n",
    "        weights /= weights.max()  # Normaliseer\n",
    "\n",
    "        # Herstructureren van de batch\n",
    "        states_batch, actions_batch, rewards_batch, next_states_batch, terminations_batch = zip(*experiences)\n",
    "        states_batch = np.array(states_batch, dtype=np.float32)\n",
    "        actions_batch = np.array(actions_batch, dtype=np.int64)\n",
    "        rewards_batch = np.array(rewards_batch, dtype=np.float32)\n",
    "        next_states_batch = np.array(next_states_batch, dtype=np.float32)\n",
    "        terminations_batch = np.array(terminations_batch, dtype=np.float32)\n",
    "\n",
    "        return states_batch, actions_batch, rewards_batch, next_states_batch, terminations_batch, weights, indices\n",
    "\n",
    "    def compute_td_errors(self, states, actions, rewards, next_states, terminations, q_networks, target_networks, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Bereken TD-errors voor meerdere agents.\n",
    "        :param states: Array van toestanden [batch_size, num_agents, state_shape]\n",
    "        :param actions: Array van acties [batch_size, num_agents]\n",
    "        :param rewards: Array van beloningen [batch_size, num_agents]\n",
    "        :param next_states: Array van volgende toestanden [batch_size, num_agents, state_shape]\n",
    "        :param terminations: Array van terminatie-vlaggen [batch_size, num_agents]\n",
    "        :param q_networks: Lijst van Q-netwerken (één per agent)\n",
    "        :param target_networks: Lijst van target Q-netwerken (één per agent)\n",
    "        :param gamma: Kortingfactor\n",
    "        :return: Array van TD-errors [num_agents, batch_size]\n",
    "        \"\"\"\n",
    "        td_errors = []\n",
    "        for agent_idx in range(len(q_networks)):\n",
    "            # Extract data voor deze agent\n",
    "            state = torch.tensor(states[agent_idx, :,:,:], dtype=torch.float32)\n",
    "            action = torch.tensor(actions[agent_idx], dtype=torch.int64)\n",
    "            reward = torch.tensor(rewards[agent_idx], dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_states[agent_idx, :,:,:], dtype=torch.float32)\n",
    "            termination = torch.tensor(terminations[agent_idx], dtype=torch.float32)\n",
    "\n",
    "            # Bereken Q(s, a) van het hoofdnetwerk\n",
    "            q_values = q_networks[agent_idx](state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Bereken target Q(s', a') met het targetnetwerk\n",
    "            next_q_values = target_networks[agent_idx](next_state).max(1)[0]\n",
    "            targets = reward + gamma * next_q_values * (1 - termination)\n",
    "\n",
    "            # TD-error: δ = Q(s, a) - target\n",
    "            td_error = q_values - targets.detach()\n",
    "            td_errors.append(td_error.cpu().numpy())\n",
    "\n",
    "        return np.array(td_errors)\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Retourneer de huidige grootte van de buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_weights(start_weights, end_weights, factor):\n",
    "    return [(1 - factor) * start + factor * end for start, end in zip(start_weights, end_weights)]\n",
    "\n",
    "def get_dynamic_weights(episode, n_episodes):\n",
    "    # Define the initial weights for the beginning, middle, and end\n",
    "    weights_beginning = [0.8, 0.05, 0.05, 0.05, 0.025, 0.025]\n",
    "    weights_middle = [0.2, 0.25, 0.25, 0.25, 0.025, 0.025]\n",
    "    weights_end = [0.02, 0.06, 0.06, 0.06, 0.4, 0.4]\n",
    "\n",
    "    # Calculate the transition points\n",
    "    transition1 = n_episodes // 3\n",
    "    transition2 = 2 * n_episodes // 3\n",
    "\n",
    "    # Adjust weights based on the current episode\n",
    "    if episode < transition1:\n",
    "        factor = episode / transition1\n",
    "        return interpolate_weights(weights_beginning, weights_middle, factor)\n",
    "    elif episode < transition2:\n",
    "        factor = (episode - transition1) / (transition2 - transition1)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    else:\n",
    "        factor = (episode - transition2) / (n_episodes - transition2)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    \n",
    "\n",
    "import os\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5xN1Mc_I0Ok4"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def train_qmix(env, name_experiment, agent_q_networks, mixing_network, target_q_networks, replay_buffer, train_p, learn_p, random_enemy = False):\n",
    "    n_episodes = train_p.n_episodes\n",
    "    batch_size = train_p.batch_size\n",
    "    epsilon = train_p.epsilon\n",
    "    epsilon_min = train_p.epsilon_min\n",
    "    epsilon_decay = train_p.epsilon_decay\n",
    "    gamma = train_p.gamma\n",
    "    target_update_frequency = train_p.target_update_frequency\n",
    "\n",
    "    #sequence_length = 5\n",
    "\n",
    "    lr_agent = learn_p.lr_agent\n",
    "    lr_mix = learn_p.lr_mix\n",
    "    learn_gamma = learn_p.gamma\n",
    "\n",
    "    optimizer = optim.Adam([param for net in agent_q_networks for param in net.parameters()], lr=lr_agent)\n",
    "    mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=lr_mix)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learn_gamma)\n",
    "    scheduler_mix = torch.optim.lr_scheduler.ExponentialLR(mixing_optimizer, gamma=learn_gamma)\n",
    "\n",
    "    steps_counter = 0\n",
    "    legal_actions = [0, 1, 2, 3, 4]\n",
    "    agent_indexes = [1, 3]\n",
    "    episode_rewards = []\n",
    "    win_rates = []\n",
    "    enemieName = env.enemieName\n",
    "    if random_enemy:\n",
    "        enemylist = ['randomTeam.py', 'baselineTeam.py', 'heuristicTeam.py', 'approxQTeam.py', 'MCTSTeam.py', 'AstarTeam.py']\n",
    "        weights = get_dynamic_weights(0, n_episodes)\n",
    "        enemieName = np.random.choice(enemylist, p=weights)\n",
    "\n",
    "    create_folder_if_not_exists(name_experiment)\n",
    "    for episode in range(n_episodes):\n",
    "        print(f\"enemy: {enemieName}\")\n",
    "\n",
    "    \n",
    "\n",
    "        done = {agent_id: False for agent_id in agent_indexes}\n",
    "        weights = get_dynamic_weights(episode, n_episodes)\n",
    "        if random_enemy:\n",
    "            enemieName = np.random.choice(enemylist, p=weights)\n",
    "        env.reset(enemieName=enemieName)\n",
    "        blue_player1_reward = 0\n",
    "        blue_player2_reward = 0\n",
    "        score = 0\n",
    "        while not all(done.values()):\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "            states = []\n",
    "            for i, agent_index in enumerate(agent_indexes):\n",
    "                obs_agent = env.get_Observation(agent_index)\n",
    "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                states.append(state)\n",
    "                action = epsilon_greedy_action(agent_q_networks[i], state, epsilon, legal_actions)\n",
    "                actions[agent_index] = action\n",
    "\n",
    "\n",
    "\n",
    "            next_states, rewards, terminations, info = env.step(actions)\n",
    "            score -= info[\"score_change\"]\n",
    "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
    "            blue_player1_reward += rewards[1]\n",
    "            blue_player2_reward += rewards[3]\n",
    "\n",
    "            next_states_converted = []\n",
    "            rewards_converted = []\n",
    "            terminations_converted = []\n",
    "            actions_converted = []\n",
    "\n",
    "            for index in agent_indexes:\n",
    "                next_states_converted.append(list(next_states.values())[index])\n",
    "                rewards_converted.append(rewards[index])\n",
    "                terminations_converted.append(terminations[index])\n",
    "                actions_converted.append(actions[index])\n",
    "\n",
    "            next_states_converted = torch.stack(next_states_converted)\n",
    "            states_converted = torch.stack(states)\n",
    "            rewards_converted = [rewards_converted]\n",
    "            terminations_converted = [terminations_converted]\n",
    "\n",
    "            replay_buffer.add((states_converted, actions_converted, rewards_converted, next_states_converted, terminations_converted))\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                loss= compute_td_loss(agent_q_networks, mixing_network, target_q_networks, batch,\n",
    "                                               gamma=gamma)\n",
    "                wandb.log({\"loss\": loss})\n",
    "\n",
    "\n",
    "                # Zero gradients for all optimizers\n",
    "                optimizer.zero_grad()\n",
    "                mixing_optimizer.zero_grad()\n",
    "\n",
    "                # Backpropagate once for all losses\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                mixing_optimizer.step()\n",
    "\n",
    "                # Update learning rate\n",
    "                scheduler.step()\n",
    "                scheduler_mix.step()\n",
    "\n",
    "        steps_counter += 1\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        wandb.log({\"blue_player1_reward\": blue_player1_reward})\n",
    "        wandb.log({\"blue_player2_reward\": blue_player2_reward})\n",
    "        wandb.log({\"episode\": episode})\n",
    "        wandb.log({'score': score})\n",
    "        wandb.log({\"enemy\": enemieName})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (episode + 1) % target_update_frequency == 0:\n",
    "            update_target_network(agent_q_networks, target_q_networks)\n",
    "\n",
    "        if (episode) % target_update_frequency == 0:\n",
    "            torch.save(agent_q_networks[0].state_dict(),f'{name_experiment}/agent1_{episode+1}.pth')\n",
    "            torch.save(agent_q_networks[1].state_dict(),f'{name_experiment}/agent3_{episode+1}.pth')\n",
    "            torch.save(mixing_network.state_dict(),f'{name_experiment}/mix_{episode+1}.pth')\n",
    "\n",
    "        # Log rewards and win rates\n",
    "        episode_rewards.append(blue_player1_reward+blue_player2_reward)\n",
    "        print(blue_player1_reward)\n",
    "        print(blue_player2_reward)\n",
    "        print(f\"Episode reward: {blue_player1_reward+blue_player2_reward}\")\n",
    "\n",
    "        if np.sum(env.game.state.getBlueFood().data) == 0:\n",
    "            win_rates.append(1)\n",
    "        else:\n",
    "            win_rates.append(0)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_win_rate = np.mean(win_rates[-10:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}, Win Rate: {avg_win_rate:.2f}\")\n",
    "\n",
    "    # Plot rewards and win rates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards, label=\"Average Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Average Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smoothed_win_rates = np.convolve(win_rates, np.ones(10)/10, mode='valid') if len(win_rates) > 10 else win_rates\n",
    "    plt.plot(smoothed_win_rates, label=\"Win Rate (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.title(\"Win Rate Over Episodes\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class train_param:\n",
    "    n_episodes: int = 100\n",
    "    batch_size: int = 32\n",
    "    gamma: float = 0.995\n",
    "    epsilon: float = 0.80\n",
    "    epsilon_min: float = 0.1\n",
    "    epsilon_decay: float = 0.995\n",
    "    target_update_frequency: int = 25\n",
    "    sequence: bool = False\n",
    "\n",
    "@dataclass\n",
    "class learn_param:\n",
    "    lr_mix: float = 0.0001\n",
    "    lr_agent: float = 0.0005\n",
    "    gamma: float = 0.99\n",
    "\n",
    "@dataclass\n",
    "class net_param:\n",
    "    hidden_dim: int = 64\n",
    "    d_model: int = 32\n",
    "    n_heads: int = 2\n",
    "    n_layers: int = 2\n",
    "\n",
    "train_p = train_param()\n",
    "learn_p = learn_param()\n",
    "net_p = net_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n"
     ]
    }
   ],
   "source": [
    "env = gymPacMan_parallel_env(display=True, layout_file=\"layouts/smallCapture.lay\", length=200, enemieName='randomTeam.py', defenceReward=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrI2Ff3uoJ6J",
    "outputId": "4f8ac6a3-c3e9-4393-9bc7-7fa2531f7f92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 11, 14]\n"
     ]
    }
   ],
   "source": [
    "# Get number of agents and their observation/action space dimensions\n",
    "n_agents = int(len(env.agents) / 2)\n",
    "action_dim_individual_agent = 5  # North, South, East, West, Stop\n",
    "\n",
    "obs_individual_agent = env.get_Observation(0)\n",
    "obs_shape = obs_individual_agent.shape\n",
    "\n",
    "# parameters\n",
    "\n",
    "\n",
    "agent_q_networks = [AgentQNetwork(obs_shape=obs_shape, action_dim=action_dim_individual_agent).to(device) for _ in\n",
    "                    range(n_agents)]\n",
    "\n",
    "target_q_networks = [AgentQNetwork(obs_shape=obs_shape, action_dim=action_dim_individual_agent).to(device) for _ in\n",
    "                     range(n_agents)]\n",
    "\n",
    "# Initialize the mixing network\n",
    "state_shape = (2, 8, env.layout.height, env.layout.width)  # Veronderstelling: de toestand is een concatenatie van agent-observaties\n",
    "global_state_shape = [len(env.agents)] + list(obs_shape)\n",
    "print(global_state_shape)\n",
    "mixing_network = AttentionMixer(n_agents=n_agents, state_shape=state_shape).to(device)\n",
    "#mixing_network.load_state_dict(torch.load('mix_75.pth'))\n",
    "\n",
    "# Initialize target Q-networks with the same weights as the main Q-networks\n",
    "update_target_network(agent_q_networks, target_q_networks)\n",
    "\n",
    "# Initialize the replay buffer\n",
    "replay_buffer = ReplayBuffer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DK95aBtt0T6U",
    "outputId": "8b427826-7e4a-4ade-8939-f82ee387a963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josse\\AppData\\Local\\Temp\\ipykernel_24184\\3963537054.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.11\n",
      "13.11\n",
      "Episode reward: 26.22\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "17.079999999999966\n",
      "17.079999999999966\n",
      "Episode reward: 34.15999999999993\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "6.179999999999973\n",
      "6.179999999999973\n",
      "Episode reward: 12.359999999999946\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "6.1799999999999695\n",
      "6.1799999999999695\n",
      "Episode reward: 12.359999999999939\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "13.959999999999997\n",
      "13.959999999999997\n",
      "Episode reward: 27.919999999999995\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "0.8299999999999557\n",
      "0.8299999999999557\n",
      "Episode reward: 1.6599999999999113\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "14.059999999999997\n",
      "14.059999999999997\n",
      "Episode reward: 28.119999999999994\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "15.119999999999997\n",
      "15.119999999999997\n",
      "Episode reward: 30.239999999999995\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "6.469999999999945\n",
      "6.469999999999945\n",
      "Episode reward: 12.93999999999989\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n"
     ]
    }
   ],
   "source": [
    "# Train QMix in the environment\n",
    "name_experiment ='CNN_tournament2_tiny'\n",
    "wandb.init(project=\"\", id = f\"{name_experiment}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "env = gymPacMan_parallel_env(display=True, layout_file=\"layouts/win.lay\", length=300, enemieName='randomTeam.py', defenceReward=True)\n",
    "\n",
    "train_qmix(env, name_experiment, agent_q_networks, mixing_network, target_q_networks, replay_buffer, train_p, learn_p, random_enemy = False)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Qxm-zQIIoJ6K"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>blue_player1_reward</td><td>▄▅▂▄▄▁▄▄▄▁▄▅▃▁▄▄▃▃▄▄▇▇█▇▄▅▄▂▄▁▄█▇▇▁▇▄▄▄▃</td></tr><tr><td>blue_player2_reward</td><td>▄▄▂▄▄▄▁▁▄▁▂▃▁▁▄▂▁▁▂▃▇███▂▇▄▂▄▇▄▇▇█▃▇▄▄▄▄</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▃▃▄▅▅▅▆▆▆▇▁▁▁▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇█</td></tr><tr><td>epsilon</td><td>▇▇▆▆▅▅▄▄▄▄▄▃▃▃▃▂▂▂██▇▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁</td></tr><tr><td>loss</td><td>▅▄▄▄▄▇▇█▇▇▇▇█▇▆▆▆▇▆▇█▂▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁</td></tr><tr><td>score</td><td>▁▇▁▅▂▅▁▂▅▄▄▄▄▅▁▄▇▇█▄█▂▇▇▇▅█▂▅██▂▇█▇██▇▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>blue_player1_reward</td><td>4.04</td></tr><tr><td>blue_player2_reward</td><td>4.04</td></tr><tr><td>enemy</td><td>randomTeam.py</td></tr><tr><td>episode</td><td>151</td></tr><tr><td>epsilon</td><td>0.42221</td></tr><tr><td>loss</td><td>2.19088</td></tr><tr><td>score</td><td>-1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CNN2_tournament2_tiny__20241209_090623</strong> at: <a href='https://wandb.ai/josse-vdk/uncategorized/runs/CNN2_tournament2_tiny__20241209_090623' target=\"_blank\">https://wandb.ai/josse-vdk/uncategorized/runs/CNN2_tournament2_tiny__20241209_090623</a><br/> View project at: <a href='https://wandb.ai/josse-vdk/uncategorized' target=\"_blank\">https://wandb.ai/josse-vdk/uncategorized</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241209_090623-CNN2_tournament2_tiny__20241209_090623\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3qJ2lOZ-Jk6"
   },
   "source": [
    "## 1.5 Reflection Questions\n",
    "\n",
    "Evaluate your results on \"smallCapture.lay\" against random agents and answer the following questions:\n",
    "\n",
    "*  How do your QMix agents improve over time during the training?\n",
    "*  How does the performance of QMix compare to the performance of IQL?\n",
    "*  Do you observe different roles for the agents within a team?\n",
    "*  What other reflection questions can you think of yourself?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You’ve now implemented the QMix algorithm for the PacMan capture the flag environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7hGqk6LKeEg"
   },
   "source": [
    "# **Section 2: Improving the training strategy**\n",
    "\n",
    "In this section, you will design and implement an improved training strategy for the blue Pacman agents on the map \"bloxCapture.lay\". Don't try to change the architecture too much, to look at the effect of the training strategy. The objective is to train the agents effectively while considering the challenges of multi-agent environments, such as who you play against, exploration, credit assignment, and balancing exploitation with learning stability. You don't have to do everything. Choose some objective you want to study, do some experiments, and interpret your results. You have to do the first topic and at least one more, but feel free to choose how many you do.\n",
    "\n",
    "## 2.1 Understanding Training Challenges\n",
    "\n",
    "Before jumping into implementation, reflect on the following challenges in training multi-agent systems:\n",
    "1. Which enemies?:\n",
    "* When do you play against which enemies? So that it does not become too easy or is not too hard from the start.\n",
    "* How well does self-play perform?\n",
    "2.\tExploration vs. Exploitation:\n",
    "* How will you encourage the agents to explore diverse strategies without overcommitting to suboptimal behaviors early on?\n",
    "* Will you use an exploration strategy like epsilon-greedy, where agents take random actions with some probability, or another method such as Boltzmann exploration?\n",
    "* Should the exploration strategy be adaptive, changing over time as the agents learn?\n",
    "4.\tStability of Learning:\n",
    "* What techniques can you use to avoid instabilities like divergent Q-values or poor convergence?\n",
    "5.\tCoordination Between Agents:\n",
    "* How do you ensure the agents learn coordinated policies rather than acting independently?\n",
    "6.\tReward Shaping and Credit Assignment:\n",
    "* How can you ensure that each agent is rewarded fairly for their contributions to the team’s success?\n",
    "* Should you use a dense reward (frequent small rewards) or a sparse reward (large rewards at key events)?\n",
    "7.\tReplay Buffer Usage\n",
    "* How can you ensure the replay buffer stores diverse experiences, and how frequently will you sample from it? And which experiences do you sample more than others?\n",
    "\n",
    "## 2.2 Reflection Questions\n",
    "\n",
    "*  For each training challenge, what have you tried, and why?  \n",
    "*   Did your agents improve, and how do you know?\n",
    "*   What trends do you observe in the rewards over episodes, how does the score and loss evolve?\n",
    "* If the agents are not learning effectively, what modifications would you make to your strategy?\n",
    "* What other reflection questions can you think of yourself?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGslIWIeMWSS"
   },
   "source": [
    "# **Section 3: Experimenting with Your Own Algorithm**\n",
    "\n",
    "In this section, you are encouraged to explore beyond the QMix algorithm. The goal is for you to select a different multi-agent reinforcement learning algorithm or propose your own modifications to QMix. You will implement and test this algorithm on the Pacman Capture the Flag environment on the map \"bloxCapture.lay\". Finally, also try train your algorithm on random maps.\n",
    "\n",
    "This section is open-ended, allowing you to experiment and think critically about the challenges and opportunities in multi-agent learning.\n",
    "## 3.1 Suggested Directions\n",
    "\n",
    "Here are some ideas to get you started. You may choose one of these or propose a completely new direction:\n",
    "1.\tPolicy Gradient Approaches\n",
    "\n",
    "* Implement a multi-agent Proximal Policy Optimization (PPO) or Actor-Critic algorithm.\n",
    "* How do policy gradient methods handle coordination between agents compared to value-based methods like QMix?\n",
    "2.\tCounterfactual Multi-Agent Policy Gradients (COMA)\n",
    "* Explore COMA, which uses counterfactual baselines to address the credit assignment problem.\n",
    "* How does COMA adjust the contribution of each agent to the team’s reward?\n",
    "3.\tModifications to QMix\n",
    "* Experiment with a different mixer architecture. For example, add attention mechanisms or use more complex neural networks for mixing Q-values.\n",
    "* Modify the reward function to encourage specific behaviors like offense or defense in the game.\n",
    "4.\tHybrid Approaches\n",
    "* Combine value-based and policy-gradient methods, such as integrating PPO with QMix.\n",
    "5. Have an idea for something else, go right ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sajuh8nfMkTW"
   },
   "source": [
    "## 3.2 Reflection Questions\n",
    "\n",
    "After implementing your chosen algorithm, reflect on the following:\n",
    "\n",
    "1. Design choices\n",
    "* Which things did you implement, and why?\n",
    "* Explain your training strategies.\n",
    "\n",
    "2.\tPerformance\n",
    "* How does your algorithm perform compared to standard QMix?\n",
    "\n",
    "3.\tStrengths and Weaknesses\n",
    "* What are the strengths of your chosen approach in the multi-agent Pacman environment?\n",
    "* What are the weaknesses or challenges you encountered?\n",
    "\n",
    "4.\tCoordination\n",
    "* Did your algorithm encourage better coordination between agents? Why or why not?\n",
    "\n",
    "5.\tGeneralization\n",
    "* How well do you think your algorithm generalizes to random maps? Did you have to change something to make this work?\n",
    "\n",
    "What other reflection questions can you think of yourself?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkRulQ_cMn-g"
   },
   "outputs": [],
   "source": [
    "## mogelijke upgrade buffer\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000, alpha=0.6):\n",
    "        \"\"\"\n",
    "        Prioritized Replay Buffer\n",
    "        :param buffer_size: Maximum number of experiences in the buffer\n",
    "        :param alpha: Degree of prioritization (0 = no prioritization, 1 = full prioritization)\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.priorities = deque(maxlen=buffer_size)  # Stores priorities of experiences\n",
    "        self.alpha = alpha  # Controls the level of prioritization\n",
    "\n",
    "    def add(self, experience, priority=1.0):\n",
    "        \"\"\"\n",
    "        Add an experience to the buffer with an initial priority.\n",
    "        :param experience: Tuple (state, action, reward, next_state, done)\n",
    "        :param priority: Initial priority of the experience\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences, using prioritized sampling.\n",
    "        :param batch_size: Number of experiences to sample\n",
    "        :param beta: Degree of importance-sampling correction (0 = no correction, 1 = full correction)\n",
    "        :return: Tuple (states, actions, rewards, next_states, dones, weights, indices)\n",
    "        \"\"\"\n",
    "        # Compute sampling probabilities\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        # Sample indices based on priorities\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # Importance-sampling weights\n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** -beta\n",
    "        weights /= weights.max()  # Normalize weights to [0, 1]\n",
    "\n",
    "        # Restructure the batch\n",
    "        states = np.array([exp[0].cpu().numpy() for exp in experiences], dtype=np.float32)\n",
    "        actions = np.array([exp[1] for exp in experiences], dtype=np.int64)\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3].cpu().numpy() for exp in experiences])\n",
    "        dones = np.array([exp[4] for exp in experiences])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"\n",
    "        Update the priorities of sampled experiences.\n",
    "        :param indices: Indices of the sampled experiences\n",
    "        :param priorities: New priorities for these experiences\n",
    "        \"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o7eoBGNa9rO"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
