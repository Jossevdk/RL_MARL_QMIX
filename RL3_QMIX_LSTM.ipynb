{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jossevdk/RL_MARL_QMIX/blob/main/RL3_QMIX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTT57uLHmIVf"
   },
   "source": [
    "\n",
    "# **Disclaimer**\n",
    "This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intSkgKwmUZm"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "\n",
    "Welcome to the third assignment of the Reinforcement Learning course! In this assignment, you will implement and train a QMix algorithm for a multi-agent environment. The task takes place in a Pacman Capture the Flag game, where you control two blue Pacman agents working together to capture food and avoid red Pacman opponents.\n",
    "\n",
    "This assignment is divided into three sections:\n",
    "\n",
    "\n",
    "1.  Implementing QMix: You will construct the QMix architecture to combine individual agent Q-values into a global Q-value. Training the Agents: You will implement the training loop, including the QMix loss function. Getting it working here is key, don't do much too fast.\n",
    "2.   Try to improve your agents, with different training strategies. Don't experiment too much with new architectures, that will come later!\n",
    "3. \tExploring Beyond QMix: You will choose and implement another multi-agent RL algorithm and compare its performance with QMix.\n",
    "\n",
    "During this assignment, there will be four tournaments conducted in a round-robin format, where each team will compete against others:\n",
    "-\tIntermediate Tournaments (3): These serve as checkpoints to test your models and evaluate your training strategies.\n",
    "-\tFinal Tournament (1): This determines the ultimate winner! Who will forever be in the Hall of Fame?\n",
    "\n",
    "The first tournament will be on “bloxCapture.lay” and depending on the progress of the teams, the next tournaments will use more maps. For more information about the submission and deadlines check the assignment. The tournament results **will not impact your grade**.\n",
    "\n",
    "\n",
    "By the end of this assignment, you will gain practical experience in implementing and experimenting with multi-agent reinforcement learning algorithms!\n",
    "\n",
    "## Environment: [PacMan, a capture the flag variant ](https://ai.berkeley.edu/contest.html)\n",
    "\n",
    "The environment you'll be working in, PacMan Capture the Flag, is a multi-agent version of the classic PacMan game. It involves multiple agents that need to cooperate to achieve a shared objective. Each agent can only observe part of the environment(although this is close to the full environment) and needs to act based on its local observations, making this an ideal setup for exploring cooperative multi-agent reinforcement learning.\n",
    "The game logic comes from [here](https://ai.berkeley.edu/contest.html), the code is not to hard understand if you want to delve a little bit deeper. Additional to this base game you will also find some additional code so that the environment is more useable like other gym environemnts. This makes it easier to train your deep RL multi agent algorithms.\n",
    "You'll also see a display option in the code. You can visualize games quite easily, however for this we recommend, copying the notebook and running it locally.  \n",
    "\n",
    "**For more information on the environment check the additional file**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IBmjgcct1f9"
   },
   "source": [
    "\n",
    "# **Setup**\n",
    "\n",
    "Before we dive into coding, let’s make sure everything is set up correctly.\n",
    "\n",
    "1. Install Dependencies\n",
    "\n",
    "You’ll need to install the following libraries to run the notebook. Run the cell below to install them:\n",
    "\n",
    "\n",
    "*   PacMan Capture the Flag: a reinforcement learning environments.\n",
    "*   Packages you'll use throught the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoP_9-CHfydw",
    "outputId": "98573555-4fc3-4e4d-f9a4-bd9fb35de759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in C:/Users/josse/OneDrive/Documenten/creditcontract/RL/RL_MARL_QMIX/.git/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'rl-lab-3-pacman' already exists and is not an empty directory.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!git init\n",
    "!git clone https://student:fFYKVqdc976PXAuyhyz4@gitlab.ilabt.imec.be/emalomgr/rl-lab-3-pacman.git --branch student_version\n",
    "!mv ./rl-lab-3-pacman/* ./\n",
    "!rm -r ./rl-lab-3-pacman/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1-5e7HR3saX"
   },
   "source": [
    "2. Import Necessary Libraries\n",
    "\n",
    "Once the dependencies are installed, import the key libraries you’ll need throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaNmbYmYrdGq",
    "outputId": "cf8d951e-2a2b-4483-cdeb-0d2c916085e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from gymPacMan import gymPacMan_parallel_env\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "collapsed": true,
    "id": "00mnpkAvlOSB",
    "outputId": "96e3a491-5ec4-4e78-98b6-d4bab0b07e28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Blue team starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\josse\\_netrc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# possible options: env = gymPacMan_parallel_env(layout_file = f'{file_path}/layouts/smallCapture.lay', display = False, length = 800, reward_forLegalAction= True, defenceReward = True, random_layout = True, enemieName = 'randomTeam', self_play = False)\n",
    "env = gymPacMan_parallel_env(layout_file='./layouts/tinyCapture.lay', display=False, length=300, enemieName='MCTSTeam.py')\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# Start a new wandb run to track this script\n",
    "\n",
    "wandb.login(key = '', relogin = True)\n",
    "name_experiment =''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 20, 20])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 10, 11])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 11, 14])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "torch.Size([8, 7, 20])\n",
      "0\n",
      "Loaded random layout\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "torch.Size([8, 20, 20])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "torch.Size([8, 9, 10])\n",
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/MCTSTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 11, 17])\n"
     ]
    }
   ],
   "source": [
    "layout = ['bloxCapture.lay','defence.lay','smallCapture.lay','tinyCapture.lay','verdedeging2.lay','win.lay','win2.lay']\n",
    "for lay in layout:\n",
    "    env = gymPacMan_parallel_env(layout_file=f'./layouts/{lay}', display=False, length=300, enemieName='MCTSTeam.py')\n",
    "    print(env.get_Observation(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjgyVEbp8sow"
   },
   "source": [
    "# **Section 1: QMix Implementation**\n",
    "\n",
    "In this section, you will implement the QMix algorithm to control agents in the PacMan environment. QMix is a powerful algorithm in multi-agent reinforcement learning that allows for centralized training with decentralized execution. The key idea behind QMix is to learn a mixing network that combines individual agent Q-values into a global Q-value, which allows agents to make coordinated decisions while still acting independently during execution.\n",
    "\n",
    "## QMix Theory Overview\n",
    "\n",
    "QMix is a value-based multi-agent reinforcement learning algorithm designed for cooperative tasks. It addresses the challenge of decentralized control while maintaining a centralized training framework. The key idea is to learn individual Q-values for each agent and combine them into a global Q-value that represents the team’s joint policy.\n",
    "\n",
    "Core Concepts:\n",
    "1.\tIndividual Q-Values: Each agent has a separate Q-network that predicts the Q-values for its actions based on its local observations.\n",
    "2.\tGlobal Q-Value: A mixer network aggregates the individual Q-values into a global Q-value, ensuring that the global Q-value is monotonic with respect to individual Q-values. This monotonicity ensures that maximizing the global Q-value aligns with maximizing the individual Q-values.\n",
    "3.\tHypernetworks: QMix uses hypernetworks to generate the weights for the mixer network dynamically. These weights depend on the global state, allowing the mixer network to adapt its behavior based on the team’s overall situation.\n",
    "Step-by-Step Implementation\n",
    "\n",
    "You will be implementing QMix step by step, focusing on the following parts:\n",
    "\n",
    "1.\tImplement the individual agent Q-networks.\n",
    "2.\tBuild the mixing network to combine individual Q-values.\n",
    "3.\tSet up the loss function and training loop.\n",
    "4.\tTrain the agents in the PacMan environment.\n",
    "\n",
    "##Let’s begin!\n",
    "\n",
    "## 1.1   Agent Q-Network Implementation\n",
    "\n",
    "Each agent in the QMix algorithm has its own Q-network that estimates the action-value function based on the agent’s local observation. The Q-network takes the agent’s observation as input and outputs the Q-values for each possible action.\n",
    "\n",
    "**Task:** Implement IQL.\n",
    "\n",
    "**Keep it simple**\n",
    "\n",
    "Don't use too advanced architecures from the start and train against random agents on a small map. Start on \"tinyCapture.lay\" and move on to \"smallCapture.lay\" when you see your algorithm works This way you can see quicker whether your algorithm is working.\n",
    "\n",
    "**Check if the agents learn without QMix**\n",
    "\n",
    "Do a run with IQL without the mixing network and another run with QMix and reflect on the improvements in the performance of QMix.\n",
    "\n",
    "**Evaluate during training**\n",
    "\n",
    "Try to evaluate your agents while you train, so you can see faster if they learn or not. A good tool for this is WANDB, where you can easily compare different stragegies during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QnE1LUgR0hzY"
   },
   "outputs": [],
   "source": [
    "class AgentQNetwork(nn.Module):\n",
    "    def __init__(self,obs_shape, action_dim,hidden_dim=64):\n",
    "        super(AgentQNetwork, self).__init__()\n",
    "        self.c1 = nn.Conv2d(obs_shape[0], 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.c2 = nn.Conv2d(32, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.c3 = nn.Conv2d(hidden_dim, 2*hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.c4 = nn.Conv2d(2*hidden_dim, 4*hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.c1(obs))\n",
    "        x = torch.relu(self.c2(x))\n",
    "        x = torch.relu(self.c3(x))\n",
    "        x = torch.relu(self.c4(x))\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AgentQNetwork2(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, hidden_dim=64):\n",
    "        super(AgentQNetwork2, self).__init__()\n",
    "        \n",
    "        self.c1 = nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=1, padding='same')\n",
    "        self.c2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n",
    "        self.c3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64,32)\n",
    "        self.fc2 = nn.Linear(32, action_dim)\n",
    "        \n",
    "        self.activation = nn.SiLU()  # Use SiLU as the activation function\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.activation(self.bn1(self.c1(obs)))\n",
    "        x = self.activation(self.bn2(self.c2(x)))\n",
    "        x = self.activation(self.bn3(self.c3(x)))\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTransformerQNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, d_model=32, n_heads=2, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten(2)  # (batch, channels, H*W)\n",
    "        self.token_proj = nn.Linear(32, d_model)  # Project to transformer tokens\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, n_heads),\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, action_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.cnn(obs)\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = self.flatten(x).permute(0, 2, 1)  # Tokens: (batch, H*W, channels)\n",
    "        x = self.token_proj(x)  # Project tokens\n",
    "        x = self.transformer(x)  # Apply transformer\n",
    "        x = x.mean(dim=1)  # Pool tokens\n",
    "        q_values = self.fc(x)\n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN-LSTM-Q Network\n",
    "\n",
    "#TODO: include sequences as input\n",
    "#TODO: use padding for the batch size\n",
    "\n",
    "\n",
    "class CNNLSTMQNetwork_hidden(nn.Module):\n",
    "    def __init__(self, embed_dim, obs_shape, action_space):\n",
    "        super(CNNLSTMQNetwork_hidden, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Shared encoder for the agent's state\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling to handle varying input sizes\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to project encoded state to embed_dim\n",
    "        self.fc = nn.Linear(128 * 1 * 1, embed_dim)\n",
    "\n",
    "        # LSTM layer to model temporal dependencies\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Final layer to compute Q-values\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, action_space),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "    def forward(self, state, hidden):\n",
    "        # Process single state using the shared encoder\n",
    "        cnn_features = []\n",
    "        for t in range(5):\n",
    "            \n",
    "            encoded_state = self.shared_encoder(state[:, t, :, :, :])  # Encode the state\n",
    "            encoded_state = encoded_state.view(encoded_state.size(0), -1)  # Flatten to [batch_size, 128 * 1 * 1]\n",
    "            projected_state = self.fc(encoded_state).unsqueeze(1)  # Project to embed_dim and add sequence dimension\n",
    "            cnn_features.append(projected_state)\n",
    "            \n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # Shape: (batch_size, seq_len, cnn_output_size)\n",
    "        # Apply LSTM to model temporal dependencies\n",
    "        hx, cx = hidden[0, :, :, :], hidden[1, :, :, :]\n",
    "        lstm_out, (hx, cx) = self.lstm(cnn_features, (hx, cx))\n",
    "        hidden_new = torch.stack([hx.detach(), cx.detach()], dim=0)\n",
    "        # Use the output of the LSTM for the final layer\n",
    "        q_values = self.final_layer(lstm_out[:, -1, :])  # Shape: [batch_size, action_space]\n",
    "\n",
    "        return q_values, hidden_new\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.embed_dim),\n",
    "                torch.zeros(1, batch_size, self.embed_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dYpvxTc8SUh"
   },
   "source": [
    "## test for IQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H9qtVQS81ds"
   },
   "source": [
    "\t•\tobs_dim: The dimension of the agent’s local observation.\n",
    "\t•\taction_dim: The number of possible actions the agent can take.\n",
    "\n",
    "## 1.2 Mixing Network\n",
    "\n",
    "The mixing network is responsible for combining the individual Q-values from each agent into a global Q-value. The mixing network ensures that the global Q-value is a monotonic function of each agent’s Q-value, which allows the system to maintain decentralized decision-making at runtime.\n",
    "\n",
    "**Task:** Implement the mixing network.\n",
    "\n",
    "The mixing network will take the Q-values of all agents as input and output a single global Q-value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "id": "1aIiC5gD8yZT"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class SimpleQMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape):\n",
    "        super(SimpleQMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.state_shape = state_shape\n",
    "        print(state_shape)\n",
    "        # Much simpler state processing\n",
    "        self.state_conv = nn.Sequential(\n",
    "            nn.Conv3d(state_shape[0], 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * state_shape[1] * state_shape[2] * state_shape[3],  n_agents)  # Output: (batch_size, 32 * n_agents)\n",
    "        )\n",
    "        # Initialize weights positively\n",
    "        for m in self.state_conv:\n",
    "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "        # Single layer mixing network\n",
    "        self.mixing_layer = nn.Linear(n_agents , 1, bias=False)\n",
    "\n",
    "        # Initialize close to equal weights\n",
    "        nn.init.uniform_(self.mixing_layer.weight, 0.9, 1.1)\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "\n",
    "        bs = agent_qs.size(0)\n",
    "        weights = F.softplus(self.state_conv(states))\n",
    "\n",
    "\n",
    "        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n",
    "\n",
    "        weighted_qs = agent_qs * weights.unsqueeze(1)\n",
    "\n",
    "        q_tot = self.mixing_layer(weighted_qs.squeeze(1))\n",
    "        return q_tot.view(bs, -1, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "id": "8E-mSFq9gIq-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape=(2, 8, 10, 20), embed_dim=32, n_heads=4):\n",
    "        super(AttentionMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.state_shape = state_shape[1:]  # Remove agent dimension\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Shared Conv2D encoder for each agent\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Conv2d(self.state_shape[0], 16, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to project to embed_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, embed_dim),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "        for m in self.shared_encoder:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "        for m in self.fc:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        # Multi-head attention for inter-agent interaction\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads, batch_first=True)\n",
    "\n",
    "        # Hypernetwork for mixing weights and biases\n",
    "        self.hyper_w = nn.Sequential(\n",
    "            nn.Linear(embed_dim * n_agents, embed_dim * embed_dim),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Linear(embed_dim * embed_dim, n_agents * embed_dim)\n",
    "        )\n",
    "        self.hyper_b = nn.Sequential(\n",
    "            nn.Linear(embed_dim * n_agents, embed_dim),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Final layer to compute Q_tot\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "        for m in self.hyper_w:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        for m in self.hyper_b:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        for m in self.final_layer:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "        batch_size = states.size(0)\n",
    "\n",
    "        # Process each agent's state individually using the shared encoder\n",
    "        agent_features = []\n",
    "        for i in range(self.n_agents):\n",
    "            agent_state = states[:, i, :, :, :]  # Shape: [batch_size, 8, 10, 20]\n",
    "            encoded_state = self.shared_encoder(agent_state)  # Encode individual agent\n",
    "            encoded_state = encoded_state.view(batch_size, -1)  # Flatten to [batch_size, 128]\n",
    "            projected_state = self.fc(encoded_state)  # Project to embed_dim\n",
    "            agent_features.append(projected_state)\n",
    "\n",
    "        # Stack agent features: Shape [batch_size, n_agents, embed_dim]\n",
    "        agent_features = torch.stack(agent_features, dim=1)\n",
    "\n",
    "        # Apply attention to model inter-agent interaction\n",
    "        attention_out, _ = self.attention(agent_features, agent_features, agent_features)\n",
    "\n",
    "        # Flatten attention output for hypernetwork\n",
    "        attention_out_flat = attention_out.reshape(batch_size, -1)  # Shape: [batch_size, n_agents * embed_dim]\n",
    "\n",
    "        # Compute hypernetwork weights and biases\n",
    "        hyper_w = F.softplus(self.hyper_w(attention_out_flat).view(batch_size, self.n_agents, self.embed_dim))\n",
    "        hyper_b = self.hyper_b(attention_out_flat).view(batch_size, self.embed_dim)\n",
    "\n",
    "        # Combine agent Q-values with weights and biases\n",
    "        agent_qs = agent_qs.view(batch_size, self.n_agents, 1)  # Shape: [batch_size, n_agents, 1]\n",
    "        weighted_qs = torch.bmm(agent_qs.transpose(1, 2), hyper_w).squeeze(1) + hyper_b\n",
    "\n",
    "        # Final projection to Q_tot\n",
    "        q_tot = self.final_layer(weighted_qs).unsqueeze(1)\n",
    "        return q_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OUjn7xq88V6"
   },
   "source": [
    "\t•\tstate_dim: The dimension of the global state (available during centralized training).\n",
    "\t•\tn_agents: The number of agents, which determines the number of Q-values being mixed.\n",
    "\t•\tWeights and biases: The weights and biases of the mixing network depend on the global state, ensuring that different states lead to different weightings of agent Q-values.\n",
    "\n",
    "## 1.3 Loss Function and Training Loop\n",
    "\n",
    "The agents need to learn their Q-values by minimizing the Temporal Difference (TD) error. The loss is computed as the difference between the predicted Q-value (from the agent’s Q-network) and the target Q-value (computed using the Bellman equation).\n",
    "\n",
    "**Task:** Set up the loss function and implement the training loop for QMix.\n",
    "\n",
    "**Hint:** Use the huber loss for more stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "id": "H2c2BYMg84p3"
   },
   "outputs": [],
   "source": [
    "#TODO: dimensions for hubertloss not the same\n",
    "\n",
    "\n",
    "def compute_td_loss(agent_q_networks, mixing_network, target_q_networks, batch, weights = None, gamma=0.99, lambda_=0.1):\n",
    "    states, actions, rewards, next_states, dones, hidden_states, next_hidden_states = batch\n",
    "    \n",
    "     # Get current Q-values for each agent\n",
    "    agent_qs = []\n",
    "    for i, agent_q_network in enumerate(agent_q_networks):\n",
    "        agent_q, _ = agent_q_network(states[:,:, i, :, :, :], hidden_states[i,:, :,:,  :])  # Shape: (batch_size, num_actions)\n",
    "       \n",
    "        agent_q = agent_q.gather(dim=1, index=actions[:,-1,i].unsqueeze(1))  # Select action Q-values\n",
    "        agent_qs.append(agent_q)\n",
    "\n",
    "    agent_qs = torch.cat(agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
    "    # Get target Q-values for each agent using Double DQN\n",
    "    with torch.no_grad():\n",
    "        target_agent_qs = []\n",
    "        for i, (agent_q_network, target_q_network) in enumerate(\n",
    "            zip(agent_q_networks, target_q_networks)\n",
    "        ):\n",
    "            # Double DQN: Get greedy actions from the current network\n",
    "\n",
    "            next_q_values, _ = agent_q_network(next_states[:,:, i, :, :, :], next_hidden_states[i,:, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "            greedy_actions = torch.argmax(next_q_values, dim=1, keepdim=True)\n",
    "            # Get target Q-values using the target network\n",
    "            target_q_values, _ = target_q_network(next_states[:,:, i, :, :, :], next_hidden_states[i,:, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "            target_q = target_q_values.gather(1, greedy_actions)\n",
    "           \n",
    "            done_mask = dones[-1, i, :].unsqueeze(1)\n",
    "            filtered_target_q = target_q * (1 - done_mask)\n",
    "            target_agent_qs.append(filtered_target_q)\n",
    "        \n",
    "        target_agent_qs = torch.cat(target_agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
    "        \n",
    "        # Compute the total target Q-value using the mixing network\n",
    "        target_q_tot = mixing_network(target_agent_qs, next_states[:, -1, :, :, :, :].contiguous()).squeeze(1)   # Shape: (batch_size, 1)\n",
    "    target_q_tot = rewards[:, -1, :].sum(dim=1).unsqueeze(1) + gamma * target_q_tot\n",
    "\n",
    "\n",
    "    # Compute the global Q-value using the mixing network\n",
    "    q_tot = mixing_network(agent_qs, states[:, -1, :, :, :, :].contiguous()).squeeze(1)  # Shape: (batch_size, 1)\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    td_loss = F.huber_loss(target_q_tot, q_tot,  reduction=\"none\")\n",
    "\n",
    "    # Apply weights if prioritized experience replay is used\n",
    "    if weights is not None:\n",
    "        td_loss *= weights\n",
    "\n",
    "    # Return the mean loss\n",
    "    total_loss = td_loss.mean()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qb_cFvZ-Bz8"
   },
   "source": [
    "## 1.4 Training the QMix Algorithm\n",
    "\n",
    "Now that you have defined the agent Q-networks, the mixing network, and the loss function, it’s time to train the agents in the gym environment.\n",
    "\n",
    "**Task:** Implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "id": "OTtfvR9w-BJQ"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Stack the experiences into tensors directly on the GPU\n",
    "        \n",
    "        states = torch.stack([torch.stack(exp[0]).to('cuda') for exp in experiences])\n",
    "        actions = torch.stack([torch.tensor(exp[1]).to('cuda') for exp in experiences])\n",
    "        rewards = torch.stack([torch.tensor(exp[2]).to('cuda') for exp in experiences])\n",
    "        next_states = torch.stack([torch.stack(exp[3]).to('cuda') for exp in experiences])\n",
    "        terminations = torch.stack([torch.tensor(exp[4], dtype=torch.int).to('cuda') for exp in experiences])\n",
    "        hidden_states = torch.cat([exp[5] for exp in experiences], dim = 3)\n",
    "        next_hidden_states = torch.cat([exp[6] for exp in experiences], dim = 3)\n",
    "        \n",
    "\n",
    "        return (states, actions, rewards, next_states, terminations, hidden_states, next_hidden_states)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "class ReplayBuffer_sequence:\n",
    "    def __init__(self, buffer_size=10000, sequence_length=5):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        sequences = []\n",
    "        for _ in range(batch_size):\n",
    "            start_index = random.randint(0, len(self.buffer) - self.sequence_length)\n",
    "            sequence = [self.buffer[i] for i in range(start_index, start_index + self.sequence_length)]\n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        # Transpose the sequences to get batches of each component\n",
    "        \n",
    "        states = torch.stack([torch.stack([torch.stack(seq[0]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        actions = torch.stack([torch.stack([torch.tensor(seq[1]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        rewards = torch.stack([torch.stack([torch.tensor(seq[2]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        next_states = torch.stack([torch.stack([torch.stack(seq[3]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        terminations = torch.stack([torch.stack([torch.tensor(seq[4], dtype=torch.int).to(device) for seq in sequence]) for sequence in sequences], dim=2)\n",
    "        hidden_states = torch.cat([sequence[-1][5] for sequence in sequences], dim = -2).to(device)\n",
    "        next_hidden_states = torch.cat([sequence[-1][6] for sequence in sequences], dim = -2).to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, terminations, hidden_states, next_hidden_states)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Adjusted epsilon-greedy action function\n",
    "def epsilon_greedy_action(agent_q_network, state, hidden, epsilon, legal_actions):\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        # Explore: take a random action\n",
    "        action = random.choice(legal_actions)\n",
    "    else:\n",
    "        state = state.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        q_values, hidden = agent_q_network(state, hidden)\n",
    "        q_values = q_values.cpu().detach().numpy()\n",
    "        action = np.random.choice(np.flatnonzero(q_values == q_values.max()))\n",
    "\n",
    "    return action, hidden\n",
    "\n",
    "\n",
    "def update_target_network(agent_q_networks, target_q_networks):\n",
    "    for target, source in zip(target_q_networks, agent_q_networks):\n",
    "        target.load_state_dict(source.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDxjxi5-E-Kd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def interpolate_weights(start_weights, end_weights, factor):\n",
    "    return [(1 - factor) * start + factor * end for start, end in zip(start_weights, end_weights)]\n",
    "\n",
    "def get_dynamic_weights(episode, n_episodes):\n",
    "    # Define the initial weights for the beginning, middle, and end\n",
    "    weights_beginning = [0.8, 0.05, 0.05, 0.05, 0.025, 0.025]\n",
    "    weights_middle = [0.2, 0.25, 0.25, 0.25, 0.025, 0.025]\n",
    "    weights_end = [0.02, 0.06, 0.06, 0.06, 0.4, 0.4]\n",
    "\n",
    "    # Calculate the transition points\n",
    "    transition1 = n_episodes // 3\n",
    "    transition2 = 2 * n_episodes // 3\n",
    "\n",
    "    # Adjust weights based on the current episode\n",
    "    if episode < transition1:\n",
    "        factor = episode / transition1\n",
    "        return interpolate_weights(weights_beginning, weights_middle, factor)\n",
    "    elif episode < transition2:\n",
    "        factor = (episode - transition1) / (transition2 - transition1)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    else:\n",
    "        factor = (episode - transition2) / (n_episodes - transition2)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    \n",
    "\n",
    "import os\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def prepare_input_sequence(state_buffer, state, sequence_length):\n",
    "        # Add the current state to the buffer\n",
    "    state_buffer.append(state)\n",
    "        \n",
    "        # If the buffer is not full, pad it with zeros\n",
    "    if len(state_buffer) < sequence_length:\n",
    "        padding = [torch.zeros_like(state) for _ in range(sequence_length - len(state_buffer))]\n",
    "        sequence = padding + list(state_buffer)\n",
    "    else:\n",
    "        sequence = list(state_buffer)\n",
    "        \n",
    "        # Stack the sequence to form a tensor\n",
    "    sequence_tensor = torch.stack(sequence)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return sequence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "id": "5xN1Mc_I0Ok4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the environment\n",
    "def train_qmix(env, name_experiment, agent_q_networks, mixing_network, target_q_networks, replay_buffer, train_p, learn_p, random_enemy=False):\n",
    "    n_episodes = train_p.n_episodes\n",
    "    batch_size = train_p.batch_size\n",
    "    epsilon = train_p.epsilon\n",
    "    epsilon_min = train_p.epsilon_min\n",
    "    epsilon_decay = train_p.epsilon_decay\n",
    "    gamma = train_p.gamma\n",
    "    target_update_frequency = train_p.target_update_frequency\n",
    "\n",
    "    lr_agent = learn_p.lr_agent\n",
    "    lr_mix = learn_p.lr_mix\n",
    "    learn_gamma = learn_p.gamma\n",
    "\n",
    "    optimizer = optim.Adam([param for net in agent_q_networks for param in net.parameters()], lr=lr_agent)\n",
    "    mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=lr_mix)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learn_gamma)\n",
    "    scheduler_mix = torch.optim.lr_scheduler.ExponentialLR(mixing_optimizer, gamma=learn_gamma)\n",
    "\n",
    "    steps_counter = 0\n",
    "    legal_actions = [0, 1, 2, 3, 4]\n",
    "    agent_indexes = [1, 3]\n",
    "    episode_rewards = []\n",
    "    win_rates = []\n",
    "    enemieName = env.enemieName\n",
    "\n",
    "    if random_enemy:\n",
    "        enemylist = ['randomTeam.py', 'baselineTeam.py', 'heuristicTeam.py', 'approxQTeam.py', 'MCTSTeam.py', 'AstarTeam.py']\n",
    "        weights = get_dynamic_weights(0, n_episodes)\n",
    "        enemieName = np.random.choice(enemylist, p=weights)\n",
    "\n",
    "    create_folder_if_not_exists(name_experiment)\n",
    "    for episode in range(n_episodes):\n",
    "        print(f\"enemy: {enemieName}\")\n",
    "        done = {agent_id: False for agent_id in agent_indexes}\n",
    "        weights = get_dynamic_weights(episode, n_episodes)\n",
    "        if random_enemy:\n",
    "            enemieName = np.random.choice(enemylist, p=weights)\n",
    "        env.reset(enemieName=enemieName)\n",
    "        blue_player1_reward = 0\n",
    "        blue_player2_reward = 0\n",
    "        score = 0\n",
    "\n",
    "        hidden_states = torch.stack([torch.stack([agent.init_hidden(1)[0].to(device), agent.init_hidden(1)[1].to(device)])  for agent in agent_q_networks]).clone()\n",
    "        next_hidden_states = torch.stack([torch.stack([agent.init_hidden(1)[0].to(device), agent.init_hidden(1)[1].to(device)])  for agent in agent_q_networks]).clone()\n",
    "        \n",
    "        state_buffers = [deque(maxlen=5) for _ in range(len(agent_indexes))]\n",
    "\n",
    "        while not all(done.values()):\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "            states = []\n",
    "\n",
    "            for i, agent_index in enumerate(agent_indexes):\n",
    "                obs_agent = env.get_Observation(agent_index)\n",
    "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                states.append(state)\n",
    "\n",
    "                \n",
    "                input_sequence = prepare_input_sequence(state_buffers[i],state, 5)\n",
    "        \n",
    "                \n",
    "                action, next_hidden_states[i, :, :, :, :] = epsilon_greedy_action(agent_q_networks[i], input_sequence, hidden_states[i, :, :, :, :], epsilon, legal_actions)\n",
    "                actions[agent_index] = action\n",
    "            \n",
    "\n",
    "            next_states, rewards, terminations, info = env.step(actions)\n",
    "            score -= info[\"score_change\"]\n",
    "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
    "            blue_player1_reward += rewards[1]\n",
    "            blue_player2_reward += rewards[3]\n",
    "\n",
    "            next_states_converted = []\n",
    "            rewards_converted = []\n",
    "            terminations_converted = []\n",
    "            actions_converted = []\n",
    "\n",
    "            for index in agent_indexes:\n",
    "                next_states_converted.append(torch.tensor(next_states[index], dtype=torch.float32))\n",
    "                rewards_converted.append(rewards[index])\n",
    "                terminations_converted.append(terminations[index])\n",
    "                actions_converted.append(actions[index])\n",
    "\n",
    "            # Ensure hidden states are properly detached before adding to the replay buffer\n",
    "            replay_buffer.add((states, actions_converted, rewards_converted, next_states_converted, terminations_converted, hidden_states, next_hidden_states))\n",
    "            \n",
    "            hidden_states = next_hidden_states.detach().clone()\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                loss = compute_td_loss(agent_q_networks, mixing_network, target_q_networks, batch, gamma=gamma)\n",
    "                wandb.log({\"loss\": loss})\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                mixing_optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                mixing_optimizer.step()\n",
    "                scheduler.step()\n",
    "                scheduler_mix.step()\n",
    "\n",
    "        steps_counter += 1\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        wandb.log({\"blue_player1_reward\": blue_player1_reward})\n",
    "        wandb.log({\"blue_player2_reward\": blue_player2_reward})\n",
    "        wandb.log({\"episode\": episode})\n",
    "        wandb.log({\"score\": score})\n",
    "        wandb.log({\"enemy\": enemieName})\n",
    "\n",
    "        if (episode + 1) % target_update_frequency == 0:\n",
    "            update_target_network(agent_q_networks, target_q_networks)\n",
    "\n",
    "        if (episode + 1) % target_update_frequency == 0:\n",
    "            torch.save(agent_q_networks[0].state_dict(), f'{name_experiment}/agent1_{episode + 1}.pth')\n",
    "            torch.save(agent_q_networks[1].state_dict(), f'{name_experiment}/agent3_{episode + 1}.pth')\n",
    "            torch.save(mixing_network.state_dict(), f'{name_experiment}/mix_{episode + 1}.pth')\n",
    "\n",
    "        episode_rewards.append(blue_player1_reward + blue_player2_reward)\n",
    "        print(f\"Episode reward: {blue_player1_reward + blue_player2_reward}\")\n",
    "\n",
    "        if np.sum(env.game.state.getBlueFood().data) == 0:\n",
    "            win_rates.append(1)\n",
    "        else:\n",
    "            win_rates.append(0)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_win_rate = np.mean(win_rates[-10:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}, Win Rate: {avg_win_rate:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards, label=\"Average Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Average Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smoothed_win_rates = np.convolve(win_rates, np.ones(10)/10, mode='valid') if len(win_rates) > 10 else win_rates\n",
    "    plt.plot(smoothed_win_rates, label=\"Win Rate (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.title(\"Win Rate Over Episodes\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class train_param:\n",
    "    n_episodes: int = 100\n",
    "    batch_size: int = 32\n",
    "    gamma: float = 0.995\n",
    "    epsilon: float = 0.95\n",
    "    epsilon_min: float = 0.1\n",
    "    epsilon_decay: float = 0.995\n",
    "    target_update_frequency: int = 25\n",
    "    sequence: bool = False\n",
    "\n",
    "@dataclass\n",
    "class learn_param:\n",
    "    lr_mix: float = 0.0001\n",
    "    lr_agent: float = 0.0005\n",
    "    gamma: float = 0.99\n",
    "\n",
    "@dataclass\n",
    "class net_param:\n",
    "    hidden_dim: int = 64\n",
    "    d_model: int = 32\n",
    "    n_heads: int = 2\n",
    "    n_layers: int = 2\n",
    "\n",
    "train_p = train_param()\n",
    "learn_p = learn_param()\n",
    "net_p = net_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrI2Ff3uoJ6J",
    "outputId": "4f8ac6a3-c3e9-4393-9bc7-7fa2531f7f92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Get number of agents and their observation/action space dimensions\n",
    "n_agents = int(len(env.agents) / 2)\n",
    "action_dim_individual_agent = 5  # North, South, East, West, Stop\n",
    "\n",
    "obs_individual_agent = env.get_Observation(0)\n",
    "obs_shape = obs_individual_agent.shape\n",
    "\n",
    "# parameters\n",
    "\n",
    "\n",
    "agent_q_networks = [CNNLSTMQNetwork_hidden(embed_dim=64, obs_shape=obs_shape, action_space=action_dim_individual_agent).to(device) for _ in\n",
    "                    range(n_agents)]\n",
    "\n",
    "target_q_networks = [CNNLSTMQNetwork_hidden(embed_dim=64, obs_shape=obs_shape, action_space=action_dim_individual_agent).to(device) for _ in\n",
    "                     range(n_agents)]\n",
    "\n",
    "# Initialize the mixing network\n",
    "state_shape = (2, 8, env.layout.height, env.layout.width)  # Veronderstelling: de toestand is een concatenatie van agent-observaties\n",
    "global_state_shape = [len(env.agents)] + list(obs_shape)\n",
    "print(global_state_shape)\n",
    "mixing_network = AttentionMixer(n_agents=n_agents, state_shape=state_shape).to(device)\n",
    "#mixing_network.load_state_dict(torch.load('mix_75.pth'))\n",
    "\n",
    "# Initialize target Q-networks with the same weights as the main Q-networks\n",
    "update_target_network(agent_q_networks, target_q_networks)\n",
    "\n",
    "# Initialize the replay buffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DK95aBtt0T6U",
    "outputId": "8b427826-7e4a-4ade-8939-f82ee387a963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josse\\AppData\\Local\\Temp\\ipykernel_28884\\4064633224.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
      "C:\\Users\\josse\\AppData\\Local\\Temp\\ipykernel_28884\\4064633224.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_states_converted.append(torch.tensor(next_states[index], dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 7.539999999999999\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 6.079999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 5.940000000000001\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 68.48\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 5.799999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 7.119999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 8.039999999999994\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 6.919999999999984\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 7.999999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 7.739999999999995\n",
      "Episode 10/100, Average Reward: 13.17, Win Rate: 0.00\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 69.84\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 68.21999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 5.060000000000002\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 6.939999999999993\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 72.01999999999997\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 7.519999999999988\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 6.499999999999986\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 5.419999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 67.55999999999999\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 9.859999999999978\n",
      "Episode 20/100, Average Reward: 31.89, Win Rate: 0.00\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 66.94\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 7.419999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n",
      "Episode reward: 7.419999999999998\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "Episode reward: 7.499999999999989\n",
      "enemy: randomTeam.py\n",
      "Loading Team: c:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory LSTM_hidden_tournament2_tiny does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[470], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Profile the train_qmix function and save the results to a file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m profile_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_qmix_profile.prof\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mcProfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_qmix(env, name_experiment, agent_q_networks, mixing_network, target_q_networks, replay_buffer, train_p, learn_p, random_enemy=False)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load the profiling results\u001b[39;00m\n\u001b[0;32m     16\u001b[0m profile_stats \u001b[38;5;241m=\u001b[39m pstats\u001b[38;5;241m.\u001b[39mStats(profile_filename)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2288.0_x64__qbz5n2kfra8p0\\Lib\\cProfile.py:18\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(statement, filename, sort)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(statement, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pyprofile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Utils\u001b[49m\u001b[43m(\u001b[49m\u001b[43mProfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2288.0_x64__qbz5n2kfra8p0\\Lib\\profile.py:55\u001b[0m, in \u001b[0;36m_Utils.run\u001b[1;34m(self, statement, filename, sort)\u001b[0m\n\u001b[0;32m     53\u001b[0m prof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     \u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2288.0_x64__qbz5n2kfra8p0\\Lib\\cProfile.py:97\u001b[0m, in \u001b[0;36mProfile.run\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m__main__\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m __main__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2288.0_x64__qbz5n2kfra8p0\\Lib\\cProfile.py:102\u001b[0m, in \u001b[0;36mProfile.runctx\u001b[1;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable()\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[464], line 114\u001b[0m, in \u001b[0;36mtrain_qmix\u001b[1;34m(env, name_experiment, agent_q_networks, mixing_network, target_q_networks, replay_buffer, train_p, learn_p, random_enemy)\u001b[0m\n\u001b[0;32m    111\u001b[0m     update_target_network(agent_q_networks, target_q_networks)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m target_update_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_q_networks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_experiment\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/agent1_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepisode\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(agent_q_networks[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_experiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/agent3_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(mixing_network\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_experiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mix_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX\\venv\\Lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX\\venv\\Lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josse\\OneDrive\\Documenten\\creditcontract\\RL\\RL_MARL_QMIX\\venv\\Lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory LSTM_hidden_tournament2_tiny does not exist."
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "replay_buffer = ReplayBuffer_sequence()\n",
    "name_experiment ='LSTM_hidden_tournament2_tiny'\n",
    "wandb.init(project=\"\", id = f\"{name_experiment}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "\n",
    "env = gymPacMan_parallel_env(display=True, layout_file=\"layouts/win.lay\", length=150, enemieName='randomTeam.py', defenceReward=True)\n",
    "\n",
    "\n",
    "# Profile the train_qmix function and save the results to a file\n",
    "profile_filename = 'train_qmix_profile.prof'\n",
    "cProfile.run('train_qmix(env, name_experiment, agent_q_networks, mixing_network, target_q_networks, replay_buffer, train_p, learn_p, random_enemy=False)', profile_filename)\n",
    "\n",
    "# Load the profiling results\n",
    "profile_stats = pstats.Stats(profile_filename)\n",
    "\n",
    "# Sort the results by cumulative time and print them\n",
    "profile_stats.sort_stats('cumtime').print_stats()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxm-zQIIoJ6K"
   },
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3qJ2lOZ-Jk6"
   },
   "source": [
    "## 1.5 Reflection Questions\n",
    "\n",
    "Evaluate your results on \"smallCapture.lay\" against random agents and answer the following questions:\n",
    "\n",
    "*  How do your QMix agents improve over time during the training?\n",
    "*  How does the performance of QMix compare to the performance of IQL?\n",
    "*  Do you observe different roles for the agents within a team?\n",
    "*  What other reflection questions can you think of yourself?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You’ve now implemented the QMix algorithm for the PacMan capture the flag environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7hGqk6LKeEg"
   },
   "source": [
    "# **Section 2: Improving the training strategy**\n",
    "\n",
    "In this section, you will design and implement an improved training strategy for the blue Pacman agents on the map \"bloxCapture.lay\". Don't try to change the architecture too much, to look at the effect of the training strategy. The objective is to train the agents effectively while considering the challenges of multi-agent environments, such as who you play against, exploration, credit assignment, and balancing exploitation with learning stability. You don't have to do everything. Choose some objective you want to study, do some experiments, and interpret your results. You have to do the first topic and at least one more, but feel free to choose how many you do.\n",
    "\n",
    "## 2.1 Understanding Training Challenges\n",
    "\n",
    "Before jumping into implementation, reflect on the following challenges in training multi-agent systems:\n",
    "1. Which enemies?:\n",
    "* When do you play against which enemies? So that it does not become too easy or is not too hard from the start.\n",
    "* How well does self-play perform?\n",
    "2.\tExploration vs. Exploitation:\n",
    "* How will you encourage the agents to explore diverse strategies without overcommitting to suboptimal behaviors early on?\n",
    "* Will you use an exploration strategy like epsilon-greedy, where agents take random actions with some probability, or another method such as Boltzmann exploration?\n",
    "* Should the exploration strategy be adaptive, changing over time as the agents learn?\n",
    "4.\tStability of Learning:\n",
    "* What techniques can you use to avoid instabilities like divergent Q-values or poor convergence?\n",
    "5.\tCoordination Between Agents:\n",
    "* How do you ensure the agents learn coordinated policies rather than acting independently?\n",
    "6.\tReward Shaping and Credit Assignment:\n",
    "* How can you ensure that each agent is rewarded fairly for their contributions to the team’s success?\n",
    "* Should you use a dense reward (frequent small rewards) or a sparse reward (large rewards at key events)?\n",
    "7.\tReplay Buffer Usage\n",
    "* How can you ensure the replay buffer stores diverse experiences, and how frequently will you sample from it? And which experiences do you sample more than others?\n",
    "\n",
    "## 2.2 Reflection Questions\n",
    "\n",
    "*  For each training challenge, what have you tried, and why?  \n",
    "*   Did your agents improve, and how do you know?\n",
    "*   What trends do you observe in the rewards over episodes, how does the score and loss evolve?\n",
    "* If the agents are not learning effectively, what modifications would you make to your strategy?\n",
    "* What other reflection questions can you think of yourself?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGslIWIeMWSS"
   },
   "source": [
    "# **Section 3: Experimenting with Your Own Algorithm**\n",
    "\n",
    "In this section, you are encouraged to explore beyond the QMix algorithm. The goal is for you to select a different multi-agent reinforcement learning algorithm or propose your own modifications to QMix. You will implement and test this algorithm on the Pacman Capture the Flag environment on the map \"bloxCapture.lay\". Finally, also try train your algorithm on random maps.\n",
    "\n",
    "This section is open-ended, allowing you to experiment and think critically about the challenges and opportunities in multi-agent learning.\n",
    "## 3.1 Suggested Directions\n",
    "\n",
    "Here are some ideas to get you started. You may choose one of these or propose a completely new direction:\n",
    "1.\tPolicy Gradient Approaches\n",
    "\n",
    "* Implement a multi-agent Proximal Policy Optimization (PPO) or Actor-Critic algorithm.\n",
    "* How do policy gradient methods handle coordination between agents compared to value-based methods like QMix?\n",
    "2.\tCounterfactual Multi-Agent Policy Gradients (COMA)\n",
    "* Explore COMA, which uses counterfactual baselines to address the credit assignment problem.\n",
    "* How does COMA adjust the contribution of each agent to the team’s reward?\n",
    "3.\tModifications to QMix\n",
    "* Experiment with a different mixer architecture. For example, add attention mechanisms or use more complex neural networks for mixing Q-values.\n",
    "* Modify the reward function to encourage specific behaviors like offense or defense in the game.\n",
    "4.\tHybrid Approaches\n",
    "* Combine value-based and policy-gradient methods, such as integrating PPO with QMix.\n",
    "5. Have an idea for something else, go right ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sajuh8nfMkTW"
   },
   "source": [
    "## 3.2 Reflection Questions\n",
    "\n",
    "After implementing your chosen algorithm, reflect on the following:\n",
    "\n",
    "1. Design choices\n",
    "* Which things did you implement, and why?\n",
    "* Explain your training strategies.\n",
    "\n",
    "2.\tPerformance\n",
    "* How does your algorithm perform compared to standard QMix?\n",
    "\n",
    "3.\tStrengths and Weaknesses\n",
    "* What are the strengths of your chosen approach in the multi-agent Pacman environment?\n",
    "* What are the weaknesses or challenges you encountered?\n",
    "\n",
    "4.\tCoordination\n",
    "* Did your algorithm encourage better coordination between agents? Why or why not?\n",
    "\n",
    "5.\tGeneralization\n",
    "* How well do you think your algorithm generalizes to random maps? Did you have to change something to make this work?\n",
    "\n",
    "What other reflection questions can you think of yourself?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkRulQ_cMn-g"
   },
   "outputs": [],
   "source": [
    "## mogelijke upgrade buffer\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000, alpha=0.6):\n",
    "        \"\"\"\n",
    "        Prioritized Replay Buffer\n",
    "        :param buffer_size: Maximum number of experiences in the buffer\n",
    "        :param alpha: Degree of prioritization (0 = no prioritization, 1 = full prioritization)\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.priorities = deque(maxlen=buffer_size)  # Stores priorities of experiences\n",
    "        self.alpha = alpha  # Controls the level of prioritization\n",
    "\n",
    "    def add(self, experience, priority=1.0):\n",
    "        \"\"\"\n",
    "        Add an experience to the buffer with an initial priority.\n",
    "        :param experience: Tuple (state, action, reward, next_state, done)\n",
    "        :param priority: Initial priority of the experience\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences, using prioritized sampling.\n",
    "        :param batch_size: Number of experiences to sample\n",
    "        :param beta: Degree of importance-sampling correction (0 = no correction, 1 = full correction)\n",
    "        :return: Tuple (states, actions, rewards, next_states, dones, weights, indices)\n",
    "        \"\"\"\n",
    "        # Compute sampling probabilities\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        # Sample indices based on priorities\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # Importance-sampling weights\n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** -beta\n",
    "        weights /= weights.max()  # Normalize weights to [0, 1]\n",
    "\n",
    "        # Restructure the batch\n",
    "        states = np.array([exp[0].cpu().numpy() for exp in experiences], dtype=np.float32)\n",
    "        actions = np.array([exp[1] for exp in experiences], dtype=np.int64)\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3].cpu().numpy() for exp in experiences])\n",
    "        dones = np.array([exp[4] for exp in experiences])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"\n",
    "        Update the priorities of sampled experiences.\n",
    "        :param indices: Indices of the sampled experiences\n",
    "        :param priorities: New priorities for these experiences\n",
    "        \"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o7eoBGNa9rO"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
