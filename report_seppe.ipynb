{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report Group 12** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading packages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/seppe/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from gymPacMan_new_reward import gymPacMan_parallel_env\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "wandb.login(key = '', relogin = True)\n",
    "team_name = 'RL_PacMan'\n",
    "\n",
    "from util_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 1: All functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Networks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentQNetwork(nn.Module):\n",
    "    def __init__(self,obs_shape, action_dim,hidden_dim=64):\n",
    "        super(AgentQNetwork, self).__init__()\n",
    "        self.c1 = nn.Conv2d(obs_shape[0], 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.c2 = nn.Conv2d(32, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.c3 = nn.Conv2d(hidden_dim, 4*hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.relu(self.c1(obs))\n",
    "        x = torch.relu(self.c2(x))\n",
    "        x = torch.relu(self.c3(x))\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN-LSTM-Q Network\n",
    "\n",
    "\n",
    "class CNNLSTMQNetwork_hidden(nn.Module):\n",
    "    def __init__(self, embed_dim, obs_shape, action_space):\n",
    "        super(CNNLSTMQNetwork_hidden, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Shared encoder for the agent's state\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling to handle varying input sizes\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to project encoded state to embed_dim\n",
    "        self.fc = nn.Linear(128 * 1 * 1, embed_dim)\n",
    "\n",
    "        # LSTM layer to model temporal dependencies\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Final layer to compute Q-values\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, action_space),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "    def forward(self, state, hidden):\n",
    "        # Process single state using the shared encoder\n",
    "        cnn_features = []\n",
    "        for t in range(5):\n",
    "            \n",
    "            encoded_state = self.shared_encoder(state[:, t, :, :, :])  # Encode the state\n",
    "            encoded_state = encoded_state.view(encoded_state.size(0), -1)  # Flatten to [batch_size, 128 * 1 * 1]\n",
    "            projected_state = self.fc(encoded_state).unsqueeze(1)  # Project to embed_dim and add sequence dimension\n",
    "            cnn_features.append(projected_state)\n",
    "            \n",
    "        cnn_features = torch.cat(cnn_features, dim=1)  # Shape: (batch_size, seq_len, cnn_output_size)\n",
    "        # Apply LSTM to model temporal dependencies\n",
    "        hx, cx = hidden[0, :, :, :], hidden[1, :, :, :]\n",
    "        lstm_out, (hx, cx) = self.lstm(cnn_features, (hx, cx))\n",
    "        hidden_new = torch.stack([hx.detach(), cx.detach()], dim=0)\n",
    "        # Use the output of the LSTM for the final layer\n",
    "        q_values = self.final_layer(lstm_out[:, -1, :])  # Shape: [batch_size, action_space]\n",
    "\n",
    "        return q_values, hidden_new\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.embed_dim),\n",
    "                torch.zeros(1, batch_size, self.embed_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape):\n",
    "        \"\"\"\n",
    "        Simple QMIX Mixing Network with Convolutional layers.\n",
    "        Args:\n",
    "            n_agents (int): Number of agents.\n",
    "            state_shape (tuple): Shape of the agent-specific input (n_agents, channels, H, W).\n",
    "        \"\"\"\n",
    "        super(SimpleQMixer, self).__init__()\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(state_shape[1], 16, kernel_size=3, stride=1, padding=1),  # state_shape[1] = number of channels\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        dummy_input = torch.zeros(1, state_shape[1], state_shape[2], state_shape[3])  # [batch_size=1, channels, H, W]\n",
    "        conv_output_size = self.conv(dummy_input).view(1, -1).size(1)\n",
    "        \n",
    "        # Hypernetwork layers\n",
    "        self.hyper_w = nn.Sequential(\n",
    "            nn.Linear(conv_output_size * n_agents, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_agents)\n",
    "        )\n",
    "        \n",
    "        self.hyper_b = nn.Sequential(\n",
    "            nn.Linear(conv_output_size * n_agents, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, agent_qs, agent_states):\n",
    "        \"\"\"\n",
    "        Forward pass for the mixing network with Convolutional layers.\n",
    "        Args:\n",
    "            agent_qs (torch.Tensor): Tensor of shape [batch_size, n_agents] containing Q-values for each agent.\n",
    "            agent_states (torch.Tensor): Tensor of shape [batch_size, n_agents, channels, H, W].\n",
    "        Returns:\n",
    "            torch.Tensor: Global Q-value of shape [batch_size, 1].\n",
    "        \"\"\"\n",
    "        batch_size, num_agents, channels, height, width = agent_states.shape\n",
    "        \n",
    "        agent_states_r = agent_states.view(batch_size * num_agents, channels, height, width)  # [batch_size * n_agents, channels, H, W]\n",
    "        \n",
    "        conv_features = self.conv(agent_states_r)  # [batch_size * n_agents, conv_channels, H, W]\n",
    "        flattened_features = conv_features.view(batch_size, num_agents, -1)  # [batch_size, n_agents, conv_output_size]\n",
    "        \n",
    "        combined_features = flattened_features.view(batch_size, -1)  # [batch_size, n_agents * conv_output_size]\n",
    "        \n",
    "        w = torch.abs(self.hyper_w(combined_features)).view(batch_size, self.n_agents, 1)\n",
    "        b = self.hyper_b(combined_features).view(batch_size, 1, 1)\n",
    "        \n",
    "        mixed_qs = torch.bmm(agent_qs.unsqueeze(1), w).squeeze(1) + b\n",
    "        \n",
    "        return mixed_qs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape=(2, 8, 10, 20), embed_dim=32, n_heads=4):\n",
    "        super(AttentionMixer, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.state_shape = state_shape[1:]  # Remove agent dimension\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Shared Conv2D encoder for each agent\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Conv2d(self.state_shape[0], 16, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to project to embed_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, embed_dim),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "        for m in self.shared_encoder:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "        for m in self.fc:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        # Multi-head attention for inter-agent interaction\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads, batch_first=True)\n",
    "\n",
    "        # Hypernetwork for mixing weights_enemy and biases\n",
    "        self.hyper_w = nn.Sequential(\n",
    "            nn.Linear(embed_dim * n_agents, embed_dim * embed_dim),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Linear(embed_dim * embed_dim, n_agents * embed_dim)\n",
    "        )\n",
    "        self.hyper_b = nn.Sequential(\n",
    "            nn.Linear(embed_dim * n_agents, embed_dim),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Final layer to compute Q_tot\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "        for m in self.hyper_w:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        for m in self.hyper_b:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "        for m in self.final_layer:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, 0.9, 1.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "        batch_size = states.size(0)\n",
    "\n",
    "        # Process each agent's state individually using the shared encoder\n",
    "        agent_features = []\n",
    "        for i in range(self.n_agents):\n",
    "            agent_state = states[:, i, :, :, :]  # Shape: [batch_size, 8, 10, 20]\n",
    "            encoded_state = self.shared_encoder(agent_state)  # Encode individual agent\n",
    "            encoded_state = encoded_state.view(batch_size, -1)  # Flatten to [batch_size, 128]\n",
    "            projected_state = self.fc(encoded_state)  # Project to embed_dim\n",
    "            agent_features.append(projected_state)\n",
    "\n",
    "        # Stack agent features: Shape [batch_size, n_agents, embed_dim]\n",
    "        agent_features = torch.stack(agent_features, dim=1)\n",
    "\n",
    "        # Apply attention to model inter-agent interaction\n",
    "        attention_out, _ = self.attention(agent_features, agent_features, agent_features)\n",
    "\n",
    "        # Flatten attention output for hypernetwork\n",
    "        attention_out_flat = attention_out.reshape(batch_size, -1)  # Shape: [batch_size, n_agents * embed_dim]\n",
    "\n",
    "        # Compute hypernetwork weights and biases\n",
    "        hyper_w = F.softplus(self.hyper_w(attention_out_flat).view(batch_size, self.n_agents, self.embed_dim))\n",
    "        hyper_b = self.hyper_b(attention_out_flat).view(batch_size, self.embed_dim)\n",
    "\n",
    "        # Combine agent Q-values with weights and biases\n",
    "        agent_qs = agent_qs.view(batch_size, self.n_agents, 1)  # Shape: [batch_size, n_agents, 1]\n",
    "        weighted_qs = torch.bmm(agent_qs.transpose(1, 2), hyper_w).squeeze(1) + hyper_b\n",
    "\n",
    "        # Final projection to Q_tot\n",
    "        q_tot = self.final_layer(weighted_qs).unsqueeze(1)\n",
    "        return q_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loss calculations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_td_loss_iql(agent_q_networks, target_q_networks, batch, weights=None, gamma=0.99, lambda_=0.1):\n",
    "    \"\"\"\n",
    "    Computes the TD loss for QMix training using the Huber loss.\n",
    "\n",
    "    Args:\n",
    "        agent_q_networks (list): List of Q-networks for each agent.\n",
    "        mixing_network (SimpleQMixer): The mixing network to compute global Q-values.\n",
    "        target_q_networks (list): List of target Q-networks for each agent.\n",
    "        batch (tuple): A batch of experiences (states, actions, rewards, next_states, dones).\n",
    "        weights (torch.Tensor): Importance sampling weights (optional).\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "        lambda_ (float): Regularization factor for stability.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Total loss for training.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Current Q-values for each agent\n",
    "    agent_q_values = []\n",
    "    for agent_index, q_net in enumerate(agent_q_networks):\n",
    "        q_vals = q_net(states[:, agent_index, :, :, :])  # Get Q-values for each agent\n",
    "        agent_q_values.append(\n",
    "            q_vals.gather(dim=1, index=actions[:, agent_index].unsqueeze(1)))  # Select Q-value for taken action\n",
    "    agent_q_values = torch.cat(agent_q_values, dim=1)  # Shape: (batch_size, n_agents)\n",
    "\n",
    "    # Target Q-values using Double DQN\n",
    "    with torch.no_grad():\n",
    "        # Get actions from current Q-networks\n",
    "        next_agent_q_values = []\n",
    "        for agent_index, (q_net, target_net) in enumerate(zip(agent_q_networks, target_q_networks)):\n",
    "            next_q_vals = q_net(next_states[:, agent_index, :, :, :])  # Get Q-values from current network\n",
    "            max_next_actions = next_q_vals.argmax(dim=1, keepdim=True)  # Greedy actions\n",
    "            target_q_vals = target_net(next_states[:, agent_index, :, :, :])  # Get Q-values from target network\\\n",
    "            max_next_q_vals = target_q_vals.gather(1, max_next_actions)\n",
    "            done_mask = dones[:, 0, 0].unsqueeze(1)\n",
    "            filtered_target_q_vals = max_next_q_vals * (1 - done_mask)\n",
    "\n",
    "            next_agent_q_values.append(filtered_target_q_vals)  # Use target Q-values for selected actions\n",
    "        next_agent_q_values = torch.cat(next_agent_q_values, dim=1)  # Shape: (batch_size, n_agents)\n",
    "\n",
    "    # Independent Q-learning target for each agent (all members of the blue team receive the same reward)\n",
    "    target_q = rewards[:, 0, 0].unsqueeze(1) + gamma * next_agent_q_values\n",
    "\n",
    "    # Compute Huber loss, try also with MSE loss\n",
    "    loss_fn = torch.nn.HuberLoss()\n",
    "\n",
    "    loss_agent1 = loss_fn(agent_q_values[:, 0], target_q[:, 0])\n",
    "    loss_agent2 = loss_fn(agent_q_values[:, 1], target_q[:, 1])\n",
    "\n",
    "    return loss_agent1, loss_agent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Q-Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss_qmix(agent_q_networks, target_q_networks, mixing_network, batch, weights = None, gamma=0.99, lambda_=0.1):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "     # Get current Q-values for each agent\n",
    "    agent_qs = []\n",
    "    for i, agent_q_network in enumerate(agent_q_networks):\n",
    "        agent_q = agent_q_network(states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "        agent_q = agent_q.gather(dim=1, index=actions[:,i].unsqueeze(1))  # Select action Q-values\n",
    "        agent_qs.append(agent_q)\n",
    "    agent_qs = torch.cat(agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
    "    # Get target Q-values for each agent using Double DQN\n",
    "    with torch.no_grad():\n",
    "        target_agent_qs = []\n",
    "        for i, (agent_q_network, target_q_network) in enumerate(\n",
    "            zip(agent_q_networks, target_q_networks)\n",
    "        ):\n",
    "            # Double DQN: Get greedy actions from the current network\n",
    "            next_q_values = agent_q_network(next_states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "            greedy_actions = torch.argmax(next_q_values, dim=1, keepdim=True)\n",
    "            # Get target Q-values using the target network\n",
    "            target_q_values = target_q_network(next_states[:, i, :, :, :])  # Shape: (batch_size, num_actions)\n",
    "            target_q = target_q_values.gather(1, greedy_actions)\n",
    "            done_mask = dones[:, 0, 0].unsqueeze(1)\n",
    "            filtered_target_q = target_q * (1 - done_mask)\n",
    "            target_agent_qs.append(filtered_target_q)\n",
    "        target_agent_qs = torch.cat(target_agent_qs, dim=1)  # Shape: (batch_size, n_agents)\n",
    "\n",
    "        # Compute the total target Q-value using the mixing network\n",
    "        target_q_tot = mixing_network(target_agent_qs, next_states).squeeze(1)   # Shape: (batch_size, 1)\n",
    "    target_q_tot = rewards.sum(dim=2) + gamma * target_q_tot\n",
    "\n",
    "    \n",
    "    # Compute the global Q-value using the mixing network\n",
    "    q_tot = mixing_network(agent_qs, states).squeeze(1)  # Shape: (batch_size, 1)\n",
    "    # Compute Huber loss\n",
    "    td_loss = F.huber_loss(target_q_tot, q_tot,  reduction=\"none\")\n",
    "\n",
    "    # Apply weights if prioritized experience replay is used\n",
    "    if weights is not None:\n",
    "        weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "        td_loss *= weights\n",
    "\n",
    "    # Return the mean loss\n",
    "    total_loss = td_loss.mean()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Q-Learning for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Replay buffers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Restructure the batch into separate arrays for states, actions, rewards, next_states, and dones\n",
    "        states = np.array([exp[0].cpu().numpy() for exp in experiences], dtype=np.float32)\n",
    "        actions = np.array([exp[1] for exp in experiences], dtype=np.int64)\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3].cpu().numpy() for exp in experiences])\n",
    "        dones = np.array([exp[4] for exp in experiences])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentPrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000, alpha=0.6, num_agents=2):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.alpha = alpha\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4, q_networks=None, target_networks=None):\n",
    "\n",
    "        states = np.array([exp[0].cpu().numpy() for exp in self.buffer], dtype=np.float32)\n",
    "        actions = np.array([exp[1] for exp in self.buffer], dtype=np.int64)\n",
    "        rewards = np.array([exp[2] for exp in self.buffer])\n",
    "        next_states = np.array([exp[3].cpu().numpy() for exp in self.buffer])\n",
    "        dones = np.array([exp[4] for exp in self.buffer])\n",
    "        td_errors = self.compute_td_errors(states,actions,rewards,next_states,q_networks, target_networks)\n",
    "\n",
    "        priorities = np.max(td_errors, axis=1) + 1e-6\n",
    "        print(priorities.shape)\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        print(probabilities.shape)\n",
    "        print(len(self.buffer))\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        weights = ((len(self.buffer) * probabilities[indices]) ** -beta).reshape(32, 1)\n",
    "\n",
    "        states_batch, actions_batch, rewards_batch, next_states_batch, terminations_batch = zip(*experiences)\n",
    "        states_batch = np.array([x.cpu().numpy() for x in states_batch], dtype=np.float32)\n",
    "        actions_batch = np.array([x for x in actions_batch], dtype=np.int64)\n",
    "        rewards_batch = np.array([x for x in rewards_batch], dtype=np.float32)\n",
    "        next_states_batch = np.array([x.cpu().numpy() for x in next_states_batch], dtype=np.float32)\n",
    "        terminations_batch = np.array([x for x in terminations_batch], dtype=np.float32)\n",
    "\n",
    "        return states_batch, actions_batch, rewards_batch, next_states_batch, terminations_batch, weights\n",
    "\n",
    "    def compute_td_errors(self, state, action, reward, next_state, q_networks, target_networks):\n",
    "        states = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(action, dtype=torch.long).to(device)\n",
    "        rewards = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "        next_states = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "        agent_q_values=[]\n",
    "        target_q_values = []\n",
    "        for agent_idx in range(len(q_networks)):\n",
    "\n",
    "            q_vals = q_networks[agent_idx](states[:, agent_idx, :, :, :])\n",
    "            agent_q_values.append(q_vals.gather(dim=1, index=actions[:, agent_idx].unsqueeze(1)))  # Select Q-value for taken action\n",
    "            \n",
    "            next_q_values = target_networks[agent_idx](states[:, agent_idx, :, :, :])\n",
    "            greedy_actions = next_q_values.argmax( dim=1, keepdim= True)\n",
    "            target_q_values.append(q_vals.gather(dim=1, index=greedy_actions))\n",
    "        \n",
    "        target_q_values = torch.cat(target_q_values, dim=1)\n",
    "        agent_q_values = torch.cat(agent_q_values, dim=1)  # Shape: (batch_size, n_agents)\n",
    "        # TD-error: δ = Q(s, a) - target\n",
    "\n",
    "        td_error = torch.abs(rewards.squeeze(1) + target_q_values - agent_q_values)\n",
    "        td_error = np.array(td_error.cpu().detach().numpy())\n",
    "        return td_error\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer for sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer_sequence:\n",
    "    def __init__(self, buffer_size=10000, sequence_length=5):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        sequences = []\n",
    "        for _ in range(batch_size):\n",
    "            start_index = random.randint(0, len(self.buffer) - self.sequence_length)\n",
    "            sequence = [self.buffer[i] for i in range(start_index, start_index + self.sequence_length)]\n",
    "            sequences.append(sequence)\n",
    "        \n",
    "        # Transpose the sequences to get batches of each component\n",
    "        \n",
    "        states = torch.stack([torch.stack([torch.stack(seq[0]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        actions = torch.stack([torch.stack([torch.tensor(seq[1]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        rewards = torch.stack([torch.stack([torch.tensor(seq[2]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        next_states = torch.stack([torch.stack([torch.stack(seq[3]).to(device) for seq in sequence]) for sequence in sequences], dim=0)\n",
    "        terminations = torch.stack([torch.stack([torch.tensor(seq[4], dtype=torch.int).to(device) for seq in sequence]) for sequence in sequences], dim=2)\n",
    "        hidden_states = torch.cat([sequence[-1][5] for sequence in sequences], dim = -2).to(device)\n",
    "        next_hidden_states = torch.cat([sequence[-1][6] for sequence in sequences], dim = -2).to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, terminations, hidden_states, next_hidden_states)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epsilon greedy, update and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted epsilon-greedy action function\n",
    "def epsilon_greedy_action(agent_q_network, state, epsilon, legal_actions, exploring = True):\n",
    "    \n",
    "    if random.random() < epsilon and exploring:\n",
    "        # Explore: take a random action\n",
    "        action = random.choice(legal_actions)\n",
    "    else:\n",
    "        state = state.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        q_values = agent_q_network(state)\n",
    "        q_values = q_values.cpu().detach().numpy()\n",
    "        action = np.random.choice(np.flatnonzero(q_values == q_values.max()))\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_target_network(agent_q_networks, target_q_networks):\n",
    "    for target, source in zip(target_q_networks, agent_q_networks):\n",
    "        target.load_state_dict(source.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_weights(start_weights, end_weights, factor):\n",
    "    return [(1 - factor) * start + factor * end for start, end in zip(start_weights, end_weights)]\n",
    "\n",
    "def get_dynamic_weights(episode, n_episodes):\n",
    "    # Define the initial weights for the beginning, middle, and end\n",
    "    weights_beginning = [1, 0, 0, 0, 0, 0]\n",
    "    weights_middle = [0, 0.33, 0.34, 0.33, 0, 0]\n",
    "    weights_end = [0, 0, 0, 0, 0.5, 0.5]\n",
    "\n",
    "    # Calculate the transition points\n",
    "    transition1 = n_episodes // 3\n",
    "    transition2 = 2 * n_episodes // 3\n",
    "\n",
    "    # Adjust weights based on the current episode\n",
    "    if episode < transition1:\n",
    "        factor = episode / transition1\n",
    "        return interpolate_weights(weights_beginning, weights_middle, factor)\n",
    "    elif episode < transition2:\n",
    "        factor = (episode - transition1) / (transition2 - transition1)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    else:\n",
    "        factor = (episode - transition2) / (n_episodes - transition2)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    \n",
    "\n",
    "import os\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## individual reward\n",
    "from capture import AgentRules\n",
    "def individual_reward(env,agentIndex,action, reward, distancer, counter):\n",
    "\n",
    "    next_state = env.game.state.generateSuccessor(agentIndex, env.action_mapping[action])\n",
    "    # --- Reward for capturing food ---\n",
    "    if env.game.state.getAgentState(agentIndex).numCarrying < next_state.getAgentState(agentIndex).numCarrying:\n",
    "        reward += 1\n",
    "\n",
    "    # --- Reward for delivering food ---\n",
    "    if env.game.state.getAgentState(agentIndex).numCarrying > 0 and next_state.getAgentState(agentIndex).numCarrying == 0 and env.game.state.getAgentState(agentIndex).start.pos != next_state.getAgentPosition(agentIndex):\n",
    "        print('gave food to side')\n",
    "        reward += (env.game.state.getAgentState(agentIndex).numCarrying)*2\n",
    "\n",
    "    # --- Maze Distance penalty ---\n",
    "    start_position = env.game.state.getAgentState(agentIndex).start.pos\n",
    "    next_position = next_state.getAgentPosition(agentIndex)\n",
    "    maze_distance = distancer.getDistance(next_position, start_position)\n",
    "\n",
    "    radius = 10\n",
    "    if maze_distance <= radius and counter<=0:\n",
    "        reward -= (radius - maze_distance)*0.001\n",
    "\n",
    "    #--- legal actions reward ---\n",
    "    legal_actions = AgentRules.getLegalActions(env.game.state, agentIndex)\n",
    "    if env.action_mapping[action] in legal_actions:\n",
    "        reward += 0.05  \n",
    "    else:\n",
    "        reward -= 0.05\n",
    "\n",
    "           \n",
    "    for red_team_index in [0, 2]:\n",
    "        if (env.game.state.data.agentStates[red_team_index].isPacman and\n",
    "                not next_state.data.agentStates[red_team_index].isPacman):\n",
    "            if next_state.data.agentStates[red_team_index].configuration.pos == next_state.data.agentStates[red_team_index].start.pos:\n",
    "                print('defended')\n",
    "                reward += 0.75\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 2: Training loops** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layout\n",
    "from distanceCalculator import Distancer\n",
    "\n",
    "def train_iql(env, name_experiment, agent_q_networks, target_q_networks, replay_buffer, train_p, learn_p, random_enemy=False, schedule=False, later_exploration = False):\n",
    "    \n",
    "    optimizer = optim.Adam([param for net in agent_q_networks for param in net.parameters()], lr=learn_p.lr_agent)\n",
    "    #mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=learn_p.lr_mixing)\n",
    "    if schedule:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learn_p.gamma)\n",
    "        #scheduler_mix = torch.optim.lr.scheduler.ExponentialLR(mixing_optimizer, gamma=learn_p.learn_gamma)\n",
    "   \n",
    "    n_episodes = train_p.n_episodes\n",
    "    batch_size = train_p.batch_size\n",
    "    gamma = train_p.gamma\n",
    "    epsilon = train_p.epsilon  # Initial exploration probability\n",
    "    epsilon_min = train_p.epsilon_min\n",
    "    epsilon_decay = train_p.epsilon_decay\n",
    "    target_update_frequency = train_p.target_update_frequency\n",
    "\n",
    "    l = layout.getLayout('bloxCapture.lay')\n",
    "    print(l.walls)\n",
    "    distancer = Distancer(l)\n",
    "    distancer.getMazeDistances()\n",
    "    \n",
    "    legal_actions = [0, 1, 2, 3, 4]\n",
    "    agent_indexes = [1, 3]\n",
    "    episode_rewards = []\n",
    "    win_rates = []\n",
    "    enemieName = env.enemieName\n",
    "    if random_enemy:\n",
    "        enemylist = ['randomTeam.py', 'baselineTeam.py', 'heuristicTeam.py', 'approxQTeam.py', 'MCTSTeam.py', 'AstarTeam.py']\n",
    "        weights_enemy = get_dynamic_weights(0, n_episodes)\n",
    "        enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "\n",
    "    create_folder_if_not_exists(name_experiment)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        steps_counter = 0\n",
    "        print(f\"enemy: {enemieName}\")\n",
    "        run_counter =[11,11]\n",
    "\n",
    "        done = {agent_id: False for agent_id in agent_indexes}\n",
    "        if random_enemy:\n",
    "            weights_enemy = get_dynamic_weights(episode, n_episodes)\n",
    "            enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "        env.reset(enemieName=enemieName)\n",
    "        blue_player1_reward = 0\n",
    "        blue_player2_reward = 0\n",
    "        score = 0\n",
    "        if later_exploration:\n",
    "            exploring = False\n",
    "        else:\n",
    "            exploring = True\n",
    "        while not all(done.values()):\n",
    "            if later_exploration and steps_counter > 10:\n",
    "                exploring = True\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "            states = []\n",
    "\n",
    "            reward_dist = [0,0]\n",
    "            for i, agent_index in enumerate(agent_indexes):\n",
    "                obs_agent = env.get_Observation(agent_index)\n",
    "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                states.append(state)\n",
    "                action = epsilon_greedy_action(agent_q_networks[i], state, epsilon, legal_actions, exploring)\n",
    "                actions[agent_index] = action\n",
    "                reward_dist[i] = individual_reward(env,agent_index,action,reward_dist[i],distancer,run_counter[i])\n",
    "\n",
    "\n",
    "\n",
    "            next_states, rewards, terminations, info = env.step(actions)\n",
    "            for i, index in enumerate(agent_indexes):\n",
    "                if env.game.state.getAgentState(index).start.pos == env.game.state.getAgentPosition(index):\n",
    "                    run_counter[i] = 11\n",
    "                else:\n",
    "                    run_counter[i] -=1\n",
    "            \n",
    "            score -= info[\"score_change\"]\n",
    "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
    "            blue_player1_reward += (rewards[1] - reward_dist[0])\n",
    "            blue_player2_reward += (rewards[3] - reward_dist[1])\n",
    "\n",
    "            next_states_converted = []\n",
    "            rewards_converted = []\n",
    "            terminations_converted = []\n",
    "            actions_converted = []\n",
    "\n",
    "            for index in agent_indexes:\n",
    "                next_states_converted.append(list(next_states.values())[index])\n",
    "                rewards_converted.append(rewards[index])\n",
    "                terminations_converted.append(terminations[index])\n",
    "                actions_converted.append(actions[index])\n",
    "\n",
    "            next_states_converted = torch.stack(next_states_converted)\n",
    "            states_converted = torch.stack(states)\n",
    "            rewards_converted = [rewards_converted]\n",
    "            terminations_converted = [terminations_converted]\n",
    "            replay_buffer.add(\n",
    "                (states_converted, actions_converted, rewards_converted, next_states_converted, terminations_converted))\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                loss1, loss2 = compute_td_loss_iql(agent_q_networks, target_q_networks, batch,\n",
    "                                               gamma=gamma)\n",
    "                wandb.log({\"loss1\": loss1})\n",
    "                wandb.log({\"loss2\": loss2})\n",
    "\n",
    "                # Zero gradients for all optimizers\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backpropagate once for all losses\n",
    "                loss1.backward(retain_graph=True)\n",
    "                loss2.backward()\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                if schedule:\n",
    "                    scheduler.step()\n",
    "                    wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "            steps_counter += 1\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        wandb.log({\"blue_player1_reward\": blue_player1_reward})\n",
    "        wandb.log({\"blue_player2_reward\": blue_player2_reward})\n",
    "        wandb.log({\"episode\": episode})\n",
    "        wandb.log({'score': score})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (episode + 1) % target_update_frequency == 0:\n",
    "            update_target_network(agent_q_networks, target_q_networks)\n",
    "        \n",
    "        if (episode) % target_update_frequency == 0:\n",
    "            torch.save(agent_q_networks[0].state_dict(),f'{name_experiment}/agent1_{episode+1}.pth')\n",
    "            torch.save(agent_q_networks[1].state_dict(),f'{name_experiment}/agent3_{episode+1}.pth')\n",
    "         # Log rewards and win rates\n",
    "        episode_rewards.append(blue_player1_reward+blue_player2_reward)\n",
    "        print(blue_player1_reward)\n",
    "        print(blue_player2_reward)\n",
    "        print(f\"Episode reward: {blue_player1_reward+blue_player2_reward}\")\n",
    "\n",
    "        if np.sum(env.game.state.getBlueFood().data) == 0:\n",
    "            win_rates.append(1)\n",
    "        else:\n",
    "            win_rates.append(0)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_win_rate = np.mean(win_rates[-10:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}, Win Rate: {avg_win_rate:.2f}\")\n",
    "\n",
    "    # Plot rewards and win rates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards, label=\"Average Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Average Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smoothed_win_rates = np.convolve(win_rates, np.ones(10)/10, mode='valid') if len(win_rates) > 10 else win_rates\n",
    "    plt.plot(smoothed_win_rates, label=\"Win Rate (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.title(\"Win Rate Over Episodes\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QMIX with individual rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layout\n",
    "from distanceCalculator import Distancer\n",
    "\n",
    "def train_qmix_indv_reward(env, name_experiment, agent_q_networks, target_q_networks, mixing_network, replay_buffer, train_p, learn_p, random_enemy=False, schedule=False, later_exploration = False):\n",
    "    \n",
    "    optimizer = optim.Adam([param for net in agent_q_networks for param in net.parameters()], lr=learn_p.lr_agent)\n",
    "    mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=learn_p.lr_mix)\n",
    "    if schedule:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learn_p.gamma)\n",
    "        scheduler_mix = torch.optim.lr_scheduler.ExponentialLR(mixing_optimizer, gamma=learn_p.gamma)\n",
    "   \n",
    "    n_episodes = train_p.n_episodes\n",
    "    batch_size = train_p.batch_size\n",
    "    gamma = train_p.gamma\n",
    "    epsilon = train_p.epsilon  # Initial exploration probability\n",
    "    epsilon_min = train_p.epsilon_min\n",
    "    epsilon_decay = train_p.epsilon_decay\n",
    "    target_update_frequency = train_p.target_update_frequency\n",
    "\n",
    "    l = layout.getLayout('bloxCapture.lay') \n",
    "    print(l.walls)\n",
    "    distancer = Distancer(l)\n",
    "    distancer.getMazeDistances()\n",
    "    print(f'hallo: {distancer._distances}')\n",
    "    \n",
    "    legal_actions = [0, 1, 2, 3, 4]\n",
    "    agent_indexes = [1, 3]\n",
    "    episode_rewards = []\n",
    "    win_rates = []\n",
    "    enemieName = env.enemieName\n",
    "    if random_enemy:\n",
    "        enemylist = ['randomTeam.py', 'baselineTeam.py', 'heuristicTeam.py', 'approxQTeam.py', 'MCTSTeam.py', 'AstarTeam.py']\n",
    "        weights_enemy = get_dynamic_weights(0, n_episodes)\n",
    "        enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "\n",
    "    create_folder_if_not_exists(name_experiment)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        steps_counter = 0\n",
    "        run_counter = [11,11]\n",
    "        print(f\"enemy: {enemieName}\")\n",
    "\n",
    "        done = {agent_id: False for agent_id in agent_indexes}\n",
    "        if random_enemy:\n",
    "            weights_enemy = get_dynamic_weights(episode, n_episodes)\n",
    "            enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "        env.reset(enemieName=enemieName)\n",
    "        blue_player1_reward = 0\n",
    "        blue_player2_reward = 0\n",
    "        score = 0\n",
    "        if later_exploration:\n",
    "            exploring = False\n",
    "        else:\n",
    "            exploring = True\n",
    "        while not all(done.values()):\n",
    "            if later_exploration and steps_counter > 10:\n",
    "                exploring = True\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "            states = []\n",
    "\n",
    "            indv_rewards = [0,0]\n",
    "            for i, agent_index in enumerate(agent_indexes):\n",
    "                obs_agent = env.get_Observation(agent_index)\n",
    "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                states.append(state)\n",
    "                action = epsilon_greedy_action(agent_q_networks[i], state, epsilon, legal_actions, exploring)\n",
    "                actions[agent_index] = action\n",
    "                indv_rewards[i] = individual_reward(env,agent_index,action,indv_rewards[i],distancer,run_counter[i])\n",
    "\n",
    "            next_states, rewards, terminations, info = env.step(actions)\n",
    "\n",
    "            for i, index in enumerate(agent_indexes):\n",
    "                if env.game.state.getAgentState(index).start.pos == env.game.state.getAgentPosition(index):\n",
    "                    run_counter[i] = 11\n",
    "                else:\n",
    "                    run_counter[i] -=1\n",
    "                    \n",
    "            score -= info[\"score_change\"]\n",
    "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
    "            blue_player1_reward += (rewards[1]+ indv_rewards[0])\n",
    "            blue_player2_reward += (rewards[3]+ indv_rewards[1])\n",
    "\n",
    "            next_states_converted = []\n",
    "            rewards_converted = []\n",
    "            terminations_converted = []\n",
    "            actions_converted = []\n",
    "\n",
    "            for index in agent_indexes:\n",
    "                next_states_converted.append(list(next_states.values())[index])\n",
    "                rewards_converted.append(rewards[index])\n",
    "                terminations_converted.append(terminations[index])\n",
    "                actions_converted.append(actions[index])\n",
    "\n",
    "            next_states_converted = torch.stack(next_states_converted)\n",
    "            states_converted = torch.stack(states)\n",
    "            rewards_converted = [rewards_converted]\n",
    "            terminations_converted = [terminations_converted]\n",
    "            replay_buffer.add(\n",
    "                (states_converted, actions_converted, rewards_converted, next_states_converted, terminations_converted))\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                loss = compute_td_loss_qmix(agent_q_networks, target_q_networks,mixing_network, batch,\n",
    "                                               gamma=gamma)\n",
    "                wandb.log({\"loss\": loss})\n",
    "                \n",
    "\n",
    "                # Zero gradients for all optimizers\n",
    "                optimizer.zero_grad()\n",
    "                mixing_optimizer.zero_grad()\n",
    "                # Backpropagate once for all losses\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                mixing_optimizer.step()\n",
    "                if schedule:\n",
    "                    scheduler.step()\n",
    "                    scheduler_mix.step()\n",
    "                    wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "            steps_counter += 1\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        wandb.log({\"blue_player1_reward\": blue_player1_reward})\n",
    "        wandb.log({\"blue_player2_reward\": blue_player2_reward})\n",
    "        wandb.log({\"episode\": episode})\n",
    "        wandb.log({'score': score})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (episode + 1) % target_update_frequency == 0:\n",
    "            update_target_network(agent_q_networks, target_q_networks)\n",
    "        \n",
    "        if (episode) % target_update_frequency == 0:\n",
    "            torch.save(agent_q_networks[0].state_dict(),f'{name_experiment}/agent1_{episode+1}.pth')\n",
    "            torch.save(agent_q_networks[1].state_dict(),f'{name_experiment}/agent3_{episode+1}.pth')\n",
    "         # Log rewards and win rates\n",
    "        episode_rewards.append(blue_player1_reward+blue_player2_reward)\n",
    "        print(blue_player1_reward)\n",
    "        print(blue_player2_reward)\n",
    "        print(f\"Episode reward: {blue_player1_reward+blue_player2_reward}\")\n",
    "\n",
    "        if np.sum(env.game.state.getBlueFood().data) == 0:\n",
    "            win_rates.append(1)\n",
    "        else:\n",
    "            win_rates.append(0)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_win_rate = np.mean(win_rates[-10:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}, Win Rate: {avg_win_rate:.2f}\")\n",
    "\n",
    "    # Plot rewards and win rates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards, label=\"Average Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Average Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smoothed_win_rates = np.convolve(win_rates, np.ones(10)/10, mode='valid') if len(win_rates) > 10 else win_rates\n",
    "    plt.plot(smoothed_win_rates, label=\"Win Rate (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.title(\"Win Rate Over Episodes\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QMIX with prioritized buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qmix_replay_buffer(env, name_experiment, agent_q_networks, target_q_networks, mixing_network, replay_buffer, train_p, learn_p, random_enemy=False, schedule=False, later_exploration = False):\n",
    "    \n",
    "    optimizer = optim.Adam([param for net in agent_q_networks for param in net.parameters()], lr=learn_p.lr_agent)\n",
    "    mixing_optimizer = optim.Adam(mixing_network.parameters(), lr=learn_p.lr_mix)\n",
    "    if schedule:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learn_p.gamma)\n",
    "        scheduler_mix = torch.optim.lr_scheduler.ExponentialLR(mixing_optimizer, gamma=learn_p.gamma)\n",
    "   \n",
    "    n_episodes = train_p.n_episodes\n",
    "    batch_size = train_p.batch_size\n",
    "    gamma = train_p.gamma\n",
    "    epsilon = train_p.epsilon  # Initial exploration probability\n",
    "    epsilon_min = train_p.epsilon_min\n",
    "    epsilon_decay = train_p.epsilon_decay\n",
    "    target_update_frequency = train_p.target_update_frequency\n",
    "    \n",
    "    legal_actions = [0, 1, 2, 3, 4]\n",
    "    agent_indexes = [1, 3]\n",
    "    episode_rewards = []\n",
    "    win_rates = []\n",
    "    enemieName = env.enemieName\n",
    "    if random_enemy:\n",
    "        enemylist = ['randomTeam.py', 'baselineTeam.py', 'heuristicTeam.py', 'approxQTeam.py', 'MCTSTeam.py', 'AstarTeam.py']\n",
    "        weights_enemy = get_dynamic_weights(0, n_episodes)\n",
    "        enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "\n",
    "    create_folder_if_not_exists(name_experiment)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        steps_counter = 0\n",
    "        print(f\"enemy: {enemieName}\")\n",
    "\n",
    "        done = {agent_id: False for agent_id in agent_indexes}\n",
    "        if random_enemy:\n",
    "            weights_enemy = get_dynamic_weights(episode, n_episodes)\n",
    "            enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "        env.reset(enemieName=enemieName)\n",
    "        blue_player1_reward = 0\n",
    "        blue_player2_reward = 0\n",
    "        score = 0\n",
    "        if later_exploration:\n",
    "            exploring = False\n",
    "        else:\n",
    "            exploring = True\n",
    "        while not all(done.values()):\n",
    "            if later_exploration and steps_counter > 10:\n",
    "                exploring = True\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "            states = []\n",
    "            for i, agent_index in enumerate(agent_indexes):\n",
    "                obs_agent = env.get_Observation(agent_index)\n",
    "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                states.append(state)\n",
    "                action = epsilon_greedy_action(agent_q_networks[i], state, epsilon, legal_actions, exploring)\n",
    "                actions[agent_index] = action\n",
    "\n",
    "\n",
    "            next_states, rewards, terminations, info = env.step(actions)\n",
    "          \n",
    "            score -= info[\"score_change\"]\n",
    "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
    "            blue_player1_reward += rewards[1]\n",
    "            blue_player2_reward += rewards[3]\n",
    "\n",
    "            next_states_converted = []\n",
    "            rewards_converted = []\n",
    "            terminations_converted = []\n",
    "            actions_converted = []\n",
    "\n",
    "            for index in agent_indexes:\n",
    "                next_states_converted.append(list(next_states.values())[index])\n",
    "                rewards_converted.append(rewards[index])\n",
    "                terminations_converted.append(terminations[index])\n",
    "                actions_converted.append(actions[index])\n",
    "\n",
    "            next_states_converted = torch.stack(next_states_converted)\n",
    "            states_converted = torch.stack(states)\n",
    "            rewards_converted = [rewards_converted]\n",
    "            terminations_converted = [terminations_converted]\n",
    "            replay_buffer.add(\n",
    "                (states_converted, actions_converted, rewards_converted, next_states_converted, terminations_converted))\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size, 0.4, agent_q_networks, target_q_networks)\n",
    "                weights = batch[-1]\n",
    "                batch = batch[:-1]\n",
    "                loss = compute_td_loss_qmix(agent_q_networks, target_q_networks,mixing_network, batch,\n",
    "                                               gamma=gamma, weights=weights)\n",
    "                wandb.log({\"loss\": loss})\n",
    "                \n",
    "\n",
    "                # Zero gradients for all optimizers\n",
    "                optimizer.zero_grad()\n",
    "                mixing_optimizer.zero_grad()\n",
    "                # Backpropagate once for all losses\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                mixing_optimizer.step()\n",
    "                if schedule:\n",
    "                    scheduler.step()\n",
    "                    scheduler_mix.step()\n",
    "                    wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "            steps_counter += 1\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        wandb.log({\"blue_player1_reward\": blue_player1_reward})\n",
    "        wandb.log({\"blue_player2_reward\": blue_player2_reward})\n",
    "        wandb.log({\"episode\": episode})\n",
    "        wandb.log({'score': score})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (episode + 1) % target_update_frequency == 0:\n",
    "            update_target_network(agent_q_networks, target_q_networks)\n",
    "        \n",
    "        if (episode) % target_update_frequency == 0:\n",
    "            torch.save(agent_q_networks[0].state_dict(),f'{name_experiment}/agent1_{episode+1}.pth')\n",
    "            torch.save(agent_q_networks[1].state_dict(),f'{name_experiment}/agent3_{episode+1}.pth')\n",
    "         # Log rewards and win rates\n",
    "        episode_rewards.append(blue_player1_reward+blue_player2_reward)\n",
    "        print(blue_player1_reward)\n",
    "        print(blue_player2_reward)\n",
    "        print(f\"Episode reward: {blue_player1_reward+blue_player2_reward}\")\n",
    "\n",
    "        if np.sum(env.game.state.getBlueFood().data) == 0:\n",
    "            win_rates.append(1)\n",
    "        else:\n",
    "            win_rates.append(0)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_win_rate = np.mean(win_rates[-10:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}, Win Rate: {avg_win_rate:.2f}\")\n",
    "\n",
    "    # Plot rewards and win rates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards, label=\"Average Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Average Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smoothed_win_rates = np.convolve(win_rates, np.ones(10)/10, mode='valid') if len(win_rates) > 10 else win_rates\n",
    "    plt.plot(smoothed_win_rates, label=\"Win Rate (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.title(\"Win Rate Over Episodes\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qmix_LSTM():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Section 3: Testing and conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing IQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 300 steps with random enemy on win.lay with fixed lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class train_param:\n",
    "    n_episodes: int = 300\n",
    "    batch_size: int = 32\n",
    "    gamma: float = 0.995\n",
    "    epsilon: float = 0.75\n",
    "    epsilon_min: float = 0.1\n",
    "    epsilon_decay: float = 0.995\n",
    "    target_update_frequency: int = 20\n",
    "\n",
    "@dataclass\n",
    "class learn_param:\n",
    "    lr_mix: float = 0.0001\n",
    "    lr_agent: float = 0.0001\n",
    "    gamma: float = 0.99995\n",
    "\n",
    "#TODO: specific net_param for each network architecture\n",
    "@dataclass\n",
    "class net_param_CNN:\n",
    "    hidden_dim: int = 64\n",
    "    \n",
    "\n",
    "train_p_qmix = train_param()\n",
    "learn_p_qmix = learn_param()\n",
    "net_p_qmix = net_param_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: /home/seppe/RL/RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n"
     ]
    }
   ],
   "source": [
    "env = gymPacMan_parallel_env(display=True, layout_file=\"layouts/bloxCapture.lay\", length=300, enemieName='randomTeam.py', defenceReward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 2\n",
    "action_dim = 5\n",
    "\n",
    "obs_individual_agent = env.get_Observation(0)\n",
    "obs_shape = obs_individual_agent.shape\n",
    "global_state = [n_agents]+list(obs_shape)\n",
    "\n",
    "agent_q_networks = [AgentQNetwork(obs_shape, action_dim, hidden_dim=net_p_qmix.hidden_dim).to(device) for _ in range(n_agents)]\n",
    "target_q_networks = [AgentQNetwork(obs_shape, action_dim, hidden_dim=net_p_qmix.hidden_dim).to(device) for _ in range(n_agents)]\n",
    "mixing_network = SimpleQMixer(n_agents,global_state).to(device)\n",
    "update_target_network(agent_q_networks, target_q_networks)\n",
    "\n",
    "replay_buffer = MultiAgentPrioritizedReplayBuffer(buffer_size=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enemy: randomTeam.py\n",
      "Loading Team: /home/seppe/RL/RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574501/419928365.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(32,)\n",
      "32\n",
      "(33,)\n",
      "(33,)\n",
      "33\n",
      "(34,)\n",
      "(34,)\n",
      "34\n",
      "(35,)\n",
      "(35,)\n",
      "35\n",
      "(36,)\n",
      "(36,)\n",
      "36\n",
      "(37,)\n",
      "(37,)\n",
      "37\n",
      "(38,)\n",
      "(38,)\n",
      "38\n",
      "(39,)\n",
      "(39,)\n",
      "39\n",
      "(40,)\n",
      "(40,)\n",
      "40\n",
      "(41,)\n",
      "(41,)\n",
      "41\n",
      "(42,)\n",
      "(42,)\n",
      "42\n",
      "(43,)\n",
      "(43,)\n",
      "43\n",
      "(44,)\n",
      "(44,)\n",
      "44\n",
      "(45,)\n",
      "(45,)\n",
      "45\n",
      "(46,)\n",
      "(46,)\n",
      "46\n",
      "(47,)\n",
      "(47,)\n",
      "47\n",
      "(48,)\n",
      "(48,)\n",
      "48\n",
      "(49,)\n",
      "(49,)\n",
      "49\n",
      "(50,)\n",
      "(50,)\n",
      "50\n",
      "(51,)\n",
      "(51,)\n",
      "51\n",
      "(52,)\n",
      "(52,)\n",
      "52\n",
      "(53,)\n",
      "(53,)\n",
      "53\n",
      "(54,)\n",
      "(54,)\n",
      "54\n",
      "(55,)\n",
      "(55,)\n",
      "55\n",
      "(56,)\n",
      "(56,)\n",
      "56\n",
      "(57,)\n",
      "(57,)\n",
      "57\n",
      "(58,)\n",
      "(58,)\n",
      "58\n",
      "(59,)\n",
      "(59,)\n",
      "59\n",
      "(60,)\n",
      "(60,)\n",
      "60\n",
      "(61,)\n",
      "(61,)\n",
      "61\n",
      "(62,)\n",
      "(62,)\n",
      "62\n",
      "(63,)\n",
      "(63,)\n",
      "63\n",
      "(64,)\n",
      "(64,)\n",
      "64\n",
      "(65,)\n",
      "(65,)\n",
      "65\n",
      "(66,)\n",
      "(66,)\n",
      "66\n",
      "(67,)\n",
      "(67,)\n",
      "67\n",
      "(68,)\n",
      "(68,)\n",
      "68\n",
      "(69,)\n",
      "(69,)\n",
      "69\n",
      "(70,)\n",
      "(70,)\n",
      "70\n",
      "(71,)\n",
      "(71,)\n",
      "71\n",
      "(72,)\n",
      "(72,)\n",
      "72\n",
      "(73,)\n",
      "(73,)\n",
      "73\n",
      "(74,)\n",
      "(74,)\n",
      "74\n",
      "(75,)\n",
      "(75,)\n",
      "75\n",
      "(76,)\n",
      "(76,)\n",
      "76\n",
      "(77,)\n",
      "(77,)\n",
      "77\n",
      "(78,)\n",
      "(78,)\n",
      "78\n",
      "(79,)\n",
      "(79,)\n",
      "79\n",
      "(80,)\n",
      "(80,)\n",
      "80\n",
      "(81,)\n",
      "(81,)\n",
      "81\n",
      "(82,)\n",
      "(82,)\n",
      "82\n",
      "(83,)\n",
      "(83,)\n",
      "83\n",
      "(84,)\n",
      "(84,)\n",
      "84\n",
      "(85,)\n",
      "(85,)\n",
      "85\n",
      "(86,)\n",
      "(86,)\n",
      "86\n",
      "(87,)\n",
      "(87,)\n",
      "87\n",
      "(88,)\n",
      "(88,)\n",
      "88\n",
      "(89,)\n",
      "(89,)\n",
      "89\n",
      "(90,)\n",
      "(90,)\n",
      "90\n",
      "(91,)\n",
      "(91,)\n",
      "91\n",
      "(92,)\n",
      "(92,)\n",
      "92\n",
      "(93,)\n",
      "(93,)\n",
      "93\n",
      "(94,)\n",
      "(94,)\n",
      "94\n",
      "(95,)\n",
      "(95,)\n",
      "95\n",
      "(96,)\n",
      "(96,)\n",
      "96\n",
      "(97,)\n",
      "(97,)\n",
      "97\n",
      "(98,)\n",
      "(98,)\n",
      "98\n",
      "(99,)\n",
      "(99,)\n",
      "99\n",
      "(100,)\n",
      "(100,)\n",
      "100\n",
      "(101,)\n",
      "(101,)\n",
      "101\n",
      "(102,)\n",
      "(102,)\n",
      "102\n",
      "(103,)\n",
      "(103,)\n",
      "103\n",
      "(104,)\n",
      "(104,)\n",
      "104\n",
      "(105,)\n",
      "(105,)\n",
      "105\n",
      "(106,)\n",
      "(106,)\n",
      "106\n",
      "(107,)\n",
      "(107,)\n",
      "107\n",
      "(108,)\n",
      "(108,)\n",
      "108\n",
      "(109,)\n",
      "(109,)\n",
      "109\n",
      "(110,)\n",
      "(110,)\n",
      "110\n",
      "(111,)\n",
      "(111,)\n",
      "111\n",
      "(112,)\n",
      "(112,)\n",
      "112\n",
      "(113,)\n",
      "(113,)\n",
      "113\n",
      "(114,)\n",
      "(114,)\n",
      "114\n",
      "(115,)\n",
      "(115,)\n",
      "115\n",
      "(116,)\n",
      "(116,)\n",
      "116\n",
      "(117,)\n",
      "(117,)\n",
      "117\n",
      "(118,)\n",
      "(118,)\n",
      "118\n",
      "(119,)\n",
      "(119,)\n",
      "119\n",
      "(120,)\n",
      "(120,)\n",
      "120\n",
      "(121,)\n",
      "(121,)\n",
      "121\n",
      "(122,)\n",
      "(122,)\n",
      "122\n",
      "(123,)\n",
      "(123,)\n",
      "123\n",
      "(124,)\n",
      "(124,)\n",
      "124\n",
      "(125,)\n",
      "(125,)\n",
      "125\n",
      "(126,)\n",
      "(126,)\n",
      "126\n",
      "(127,)\n",
      "(127,)\n",
      "127\n",
      "(128,)\n",
      "(128,)\n",
      "128\n",
      "(129,)\n",
      "(129,)\n",
      "129\n",
      "(130,)\n",
      "(130,)\n",
      "130\n",
      "(131,)\n",
      "(131,)\n",
      "131\n",
      "(132,)\n",
      "(132,)\n",
      "132\n",
      "(133,)\n",
      "(133,)\n",
      "133\n",
      "(134,)\n",
      "(134,)\n",
      "134\n",
      "(135,)\n",
      "(135,)\n",
      "135\n",
      "(136,)\n",
      "(136,)\n",
      "136\n",
      "(137,)\n",
      "(137,)\n",
      "137\n",
      "(138,)\n",
      "(138,)\n",
      "138\n",
      "(139,)\n",
      "(139,)\n",
      "139\n",
      "(140,)\n",
      "(140,)\n",
      "140\n",
      "(141,)\n",
      "(141,)\n",
      "141\n",
      "(142,)\n",
      "(142,)\n",
      "142\n",
      "(143,)\n",
      "(143,)\n",
      "143\n",
      "(144,)\n",
      "(144,)\n",
      "144\n",
      "(145,)\n",
      "(145,)\n",
      "145\n",
      "(146,)\n",
      "(146,)\n",
      "146\n",
      "(147,)\n",
      "(147,)\n",
      "147\n",
      "(148,)\n",
      "(148,)\n",
      "148\n",
      "(149,)\n",
      "(149,)\n",
      "149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m team_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRL_PacMan\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_experiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_qmix_replay_buffer(env, name_experiment, agent_q_networks, target_q_networks, mixing_network, replay_buffer, train_p_qmix, learn_p_qmix)\n\u001b[1;32m      5\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[48], line 58\u001b[0m, in \u001b[0;36mtrain_qmix_replay_buffer\u001b[0;34m(env, name_experiment, agent_q_networks, target_q_networks, mixing_network, replay_buffer, train_p, learn_p, random_enemy, schedule, later_exploration)\u001b[0m\n\u001b[1;32m     54\u001b[0m     action \u001b[38;5;241m=\u001b[39m epsilon_greedy_action(agent_q_networks[i], state, epsilon, legal_actions, exploring)\n\u001b[1;32m     55\u001b[0m     actions[agent_index] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m---> 58\u001b[0m next_states, rewards, terminations, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m     60\u001b[0m score \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_change\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     61\u001b[0m done \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m terminations\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m agent_indexes}\n",
      "File \u001b[0;32m~/RL/RL_MARL_QMIX/gymPacMan.py:169\u001b[0m, in \u001b[0;36mgymPacMan_parallel_env.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    167\u001b[0m     observations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents[agentIndex]] \u001b[38;5;241m=\u001b[39m observation\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    171\u001b[0m blue_reward \u001b[38;5;241m=\u001b[39m blue_reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(blue_score_change, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    172\u001b[0m red_reward \u001b[38;5;241m=\u001b[39m  red_reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(red_score_change, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/RL/RL_MARL_QMIX/captureGraphicsDisplay.py:264\u001b[0m, in \u001b[0;36mPacmanGraphics.update\u001b[0;34m(self, newState)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimatePacman(agentState, prevState, prevImage)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveGhost(agentState, agentIndex, prevState, prevImage)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magentImages[agentIndex] \u001b[38;5;241m=\u001b[39m (agentState, prevImage)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newState\u001b[38;5;241m.\u001b[39m_foodEaten \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/RL/RL_MARL_QMIX/captureGraphicsDisplay.py:418\u001b[0m, in \u001b[0;36mPacmanGraphics.moveGhost\u001b[0;34m(self, ghost, ghostIndex, prevGhost, ghostImageParts)\u001b[0m\n\u001b[1;32m    415\u001b[0m delta \u001b[38;5;241m=\u001b[39m new_x \u001b[38;5;241m-\u001b[39m old_x, new_y \u001b[38;5;241m-\u001b[39m old_y\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ghostImagePart \u001b[38;5;129;01min\u001b[39;00m ghostImageParts:\n\u001b[0;32m--> 418\u001b[0m   move_by(ghostImagePart, delta, lift\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    419\u001b[0m refresh()\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ghost\u001b[38;5;241m.\u001b[39mscaredTimer \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/RL/RL_MARL_QMIX/graphicsUtils.py:365\u001b[0m, in \u001b[0;36mmove_by\u001b[0;34m(object, x, y, d_o_e, d_w, lift)\u001b[0m\n\u001b[1;32m    362\u001b[0m     newCoords\u001b[38;5;241m.\u001b[39mappend(coord \u001b[38;5;241m+\u001b[39m inc)\n\u001b[1;32m    364\u001b[0m _canvas\u001b[38;5;241m.\u001b[39mcoords(\u001b[38;5;28mobject\u001b[39m, \u001b[38;5;241m*\u001b[39mnewCoords)\n\u001b[0;32m--> 365\u001b[0m d_o_e(d_w)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lift:\n\u001b[1;32m    367\u001b[0m     _canvas\u001b[38;5;241m.\u001b[39mtag_raise(\u001b[38;5;28mobject\u001b[39m)\n",
      "File \u001b[0;32m~/RL/RL_MARL_QMIX/graphicsUtils.py:347\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    343\u001b[0m     _canvas\u001b[38;5;241m.\u001b[39mcoords(\u001b[38;5;28mobject\u001b[39m, \u001b[38;5;241m*\u001b[39mnewCoords)\n\u001b[1;32m    344\u001b[0m     d_o_e(d_w)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_by\u001b[39m(\u001b[38;5;28mobject\u001b[39m, x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 347\u001b[0m             d_o_e\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m arg: _root_window\u001b[38;5;241m.\u001b[39mdooneevent(arg),\n\u001b[1;32m    348\u001b[0m                        d_w\u001b[38;5;241m=\u001b[39mtkinter\u001b[38;5;241m.\u001b[39m_tkinter\u001b[38;5;241m.\u001b[39mDONT_WAIT, lift\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: x, y \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "name_experiment ='SimpleQMIX_CNN_random_blox_basic'\n",
    "team_name = 'RL_PacMan'\n",
    "wandb.init(project=\"\", id = f\"{name_experiment}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_qmix_replay_buffer(env, name_experiment, agent_q_networks, target_q_networks, mixing_network, replay_buffer, train_p_qmix, learn_p_qmix)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class train_param:\n",
    "    n_episodes: int = 100\n",
    "    batch_size: int = 32\n",
    "    gamma: float = 0.995\n",
    "    epsilon: float = 0.3\n",
    "    epsilon_min: float = 0.1\n",
    "    epsilon_decay: float = 0.995\n",
    "    target_update_frequency: int = 20\n",
    "\n",
    "@dataclass\n",
    "class learn_param:\n",
    "    lr_mix: float = 0.0001\n",
    "    lr_agent: float = 0.0001\n",
    "    gamma: float = 0.99995\n",
    "\n",
    "#TODO: specific net_param for each network architecture\n",
    "@dataclass\n",
    "class net_param_CNN:\n",
    "    hidden_dim: int = 64\n",
    "    \n",
    "\n",
    "train_p_qmix = train_param()\n",
    "learn_p_qmix = learn_param()\n",
    "net_p_qmix = net_param_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymPacMan_parallel_env(display=True, layout_file=\"layouts/bloxCapture.lay\", length=300, enemieName='baselineTeam.py', defenceReward=True)\n",
    "name_experiment ='SimpleQMIX_CNN_random_blox_basic'\n",
    "team_name = 'RL_PacMan'\n",
    "wandb.init(entity = team_name, id = f\"{name_experiment}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_qmix_indv_reward(env, name_experiment, agent_q_networks, target_q_networks, mixing_network, replay_buffer, train_p_qmix, learn_p_qmix)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQL on tiny with random enemy with scheduled learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: /home/seppe/RL/RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'walls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m gymPacMan_parallel_env(display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, layout_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayouts/bloxCapture.lay\u001b[39m\u001b[38;5;124m\"\u001b[39m, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, enemieName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomTeam.py\u001b[39m\u001b[38;5;124m'\u001b[39m, defenceReward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(entity \u001b[38;5;241m=\u001b[39m team_name, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPacManCapture\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_experiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_iql(env, name_experiment, agent_q_networks, target_q_networks, replay_buffer, train_p_iql, learn_p_iql)\n\u001b[1;32m      9\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[22], line 23\u001b[0m, in \u001b[0;36mtrain_iql\u001b[0;34m(env, name_experiment, agent_q_networks, target_q_networks, replay_buffer, train_p, learn_p, random_enemy, schedule, later_exploration)\u001b[0m\n\u001b[1;32m     21\u001b[0m l \u001b[38;5;241m=\u001b[39m layout\u001b[38;5;241m.\u001b[39mgetLayout(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/layouts/bloxCapture.lay\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m distancer \u001b[38;5;241m=\u001b[39m Distancer(l)\n\u001b[0;32m---> 23\u001b[0m distancer\u001b[38;5;241m.\u001b[39mgetMazeDistances()\n\u001b[1;32m     25\u001b[0m legal_actions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     26\u001b[0m agent_indexes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/RL/RL_MARL_QMIX/distanceCalculator.py:36\u001b[0m, in \u001b[0;36mDistancer.getMazeDistances\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetMazeDistances\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdc\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/RL/RL_MARL_QMIX/distanceCalculator.py:103\u001b[0m, in \u001b[0;36mDistanceCalculator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m distanceMap\n\u001b[1;32m    101\u001b[0m distanceMap \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout\u001b[38;5;241m.\u001b[39mwalls \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m distanceMap:\n\u001b[1;32m    104\u001b[0m   distances \u001b[38;5;241m=\u001b[39m computeDistances(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout)\n\u001b[1;32m    105\u001b[0m   distanceMap[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout\u001b[38;5;241m.\u001b[39mwalls] \u001b[38;5;241m=\u001b[39m distances\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'walls'"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer()\n",
    "name_experiment ='IQL_CNN_tiny'\n",
    "team_name = 'RL_PacMan'\n",
    "\n",
    "env = gymPacMan_parallel_env(display=True, layout_file=\"layouts/tinyCapture.lay\", length=300, enemieName='randomTeam.py', defenceReward=True)\n",
    "wandb.init(entity = team_name, project=\"PacManCapture\", id = f\"{name_experiment}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "\n",
    "train_iql(env, name_experiment, agent_q_networks, target_q_networks, replay_buffer, train_p_iql, learn_p_iql)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the list of enemies\n",
    "enemies = ['randomTeam.py', 'baselineTeam.py', 'heuristicTeam.py', 'approxQTeam.py', 'MCTSTeam.py', 'AstarTeam.py']\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {enemy: {'wins': 0, 'losses': 0, 'draws': 0} for enemy in enemies}\n",
    "\n",
    "# Number of games to play against each enemy\n",
    "num_games = 10\n",
    "\n",
    "# Function to evaluate the agent against an enemy\n",
    "def evaluate_agent(env, enemy_name, num_games):\n",
    "    wins, losses, draws = 0, 0, 0\n",
    "    for _ in range(num_games):\n",
    "        env.reset(enemieName=enemy_name)\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "            states = []\n",
    "            for i, agent_index in enumerate([1,3]):\n",
    "                obs_agent = env.get_Observation(agent_index)\n",
    "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                states.append(state)\n",
    "                state = state.unsqueeze(0).to(device)  # Add batch dimension\n",
    "                q_values = agent_q_networks[i](state)\n",
    "                q_values = q_values.cpu().detach().numpy()\n",
    "                action = np.random.choice(np.flatnonzero(q_values == q_values.max()))\n",
    "                actions[agent_index] = action\n",
    "            next_state, reward, done, info = env.step(actions)\n",
    "            if done:\n",
    "                if info['result'] == 'win':\n",
    "                    wins += 1\n",
    "                elif info['result'] == 'loss':\n",
    "                    losses += 1\n",
    "                else:\n",
    "                    draws += 1\n",
    "    return wins, losses, draws\n",
    "\n",
    "# Evaluate the agent against each enemy\n",
    "for enemy in enemies:\n",
    "    env = gymPacMan_parallel_env(display=False, layout_file=\"layouts/smallCapture.lay\", length=200, enemieName=enemy, defenceReward=True)\n",
    "    wins, losses, draws = evaluate_agent(env, enemy, num_games)\n",
    "    results[enemy]['wins'] = wins\n",
    "    results[enemy]['losses'] = losses\n",
    "    results[enemy]['draws'] = draws\n",
    "\n",
    "# Convert results to a DataFrame for better display\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing idea's**\n",
    "\n",
    "- experience buffer\n",
    "- only exploration after first 10 steps\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## individual reward\n",
    "from capture import AgentRules\n",
    "def individual_reward(env,agentIndex,action, reward, distancer, counter):\n",
    "    stop = False\n",
    "    next_state = env.game.state.generateSuccessor(agentIndex, env.action_mapping[action])\n",
    "    # --- Reward for capturing food ---\n",
    "   \n",
    "    if env.game.state.getAgentState(agentIndex).numCarrying < next_state.getAgentState(agentIndex).numCarrying:\n",
    "        reward += 1\n",
    "        print('captured food')\n",
    "\n",
    "    # --- Reward for delivering food ---\n",
    "    if env.game.state.getAgentState(agentIndex).numCarrying > 0 and next_state.getAgentState(agentIndex).numCarrying == 0 and env.game.state.getAgentState(agentIndex).start.pos != next_state.getAgentPosition(agentIndex):\n",
    "        print('gave food to side')\n",
    "        print(f\"number of food: {env.game.state.getAgentState(agentIndex).numCarrying}\")\n",
    "        reward += (env.game.state.getAgentState(agentIndex).numCarrying)*5\n",
    "\n",
    "           \n",
    "    for red_team_index in [0, 2]:\n",
    "        if (env.game.state.data.agentStates[red_team_index].isPacman and\n",
    "                not next_state.data.agentStates[red_team_index].isPacman):\n",
    "            if next_state.data.agentStates[red_team_index].configuration.pos == next_state.data.agentStates[red_team_index].start.pos:\n",
    "                print('defended')\n",
    "                reward += 0.75\n",
    "\n",
    "    return reward\n",
    "\n",
    "def interpolate_weights(start_weights, end_weights, factor):\n",
    "    return [(1 - factor) * start + factor * end for start, end in zip(start_weights, end_weights)]\n",
    "\n",
    "def get_dynamic_weights(episode, n_episodes):\n",
    "    # Define the initial weights for the beginning, middle, and end\n",
    "    weights_beginning = [1, 0, 0, 0, 0, 0]\n",
    "    weights_middle = [0, 0.33, 0.34, 0.33, 0, 0]\n",
    "    weights_end = [0, 0, 0, 0, 0.5, 0.5]\n",
    "\n",
    "    # Calculate the transition points\n",
    "    transition1 = n_episodes // 3\n",
    "    transition2 = 2 * n_episodes // 3\n",
    "\n",
    "    # Adjust weights based on the current episode\n",
    "    if episode < transition1:\n",
    "        factor = episode / transition1\n",
    "        return interpolate_weights(weights_beginning, weights_middle, factor)\n",
    "    elif episode < transition2:\n",
    "        factor = (episode - transition1) / (transition2 - transition1)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    else:\n",
    "        factor = (episode - transition2) / (n_episodes - transition2)\n",
    "        return interpolate_weights(weights_middle, weights_end, factor)\n",
    "    \n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, action_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size after conv layers\n",
    "        example_input = torch.rand(1, *input_shape)\n",
    "        conv_output_dim = self.conv(example_input).shape[1]\n",
    "        \n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(conv_output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        features = self.conv(obs)\n",
    "        logits = self.actor(features)\n",
    "        return logits\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size after conv layers\n",
    "        example_input = torch.rand(1, *input_shape)\n",
    "        conv_output_dim = self.conv(example_input).shape[1]\n",
    "        \n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(conv_output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        features = self.conv(obs)\n",
    "        value = self.critic(features)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sample_and_logp(s, actor):\n",
    "    with torch.no_grad():\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float32).to(device)\n",
    "        logits = actor(s_batch)\n",
    "        logits = logits.squeeze(0)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        a = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "        logprb = -F.cross_entropy(logits,a, reduction='none')\n",
    "        return a.tolist(), logits.tolist(), logprb.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def compute_loss(actor, value_network, gamma, kl_coeff, states_1, states_2, actions_1, actions_2, logits_1, logits_2, probs_1, probs_2, rewards_1, rewards_2):\n",
    "    cum_rewards_1 = np.zeros_like(rewards_1)\n",
    "    cum_rewards_2 = np.zeros_like(rewards_2)\n",
    "    for j in reversed(range(len(rewards_1))):\n",
    "        cum_rewards_1[j] = rewards_1[j] + (gamma * cum_rewards_1[j+1] if j+1 < len(rewards_1) else 0)\n",
    "        cum_rewards_2[j] = rewards_2[j] + (gamma * cum_rewards_2[j+1] if j+1 < len(rewards_2) else 0)\n",
    "    \n",
    "    states_1 = torch.tensor(states_1, dtype=torch.float32).to(device)\n",
    "    states_2 = torch.tensor(states_2, dtype=torch.float32).to(device)\n",
    "    actions_1 = torch.tensor(actions_1, dtype=torch.long).to(device)\n",
    "    actions_2 = torch.tensor(actions_2, dtype=torch.long).to(device)\n",
    "    logits_1 = torch.tensor(logits_1, dtype=torch.float32).to(device)\n",
    "    logits_2 = torch.tensor(logits_2, dtype=torch.float32).to(device)\n",
    "    probs_1 = torch.tensor(probs_1, dtype=torch.float32).to(device)\n",
    "    probs_2 = torch.tensor(probs_2, dtype=torch.float32).to(device)\n",
    "\n",
    "    logits_1_new = actor[0](states_1) \n",
    "    logits_2_new = actor[1](states_2)\n",
    "    \n",
    "    values_1_new = value_network(states_1)\n",
    "    values_2_new = value_network(states_2)\n",
    "\n",
    "    probs_1 = probs_1.unsqueeze(1)\n",
    "    probs_2 = probs_2.unsqueeze(1)\n",
    "\n",
    "    cum_rewards_1 = torch.tensor(cum_rewards_1, dtype=torch.float32).to(device)\n",
    "    cum_rewards_2 = torch.tensor(cum_rewards_2, dtype=torch.float32).to(device)\n",
    "    cum_rewards_1 = cum_rewards_1.unsqueeze(1)\n",
    "    cum_rewards_2 = cum_rewards_2.unsqueeze(1)\n",
    "\n",
    "    advantage_1 = cum_rewards_1 - values_1_new\n",
    "    advantage_2 = cum_rewards_2 - values_2_new\n",
    "    advantage_1 = (advantage_1 - advantage_1.mean()) / (advantage_1.std() + 1e-8)\n",
    "    advantage_2 = (advantage_2 - advantage_2.mean()) / (advantage_2.std() + 1e-8)\n",
    "    \n",
    "    logprobs_1 = -F.cross_entropy(logits_1_new, actions_1, reduction='none').unsqueeze(1)\n",
    "    logprobs_2 = -F.cross_entropy(logits_2_new, actions_2, reduction='none').unsqueeze(1)\n",
    "\n",
    "\n",
    "    prob_ratio_1 = torch.exp(logprobs_1 - probs_1)\n",
    "    prob_ratio_2 = torch.exp(logprobs_2 - probs_2)\n",
    "\n",
    "    probs_old_1 = F.softmax(logits_1, dim=-1)\n",
    "    probs_new_1 = F.softmax(logits_1_new, dim=-1)\n",
    "    dist_old_1 = Categorical(probs_old_1)\n",
    "    dist_new_1 = Categorical(probs_new_1)\n",
    "    kl_1 = torch.distributions.kl_divergence(dist_old_1, dist_new_1).mean()\n",
    "\n",
    "    probs_old_2 = F.softmax(logits_2, dim=-1)\n",
    "    probs_new_2 = F.softmax(logits_2_new, dim=-1)\n",
    "    dist_old_2 = Categorical(probs_old_2)\n",
    "    dist_new_2 = Categorical(probs_new_2)\n",
    "    kl_2 = torch.distributions.kl_divergence(dist_old_2, dist_new_2).mean()\n",
    "\n",
    "    loss_1 = (-advantage_1 * prob_ratio_1 + kl_coeff * kl_1).mean()\n",
    "    loss_2 = (-advantage_2 * prob_ratio_2 + kl_coeff * kl_2).mean()\n",
    "    \n",
    "\n",
    "    combined_rewards = torch.cat([cum_rewards_1, cum_rewards_2], dim=0)\n",
    "    combined_rewards = torch.tensor(combined_rewards, dtype=torch.float32).to(device)\n",
    "    values_combined = value_network(torch.cat([states_1, states_2], dim=0))\n",
    "\n",
    "    loss_critic = F.mse_loss(values_combined, combined_rewards)\n",
    "\n",
    "    \n",
    "    return loss_1, loss_2, loss_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layout\n",
    "from distanceCalculator import Distancer\n",
    "\n",
    "def train_ppo(env, agent_networks, value_network, gamma, n_episodes, random_enemy=True, enemyName='randomTeam.py'):\n",
    "    \n",
    "    optimizer_1 = optim.Adam(agent_networks[0].parameters(), lr=0.0001)\n",
    "    optimizer_2 = optim.Adam(agent_networks[1].parameters(), lr=0.0001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.0001)\n",
    "\n",
    "    l = layout.getLayout('bloxCapture.lay')\n",
    "    distancer = Distancer(l)\n",
    "    distancer.getMazeDistances()\n",
    "    \n",
    "    agent_indexes = [1, 3]\n",
    "    win_rates = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    if random_enemy:\n",
    "        enemylist = ['randomTeam.py', 'baselineTeam.py', 'heuristicTeam.py', 'approxQTeam.py', 'MCTSTeam.py', 'AstarTeam.py']\n",
    "        weights_enemy = get_dynamic_weights(0, n_episodes)\n",
    "        enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "    else:\n",
    "        enemieName = enemyName\n",
    "\n",
    "    create_folder_if_not_exists(name_experiment)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        steps_counter = 0\n",
    "        run_counter =[11,11]\n",
    "\n",
    "        print(f\"episode:   {episode}\")\n",
    "        print(f\"enemy: {enemieName}\")\n",
    "\n",
    "        done = {agent_id: False for agent_id in agent_indexes}\n",
    "        if random_enemy:\n",
    "            weights_enemy = get_dynamic_weights(episode, n_episodes)\n",
    "            enemieName = np.random.choice(enemylist, p=weights_enemy)\n",
    "        env.reset(enemieName=enemieName)\n",
    "\n",
    "        blue_player1_reward = 0\n",
    "        blue_player2_reward = 0\n",
    "        score = 0\n",
    "\n",
    "        states_1 = []\n",
    "        states_2 = []\n",
    "        actions_1 = []\n",
    "        actions_2 = []\n",
    "        logits_1 = []\n",
    "        logits_2 = []\n",
    "        probs_1 = []\n",
    "        probs_2 = []\n",
    "        rewards_1 = []\n",
    "        rewards_2 = []\n",
    "            \n",
    "\n",
    "        while not all(done.values()):\n",
    "\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "\n",
    "            reward_dist = [0,0]\n",
    "            for i, agent_index in enumerate(agent_indexes):\n",
    "                if i == 0:\n",
    "                    obs_agent = env.get_Observation(agent_index)\n",
    "                    state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                    states_1.append(state)\n",
    "                    action, logits, probs = pick_sample_and_logp(state, agent_networks[i])\n",
    "                    actions_1.append(action)\n",
    "                    logits_1.append(logits)\n",
    "                    probs_1.append(probs)\n",
    "                    reward_dist[i] = individual_reward(env,agent_index,action,reward_dist[i],distancer,run_counter[i])\n",
    "                else:\n",
    "                    obs_agent = env.get_Observation(agent_index)\n",
    "                    state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                    states_2.append(state)\n",
    "                    action, logits, probs = pick_sample_and_logp(state, agent_networks[i])\n",
    "                    actions_2.append(action)\n",
    "                    logits_2.append(logits)\n",
    "                    probs_2.append(probs)\n",
    "                    reward_dist[i] = individual_reward(env,agent_index,action,reward_dist[i],distancer,run_counter[i])\n",
    "\n",
    "                actions[agent_index] = action\n",
    "\n",
    "\n",
    "            _, rewards, terminations, info = env.step(actions)\n",
    "\n",
    "            for i, index in enumerate(agent_indexes):\n",
    "                    next_state = env.game.state.generateSuccessor(index, env.action_mapping[actions[index]])\n",
    "                    if (env.game.state.data.agentStates[index].isPacman and not next_state.data.agentStates[index].isPacman):\n",
    "                        if next_state.data.agentStates[index].configuration.pos == next_state.data.agentStates[index].start.pos:\n",
    "                            run_counter[i] = 11\n",
    "                            reward_dist[i] -= 10\n",
    "                            print(\"reset counter\")\n",
    "                    else:\n",
    "                        run_counter[i] -=1\n",
    "          \n",
    "            score -= info[\"score_change\"]\n",
    "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
    "            if all(done.values()):\n",
    "                if env.game.state.getBlueFood().count() == 0:\n",
    "                    reward_dist[0] -= 2\n",
    "                    reward_dist[1] -= 2\n",
    "                elif env.game.state.getRedFood().count() == 0:\n",
    "                    reward_dist[0] += 100\n",
    "                    reward_dist[1] += 100\n",
    "            rewards_1.append(reward_dist[0])\n",
    "            rewards_2.append(reward_dist[1])\n",
    "            blue_player1_reward += rewards[1] + reward_dist[0]\n",
    "            blue_player2_reward += rewards[3] + reward_dist[1]\n",
    "\n",
    "        states_1 = np.array(states_1, dtype=np.float32)\n",
    "        states_2 = np.array(states_2, dtype=np.float32)\n",
    "        actions_1 = np.array(actions_1, dtype=np.int64)\n",
    "        actions_2 = np.array(actions_2, dtype=np.int64)\n",
    "        logits_1 = np.array(logits_1, dtype=np.float32)\n",
    "        logits_2 = np.array(logits_2, dtype=np.float32)\n",
    "        probs_1 = np.array(probs_1, dtype=np.float32)\n",
    "        probs_2 = np.array(probs_2, dtype=np.float32)\n",
    "        rewards_1 = np.array(rewards_1, dtype=np.float32)\n",
    "        rewards_2 = np.array(rewards_2, dtype=np.float32)\n",
    "\n",
    "        # loss_1,loss_2,loss_critic = compute_loss(agent_networks, value_network, gamma, 0.2, states_1, states_2, actions_1, actions_2, logits_1, logits_2, probs_1, probs_2, rewards_1, rewards_2)\n",
    "        # # Update Actor 1\n",
    "        # optimizer_1.zero_grad()\n",
    "        # loss_1.backward()\n",
    "        # optimizer_1.step()\n",
    "\n",
    "        # # Update Actor 2\n",
    "        # optimizer_2.zero_grad()\n",
    "        # loss_2.backward()\n",
    "        # optimizer_2.step()\n",
    "\n",
    "        # # Update Critic\n",
    "        # value_optimizer.zero_grad()\n",
    "        # loss_critic.backward()\n",
    "        # value_optimizer.step()\n",
    "\n",
    "        steps_counter += 1\n",
    "        # wandb.log({\"loss1\": loss_1})\n",
    "        # wandb.log({\"loss2\": loss_2})\n",
    "        wandb.log({\"blue_player1_reward\": blue_player1_reward})\n",
    "        wandb.log({\"blue_player2_reward\": blue_player2_reward})\n",
    "        wandb.log({\"episode\": episode})\n",
    "        wandb.log({'score': score})\n",
    "\n",
    "\n",
    "\n",
    "        if (episode) % 20 == 0:\n",
    "            torch.save(agent_networks[0].state_dict(),f'{name_experiment}/agent1_{episode+1}_baseline.pth')\n",
    "            torch.save(agent_networks[1].state_dict(),f'{name_experiment}/agent3_{episode+1}_baseline.pth')\n",
    "            torch.save(value_network.state_dict(),f'{name_experiment}/value_{episode+1}.pth')\n",
    "         # Log rewards and win rates\n",
    "        episode_rewards.append(blue_player1_reward+blue_player2_reward)\n",
    "        print(blue_player1_reward)\n",
    "        print(blue_player2_reward)\n",
    "        print(f\"Episode reward: {blue_player1_reward+blue_player2_reward}\")\n",
    "\n",
    "        if np.sum(env.game.state.getRedFood().data) == 0:\n",
    "            win_rates.append(1)\n",
    "        else:\n",
    "            win_rates.append(0)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_win_rate = np.mean(win_rates[-10:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}, Win Rate: {avg_win_rate:.2f}\")\n",
    "\n",
    "    # Plot rewards and win rates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards, label=\"Average Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Average Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smoothed_win_rates = np.convolve(win_rates, np.ones(10)/10, mode='valid') if len(win_rates) > 10 else win_rates\n",
    "    plt.plot(smoothed_win_rates, label=\"Win Rate (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.title(\"Win Rate Over Episodes\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout from file\n",
      "Loading Team: /home/seppe/RL/project3/RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Red team starts\n",
      "torch.Size([8, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221298/4087282216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  value_networks.load_state_dict(torch.load('PPO_bloxCapture_baseline/value_101.pth'))\n",
      "/tmp/ipykernel_221298/4087282216.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_networks[0].load_state_dict(torch.load('PPO_bloxCapture_baseline/agent1_101_baseline.pth'))\n",
      "/tmp/ipykernel_221298/4087282216.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_networks[1].load_state_dict(torch.load('PPO_bloxCapture_baseline/agent3_101_baseline.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gymPacMan_parallel_env(display=False, layout_file=\"layouts/bloxCapture.lay\", length=300, enemieName='randomTeam.py', defenceReward=True)\n",
    "n_agents = 2\n",
    "action_dim = 5\n",
    "\n",
    "obs_individual_agent = env.get_Observation(0)\n",
    "obs_shape = obs_individual_agent.shape\n",
    "print(obs_shape)\n",
    "\n",
    "# Initialiseer netwerken\n",
    "value_networks = CriticNetwork(obs_shape).to(device)\n",
    "agent_networks = [ActorNetwork(obs_shape, action_dim).to(device) for _ in range(n_agents)]\n",
    "\n",
    "value_networks.load_state_dict(torch.load('PPO_bloxCapture_baseline/value_101.pth'))\n",
    "agent_networks[0].load_state_dict(torch.load('PPO_bloxCapture_baseline/agent1_101_baseline.pth'))\n",
    "agent_networks[1].load_state_dict(torch.load('PPO_bloxCapture_baseline/agent3_101_baseline.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/seppe/RL/project3/RL_MARL_QMIX/wandb/run-20241215_203106-random__20241215_203106</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized/runs/random__20241215_203106' target=\"_blank\">random__20241215_203106</a></strong> to <a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized' target=\"_blank\">https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized/runs/random__20241215_203106' target=\"_blank\">https://wandb.ai/seppe-wuyts3-universiteit-gent/uncategorized/runs/random__20241215_203106</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   0\n",
      "enemy: randomTeam.py\n",
      "Loading Team: /home/seppe/RL/project3/RL_MARL_QMIX//agents/randomTeam.py\n",
      "Arguments: {}\n",
      "Blue team starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221298/3364145381.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_221298/3364145381.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "reset counter\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n",
      "captured food\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m team_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRL_PacMan_PPO\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_networks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_networks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.995\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandomTeam.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m      6\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 138\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(env, agent_networks, value_network, gamma, n_episodes, random_enemy, enemyName)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# loss_1,loss_2,loss_critic = compute_loss(agent_networks, value_network, gamma, 0.2, states_1, states_2, actions_1, actions_2, logits_1, logits_2, probs_1, probs_2, rewards_1, rewards_2)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# # Update Actor 1\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# optimizer_1.zero_grad()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# loss_critic.backward()\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# value_optimizer.step()\u001b[39;00m\n\u001b[1;32m    137\u001b[0m steps_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 138\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mloss_1\u001b[49m})\n\u001b[1;32m    139\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss2\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_2})\n\u001b[1;32m    140\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue_player1_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: blue_player1_reward})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_1' is not defined"
     ]
    }
   ],
   "source": [
    "name_experiment ='PPO_bloxCapture_baseline'\n",
    "team_name = 'RL_PacMan_PPO'\n",
    "wandb.init(project=\"\", id = f\"{'random'}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_ppo(env, agent_networks, value_networks, 0.995,50,False,'randomTeam.py')\n",
    "wandb.finish()\n",
    "wandb.init(project=\"\", id = f\"{'baseline'}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_ppo(env, agent_networks, value_networks, 0.995,50,False,'baselineTeam.py')\n",
    "wandb.finish()\n",
    "wandb.init(project=\"\", id = f\"{'heuristic'}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_ppo(env, agent_networks, value_networks, 0.995,50,False,'heuristicTeam.py')\n",
    "wandb.finish()\n",
    "wandb.init(project=\"\", id = f\"{'approxQ'}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_ppo(env, agent_networks, value_networks, 0.995,50,False,'approxQTeam.py')\n",
    "wandb.finish()\n",
    "wandb.init(project=\"\", id = f\"{'MCTS'}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_ppo(env, agent_networks, value_networks, 0.995,50,False,'MCTSTeam.py')\n",
    "wandb.finish()\n",
    "wandb.init(project=\"\", id = f\"{'ASTAR'}__{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "train_ppo(env, agent_networks, value_networks, 0.995,50,False,'AstarTeam.py')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
